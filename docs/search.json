[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Index",
    "section": "",
    "text": "Repository with the materials of the course MP223 - Applied Econometrics Methods in the Social Sciences for summer Semester 2023 at the JLU.\n\n\nThis is an approximate course plan that is updated before each lecture with the slides and materials relevant for each lecture.\n\n\nSlides:\n\nw01a-introduction + pdf\nw01b-selection-bias + pdf\nw01c-r-rstudio-intro + pdf\n\nExercises:\n\nex01-rct.zip - the R project with code and data\nSupplementary recording on how to execute the analysis https://youtu.be/-ednrypEv4o.\n\n\n\n\nSlides: See Ilias.\n\n\n\nSlides:\n\nw03-mlr-part1 + pdf\n\nExercises + Recordings:\n\nex03-regression-part1.zip - the R project with code and data\nex.03.a Regression basics https://youtu.be/rUlpdHsXPRw\nex.03.b Wage and Education https://youtu.be/5zwYnnjnZXo\nex.03.c Hedonic regression https://youtu.be/-tIylFJUNX4\n\n\n\n\nSlides:\n\nw04-mlr-part2 + pdf\n\nExercises:\n\nex04-regression-part2.zip - the R project with code and data\nex.04.b Complex production function https://youtu.be/DMxFupZSzTs\n\n\n\n\nSlides: See Ilias.\n\n\n\nSlides:\n\nw06a-interactions + pdf\n\nPre-recorded materials on interaction term https://youtu.be/2M4LRVGPgoI\n\nw06b-DiD + pdf\n\nExercises:\n\nex06-did-and-interactions.zip - the R project with code and data\n\nex.06.a Interaction terms https://youtu.be/GJLt36FAj48\nex.06.b DiD https://youtu.be/QAI3-Rh1Sok\nex.06.C DiD Card Krueger https://youtu.be/ogaVyGCO_6c\n\n\n\n\n\nSlides:\n\nw07-OVB + pdf\n\nw07 OVB Lecture https://youtu.be/AM6fhMby0W4\n\n\nExercises:\n\nex07-ovb.zip - R project with code and data\n\nex.07.a Wage, Education and ability https://youtu.be/x3g9z63xqFE\nex.07.b Wage, union membership and education https://youtu.be/EJhmkJGLsBc\n\n\n\n\n\nSlides:\n\nw08a-panel-regression + pdf\nw08b-panel-regression-production-function.qmd + pdf\n\nExercises:\n\nex08-panel-regression.zip - R project with code and data\n\n\n\n\n\n\n\nSlides:\n\nw10a-IV.qmd + pdf\nw10b-IV-examples.qmd + pdf\n\nExercises:\n\nex10-IV.zip - R project with code and data"
  },
  {
    "objectID": "slides/w01a-introduction.html#welcome-to-meta-course",
    "href": "slides/w01a-introduction.html#welcome-to-meta-course",
    "title": "Introduction. Organisation. Setup.",
    "section": "Welcome to MP-223-EN Applied Econometrics Methods for the Social Sciences (SoSe 2023)",
    "text": "Welcome to MP-223-EN Applied Econometrics Methods for the Social Sciences (SoSe 2023)\nAuthor: Eduard Bukin eduard.bukin@agrar.uni-giessen.de\n\nInstitute of Agricultural Policy and Market Research\nphone: +49 641 99-37055\noffice: Zeughaus (Senckenbergstr. 3). Room: 132\nhours: Part time (better to make an appointment)"
  },
  {
    "objectID": "slides/w01a-introduction.html#course-objectives",
    "href": "slides/w01a-introduction.html#course-objectives",
    "title": "Introduction. Organisation. Setup.",
    "section": "Course objectives",
    "text": "Course objectives\n\n\n\n\nget familiar with fundamental econometric techniques;\ndevelop ability to reason on the appropriation of specific econometric methods;\nlearn how to apply those econometrics in “R”;\nexercise interpreting and communicating results;\n\n\n\n\n\nDevelop practical skills of applying following empirical econometric methods in R:\n\nMultiple regression analysis;\nPanel regression analysis;\nBinary outcome variable;\nInstrumental variable;\nImpact evaluation (DID and RDD);"
  },
  {
    "objectID": "slides/w01a-introduction.html#have-you-ever-heard-that-vaccination-causes-autism",
    "href": "slides/w01a-introduction.html#have-you-ever-heard-that-vaccination-causes-autism",
    "title": "Introduction. Organisation. Setup.",
    "section": "Have you ever heard that “vaccination causes autism”?",
    "text": "Have you ever heard that “vaccination causes autism”?\n\n\nSee (Wakefield et al., 1998).\n\n\nA. Wakefield et, al. “Ileal-lymphoid-nodular hyperplasia, non-specific colitis, and pervasive developmental disorder in children”. In: The Lancet 351.9103 (Feb. 1998), pp. 637-641. DOI: 10.1016/s0140-6736(97)11096-0 web-page.\nRetracted in February 2010. See Lancet MMR autism fraud.\n\n\n\n\n\n“The Lancet” is an influential journals\n\nImpact Factor (IF) in 2020 was 79.32\n\nEach field has own seminal journal:\n\nAg. Econ - “American Journal of Agricultural Economics” with IF 2.245 (2017)\nEconometrics - “Econometrica” with IF 5.84 (2020)\nDevelopment economics - “World Development” has the IF 5.278 (2020)"
  },
  {
    "objectID": "slides/w01a-introduction.html#wakefield1998.-introduction-12",
    "href": "slides/w01a-introduction.html#wakefield1998.-introduction-12",
    "title": "Introduction. Organisation. Setup.",
    "section": "Wakefield et al. (1998). Introduction (1/2)",
    "text": "Wakefield et al. (1998). Introduction (1/2)\nResearch question: Early report on medical cases;\n\nData:\n\n12 children (mean age 6 years [range 3–10], 11 boys).\n\nregularly saw a gastroenterologist;\nhad “… a history of normal development followed by loss of acquired skills, including language [autism]…”;\n\n“Control” group is made post factum, artificially, matching 12 random children of the same age/gender."
  },
  {
    "objectID": "slides/w01a-introduction.html#wakefield1998.-introduction-22",
    "href": "slides/w01a-introduction.html#wakefield1998.-introduction-22",
    "title": "Introduction. Organisation. Setup.",
    "section": "Wakefield et al. (1998). Introduction (2/2)",
    "text": "Wakefield et al. (1998). Introduction (2/2)\nTreatment:\n\nMeasles and/or MMR (measles, mumps, and rubella) vaccines at the age of 12-16 month;\n\n\nOutcome:\n\nAutism diagnosis [Yes/No] and linguistic disorders in 24 hours to 2 month after vaccination\nBlood and urine sample tests out of norms at the time of research [3-10 years old];"
  },
  {
    "objectID": "slides/w01a-introduction.html#wakefield1998.-findings-contributions",
    "href": "slides/w01a-introduction.html#wakefield1998.-findings-contributions",
    "title": "Introduction. Organisation. Setup.",
    "section": "Wakefield et al. (1998). Findings / Contributions",
    "text": "Wakefield et al. (1998). Findings / Contributions\n\n\nVaccinated group (treatment):\n\n\nin 8/12 children behavioral problems [autism].\nintestinal abnormalities and chronic inflammation;\nurine tests are significantly different from “control” group;\n\n\n\nCounter factual (12 children from the population with the same age/gender):\n\n\nno disorders.\nno abnormalities or inflammation;\ngood tests;\n\n\n\n\n\n\nContributes with a theoretical mechanism of the consequences of the MMR vaccination."
  },
  {
    "objectID": "slides/w01a-introduction.html#wakefield1998.-conclusions-impact",
    "href": "slides/w01a-introduction.html#wakefield1998.-conclusions-impact",
    "title": "Introduction. Organisation. Setup.",
    "section": "Wakefield et al. (1998). Conclusions / Impact:",
    "text": "Wakefield et al. (1998). Conclusions / Impact:\n\n\nConclusion:\n\nMumps or MMR vaccination causes autism.\n\nSocietal impact\n\nMisinformation;\nVaccine hesitancy and anti-vax movements;\nFT: The true toll of the antivax movement;\nSocietal segregation. Which Americans are against the jab?"
  },
  {
    "objectID": "slides/w01a-introduction.html#what-is-wrong-with-wakefield1998",
    "href": "slides/w01a-introduction.html#what-is-wrong-with-wakefield1998",
    "title": "Introduction. Organisation. Setup.",
    "section": "What is wrong with Wakefield et al. (1998) ?",
    "text": "What is wrong with Wakefield et al. (1998) ?\n\n\nAny guesses?\nLet us use the whiteboard…\nBy the end of this lecture, we should (ideally) be able to reason about this!"
  },
  {
    "objectID": "slides/w01a-introduction.html#lecturers-eduard-bukin",
    "href": "slides/w01a-introduction.html#lecturers-eduard-bukin",
    "title": "Introduction. Organisation. Setup.",
    "section": "Lecturers: Eduard Bukin",
    "text": "Lecturers: Eduard Bukin\nData science enthusiast, econometrics practitioner. PhD Student.\n\nInstitute of Agricultural Policy and Market Research\n\n2015 – MS in Rural Development:\n\nGhent University, Belgium\n\nResearch interests:\n\nAgricultural structures and productivity\nLand and labor in agriculture\nSpatial econometrics"
  },
  {
    "objectID": "slides/w01a-introduction.html#lecturers-christoph-funk",
    "href": "slides/w01a-introduction.html#lecturers-christoph-funk",
    "title": "Introduction. Organisation. Setup.",
    "section": "Lecturers: Christoph Funk",
    "text": "Lecturers: Christoph Funk\nChristoph.Funk@wirtschaft.uni-giessen.de. Website.\nPost Doc.\n\nCenter for international Development and Environmental Research (ZEU) Justus Liebig Universität\n\n2020 - PhD in economics from Justus Liebig University Giessen\nResearch interests:\n\nSDG monitoring\nClimate change vulnerability\nAdaptation strategies\nEnergy economics\nEconometric modelling"
  },
  {
    "objectID": "slides/w01a-introduction.html#lecturers-vladimir-otrachshenko",
    "href": "slides/w01a-introduction.html#lecturers-vladimir-otrachshenko",
    "title": "Introduction. Organisation. Setup.",
    "section": "Lecturers: Vladimir Otrachshenko",
    "text": "Lecturers: Vladimir Otrachshenko\nVladimir.Otrachshenko@zeu.uni-giessen.de. Website.\nSenior Researcher.\n\nCenter for international Development and Environmental Research (ZEU) Justus Liebig Universität\n\n2013 - PhD in Economics from Nova School of Business and Economics, Lisbon, Portugal\nResearch interests:\n\nEnvironmental and Resource Economics\nClimate Change\nHealth and Population Economics"
  },
  {
    "objectID": "slides/w01a-introduction.html#your-turn",
    "href": "slides/w01a-introduction.html#your-turn",
    "title": "Introduction. Organisation. Setup.",
    "section": "Your turn!",
    "text": "Your turn!\n\n\nPlease introduce yourself\n\nWhat is your name?\nWhere do you come from?\nWhat do you study?\n\nWhat is your background?\nWhat are your expectations?"
  },
  {
    "objectID": "slides/w01a-introduction.html#course-structure-14",
    "href": "slides/w01a-introduction.html#course-structure-14",
    "title": "Introduction. Organisation. Setup.",
    "section": "Course structure (1/4)",
    "text": "Course structure (1/4)\nIn presence\n\nEvery Wednesday 14:00 - 18:00, Room: Senckenbergstr. 03, 216 (Ze-PC2)"
  },
  {
    "objectID": "slides/w01a-introduction.html#course-structure-24",
    "href": "slides/w01a-introduction.html#course-structure-24",
    "title": "Introduction. Organisation. Setup.",
    "section": "Course structure (2/4)",
    "text": "Course structure (2/4)\nOnline resources:\n\nIlias is used for materials dissemination.\nCode and materials mirror: github.com/EBukin/mp223-2023-aem-R-public\nStudIP is only used for announcements."
  },
  {
    "objectID": "slides/w01a-introduction.html#course-structure-34",
    "href": "slides/w01a-introduction.html#course-structure-34",
    "title": "Introduction. Organisation. Setup.",
    "section": "Course structure (3/4)",
    "text": "Course structure (3/4)\n\nLectures in person\n\nSlides on Ilias.\n\nExercises (in class or at home)\n\nSometimes have pre requisites (watch a video, read a paper)\nSometimes require preparation in advance;\nSometimes are also pre-recorded."
  },
  {
    "objectID": "slides/w01a-introduction.html#course-structure-44",
    "href": "slides/w01a-introduction.html#course-structure-44",
    "title": "Introduction. Organisation. Setup.",
    "section": "Course structure (4/4)",
    "text": "Course structure (4/4)\nExamination:\n\n60% written exam (90 minutes in the time of examination session)\n40% practical homework.\n\n2 short individual assignments (20% + 20%).\nStudents are expected to develop econometric analysis in R and submit it’s results (and code).\nHomework build on the materials form the class and demonstrate ability to perform selected econometric analysis independently."
  },
  {
    "objectID": "slides/w01b-selection-bias.html#potential-outcomes-framework",
    "href": "slides/w01b-selection-bias.html#potential-outcomes-framework",
    "title": "Selection Bias and how to fight it",
    "section": "Potential outcomes framework",
    "text": "Potential outcomes framework\n\nSeminal papers are: (Holland, 1986; Rubin, 1974, 1977).\n\n\n\\(D_i=\\{0,1\\}\\) is a treatment that causes a change in the actual outcome \\(Y_i\\);\n\n\n\\(Y_{\\color{Red}{0}i}\\) and \\(Y_{\\color{Red}{1}i}\\) are two potential outcomes for an individual \\(i\\);\n\n\n\\[\n\\text{Potential outcome} =\n\\begin{cases}\nY_{\\color{Red}{1}i} & \\text{ if } D_i=1 \\\\\nY_{\\color{Red}{0}i} & \\text{ if } D_i=0\n\\end{cases}\n\\]\n\n\nPotential outcome is what we would measure if we could go back in time and change a person’s treatment status.\n\n\nWe observe:\n\n\\(Y_i = Y_{\\color{Red}{1}i}\\), when \\(D_i=1\\)\n\\(Y_i = Y_{\\color{Red}{0}i}\\), when \\(D_i=0\\)"
  },
  {
    "objectID": "slides/w01b-selection-bias.html#causal-effect-of-a-treatment",
    "href": "slides/w01b-selection-bias.html#causal-effect-of-a-treatment",
    "title": "Selection Bias and how to fight it",
    "section": "Causal effect of a treatment",
    "text": "Causal effect of a treatment\n\nIs the difference between two potential outcomes:\n\n\n\\[\n\\rho = Y_{1i} - Y_{0i}\n\\]\n\n\nDepending on what we observe as a factual:\n\n\n\n\\(Y_{0i}\\) is the counterfactual for \\(Y_{1i}\\)\n\n\n\n\n\\(Y_{1i}\\) is the counterfactual for \\(Y_{0i}\\)"
  },
  {
    "objectID": "slides/w01b-selection-bias.html#actual-outcome",
    "href": "slides/w01b-selection-bias.html#actual-outcome",
    "title": "Selection Bias and how to fight it",
    "section": "Actual outcome",
    "text": "Actual outcome\n\\[\n\\begin{align}\nY_i & =\n\\begin{cases}\nY_{1i} & \\text{ if } D_i=1 \\\\\nY_{0i} & \\text{ if } D_i=0\n\\end{cases} \\\\\n& = Y_{0i} + \\rho \\cdot D_i\n\\end{align}\n\\]\n\nActual outcome: single path that an individual walks."
  },
  {
    "objectID": "slides/w01b-selection-bias.html#conditional-expectation",
    "href": "slides/w01b-selection-bias.html#conditional-expectation",
    "title": "Selection Bias and how to fight it",
    "section": "Conditional Expectation:",
    "text": "Conditional Expectation:\nActual outcome is Conditional on treatment\n\n\nWe use “|” to denote conditional on something (\\(D_i\\));\n\n\n\n\n\\([Y_i|D_i]\\) means actual outcome \\(Y_i\\) conditional on \\(D_i\\)\n\n\n\n\nWhen \\(D_i=0, \\;\\;\\; [Y_i|D_i=0] = Y_{0i} + 0\\)\n\n\n\n\nWhen \\(D_i=1, \\;\\;\\; [Y_i|D_i=1] = Y_{0i} + (Y_{1i} - Y_{0i})\\)"
  },
  {
    "objectID": "slides/w01b-selection-bias.html#comparing-of-two-individuals",
    "href": "slides/w01b-selection-bias.html#comparing-of-two-individuals",
    "title": "Selection Bias and how to fight it",
    "section": "Comparing of two individuals",
    "text": "Comparing of two individuals\n\nAsk: what is the difference between two individuals. \\([Y_{0i}|\\color{Green}{D_i = 1}] \\ne \\color{Red}{[Y_i|D_i=0]}\\) or \\([Y_{0i}|\\color{Green}{D_i = 1}] \\ne [Y_{0i}|\\color{Red}{D_i=0}]\\)\n\nWe have two individuals with actually observed outcomes:\n\n\\([Y_i|D_i=1]\\)\n\\([Y_i|D_i=0]\\)\n\n\n\\[\n\\begin{align}\n\\underbrace{[Y_i|D_i=1] - [Y_i|D_i=0]}_\\text{Observed difference}\n\\end{align}\n\\]\n\n\n\\[\n\\begin{align}\n\\underbrace{\\color{Blue}{[Y_i|D_i=1]} - \\color{Red}{[Y_i|D_i=0]}}_\\text{Observed difference} &\n= \\underbrace{\\color{Blue}{[Y_i|D_i=1]} - [Y_{0i}|\\color{Green}{D_i = 1}]}_\\text{Average treatment effect on treated} \\\\\n& + \\underbrace{[Y_{0i}|\\color{Green}{D_i = 1}] - \\color{Red}{[Y_i|D_i=0]}}_\\text{Selection bias}\n\\end{align}\n\\]\n\n\nDifferences between treated and not treated are always affected by the Selection Bias"
  },
  {
    "objectID": "slides/w01b-selection-bias.html#the-origin-of-the-selection-bias",
    "href": "slides/w01b-selection-bias.html#the-origin-of-the-selection-bias",
    "title": "Selection Bias and how to fight it",
    "section": "The origin of the selection bias",
    "text": "The origin of the selection bias\nIn:\n\\[\n\\begin{align}\n\\color{Blue}{[Y_i|D_i=1]} - \\color{Red}{[Y_i|D_i=0]} &\n= \\color{Blue}{[Y_i|D_i=1]} -  [Y_{0i}|\\color{Green}{D_i = 1}] \\\\\n& +  [Y_{0i}|\\color{Green}{D_i = 1}] - \\color{Red}{[Y_i|D_i=0]}\n\\end{align}\n\\]\n\n\nactual outcome of NO-treatment in NOT treated is the same as potential outcome of NO-treatment in NOT treated.\n\n\\[\n\\color{Red}{[Y_i|D_i=0]} = [Y_{0i}|\\color{Red}{D_i=0}]\n\\]\n\n\nHowever:\n\npotential outcome of NO-treatment in treated is NOT the same as potential outcome of NO-treatment in NOT treated.\n\n\\[\n[Y_{0i}|\\color{Green}{D_i = 1}] \\ne [Y_{0i}|\\color{Red}{D_i = 0}]\n\\]"
  },
  {
    "objectID": "slides/w01b-selection-bias.html#selection-bias-in-the-nutshell",
    "href": "slides/w01b-selection-bias.html#selection-bias-in-the-nutshell",
    "title": "Selection Bias and how to fight it",
    "section": "Selection bias in the nutshell",
    "text": "Selection bias in the nutshell\n\n\nSelection bias arises from the lack of compatibility:\n\nwhen we compare phenomena that are not comparable;\nlike apples and oranges;\n\nAs every person is unique, the potential outcomes of treatment and no treatment are different between people.\nSelection bias arises when we do not have the Ceteris Paribus!"
  },
  {
    "objectID": "slides/w01b-selection-bias.html#two-households",
    "href": "slides/w01b-selection-bias.html#two-households",
    "title": "Selection Bias and how to fight it",
    "section": "Two households",
    "text": "Two households\n\n\n\nHousehold (\\(i=1\\)):\n\n\nReceived CCT (treatment):\n\n\\(D_i=1\\)\n\nObserved wasting: 2 SD;\n\nActual outcome = Potential outcome when treated;\n\\[\n[Y_{i=1}|D_{i=1}=1] \\\\= [Y_{1,i=1}|D_{i=1}=1] = 2\n\\]\n\n\n\n\n\n\nHousehold (\\(i=2\\)):\n\n\nNo CCT (no treatment):\n\n\\(D_i=0\\)\n\nWasting: 1 SD;\n\nActual outcome = Potential outcome when NOT treated;\n\\[\n[Y_{i=2}|D_{i=2}=0] \\\\= [Y_{0,i=2}|D_{i=2}=0] = 1\n\\]\n\n\n\n\n\n\n\nWhat are the similarities between two household?\nWhat are the differences?"
  },
  {
    "objectID": "slides/w01b-selection-bias.html#comparing-two-households-the-difference",
    "href": "slides/w01b-selection-bias.html#comparing-two-households-the-difference",
    "title": "Selection Bias and how to fight it",
    "section": "Comparing two households (the difference)",
    "text": "Comparing two households (the difference)\n\\[\n\\begin{align}\n\\text{Difference between two HH} & = \\\\\n& = [Y_{i=1}|D_{i=1}=1] - [Y_{i=2}|D_{i=2}=0] \\\\\n& = 2 - 1 \\\\\n& = 1\n\\end{align}\n\\]\n\n\nIs this an Average Treatment Effect (ATE)?\n\n\n\n\nIs this the Treatment Effect on Treated?\n\n\n\n\nVote?"
  },
  {
    "objectID": "slides/w01b-selection-bias.html#potential-outcomes-of-two-households-12",
    "href": "slides/w01b-selection-bias.html#potential-outcomes-of-two-households-12",
    "title": "Selection Bias and how to fight it",
    "section": "Potential Outcomes of two households (1/2)",
    "text": "Potential Outcomes of two households (1/2)\n\n\n\n\n\n\n\n\n\n\n\nHousehold \\(i=1\\)\n(Treated)\nHousehold \\(i=2\\)\n(Not treated)\n\n\n\n\nPotential outcome without CCT:\n\\(Y_{0i}\\)\n-2\nNot observed\n1\nObserved\n\n\nPotential outcome with CCT:\n\\(Y_{1i}\\)\n2\nObserved\n1\nNot observed\n\n\nActual treatment status:\n\\(D_{i}\\)\n1\nObserved\n0\nObserved\n\n\nActual outcome:\n\\(Y_{i}\\)\n2\nObserved\n1\nObserved\n\n\nTreatment effect on treated:\n\\(Y_{1i} - Y_{0i}\\)\n\\(2-(-2)=4\\)\nNot observed\n\\(1-1=0\\)\nNot observed\n\n\n\n\nWhat can we actually observe? What (from above) can we NOT observe in the real world?\n\n\nPotential outcomes are different for two HHs.\n\n\n\nWhy are they different that so?"
  },
  {
    "objectID": "slides/w01b-selection-bias.html#potential-outcomes-of-two-households-22",
    "href": "slides/w01b-selection-bias.html#potential-outcomes-of-two-households-22",
    "title": "Selection Bias and how to fight it",
    "section": "Potential Outcomes of two households (2/2)",
    "text": "Potential Outcomes of two households (2/2)\n\n\n\n\n\n\n\n\n\n\n\nHousehold \\(i=1\\)\n(Treated)\nHousehold \\(i=2\\)\n(Not treated)\n\n\n\n\nPotential outcome without CCT:\n\\(Y_{0i}\\)\n-2\n1\n\n\nPotential outcome with CCT:\n\\(Y_{1i}\\)\n2\n1\n\n\nActual treatment status:\n\\(D_{i}\\)\n1\n0\n\n\nActual outcome:\n\\(Y_{i}\\)\n2\n1\n\n\nEffect of treatment on treated:\n\\(Y_{1i} - Y_{0i}\\)\n\\(2-(-2)=4\\)\n\\(1-1=0\\)\n\n\n\n\nTreatment causes different effects:\n\n\n\nEffect of treatment on the treated:\n\n\\[\n\\text{ETT}_i = Y_{1i} - Y_{0i}\\\\\n\\text{ETT}_1 = Y_{1,i=1} - Y_{0,i=1} = 2 - (-2) = 4\\\\\n\\text{ETT}_2 = Y_{1,i=2} - Y_{0,i=2} = 1 - 1 = 0\n\\]\n\n\n\nAverage Treatment Effect (ATE):\n\n\\[\n\\text{ATE} = E[\\text{ETT}_i] = E[Y_{1i} - Y_{0i}] = \\frac{1}{2}[2-(-2) + 1 - 1] = 2\n\\]\n\n\nBut the difference in the actual outcomes shows a biased effect: \\(Y_{1,i=1}-Y_{0,i=0} = 2-1 = 0\\)"
  },
  {
    "objectID": "slides/w01b-selection-bias.html#comparing-two-households-the-bias",
    "href": "slides/w01b-selection-bias.html#comparing-two-households-the-bias",
    "title": "Selection Bias and how to fight it",
    "section": "Comparing two households (the bias)",
    "text": "Comparing two households (the bias)\n\\[\n\\begin{align}\n\\text{Diff. between two HH} & = [Y_{i=1}|D_{i=1}=1] - [Y_{i=2}|D_{i=2}=0] \\\\\n& = \\underbrace{[Y_{i=1}|D_{i=1}=1] - [Y_{0,i=1}|D_{i=1}=1]}_\\text{Effect of treatment on treated (ETT)} \\\\\n& \\qquad + \\underbrace{[Y_{0,i=1}|D_{i=1}=1] - [Y_{i=2}|D_{i=2}=0]}_\\text{Selection bias}\n\\end{align}\n\\]\n\n\\[\n\\begin{align}\n\\text{Diff. between two HH} & = \\underbrace{2 - (-2)}_\\text{4} + \\underbrace{(-2) - 1}_\\text{-3} = \\underbrace{1}_\\text{Biased effect}\n\\end{align}\n\\]\n\n\nTrue causal effect of treatment on treated in \\(i=1\\) is \\(4\\) and in \\(i=2\\) is \\(0\\);\nBut comparing two groups does not reveal this!\nInstead, we have a negative bias of a HH who is OK without treatment.\nSuch negative selection bias can mask true causal effect completely."
  },
  {
    "objectID": "slides/w01b-selection-bias.html#average-treatment-effect",
    "href": "slides/w01b-selection-bias.html#average-treatment-effect",
    "title": "Selection Bias and how to fight it",
    "section": "Average treatment effect",
    "text": "Average treatment effect\n\nThe constant-effects assumption!\n\\[\nY_{1i} = \\rho + Y_{0i}\n\\]\n\nThe treatment has a constant effect \\(\\rho\\) on all individuals.\n\n\nWhen we reveal the causal effect \\(\\rho\\) we assume that it is constant for all treated an not treated individuals."
  },
  {
    "objectID": "slides/w01b-selection-bias.html#bias-of-the-group-means-difference",
    "href": "slides/w01b-selection-bias.html#bias-of-the-group-means-difference",
    "title": "Selection Bias and how to fight it",
    "section": "Bias of the group means difference",
    "text": "Bias of the group means difference\n\\[\n\\begin{align}\n\\text{Diff. in group means} & = \\color{Blue}{E[Y_{i}|D_{i}=1]} - \\color{Red}{E[Y_{i}|D_{i}=0]} \\\\ \\\\\n& = \\underbrace{\\color{Blue}{E[Y_{i}|D_{i}=1]} - \\color{Green}{E[Y_{0i}|D_{i}=1]}}_\\text{Average treatment effect (ATE)} \\\\\n& \\qquad + \\underbrace{\\color{Green}{E[Y_{0i}|D_{i}=1]} - \\color{Red}{E[Y_{i}|D_{i}=0]}}_\\text{Selection bias}\n\\end{align}\n\\]\n\n\\[\n\\begin{align}\n\\text{Diff. in group means} & = \\rho + \\text{Selection bias}\n\\end{align}\n\\]\n\n\nBecause:\n\naverage outcome of no-treatment in not treatedis the same as average potential outcome of no-treatment in not treated.\n\n\\[\n\\color{Red}{E[Y_i|D_i=0]} = \\color{Red}{E[Y_{0i}|D_{i}=0]}\n\\]\n\n\nHowever:\n\naverage potential outcome of no-treatment in treated is NOT the same as average potential outcome of no-treatment in not treated.\n\n\\[\n\\color{Green}{E[Y_{0i}|D_{i}=1]} \\ne \\color{Red}{E[Y_{0i}|D_{i}=0]}\n\\]"
  },
  {
    "objectID": "slides/w01b-selection-bias.html#selection-bias-conclusions",
    "href": "slides/w01b-selection-bias.html#selection-bias-conclusions",
    "title": "Selection Bias and how to fight it",
    "section": "Selection Bias: conclusions",
    "text": "Selection Bias: conclusions\n\n\nMortal enemy of the causal inference: leads to interpreting naive difference as causal effects.\nExists in any comparison, where there are systematic difference between the groups compared.\nAppears when individuals are being “selected” for the comparison, thus:\n\non average, individuals are not the same, or\nthere is no Ceteris Paribus."
  },
  {
    "objectID": "slides/w01b-selection-bias.html#what-is-wrong-with-wakefield1998",
    "href": "slides/w01b-selection-bias.html#what-is-wrong-with-wakefield1998",
    "title": "Selection Bias and how to fight it",
    "section": "What is wrong with (Wakefield et al., 1998)?",
    "text": "What is wrong with (Wakefield et al., 1998)?\nRemember: (1) treatment is the MMR vaccination. (2) outcome is the autism/inflation.\n\n\n\nTreated group:\n\n12 children with bad symptoms (all are vaccinated);\n\nCounter factual:\n\n12 “random” children with same age and gender (not vaccinated);\n\nComparison:\n\nMean prevalence of autism and inflammations by MMR vaccination status;\n\n\n\n\n\nWhat is wrong?\n\nIdeas?…\nCounterfactuals are not the same as treated.\nCounterfactuals do not represent the population (nearly everyong is vaccinated against MMR).\nSelection bias affects the means comparison.\nDoes not reviel the causal effect."
  },
  {
    "objectID": "slides/w01b-selection-bias.html#solutions-to-the-selection-bias",
    "href": "slides/w01b-selection-bias.html#solutions-to-the-selection-bias",
    "title": "Selection Bias and how to fight it",
    "section": "Solutions to the Selection Bias",
    "text": "Solutions to the Selection Bias\nThe only way to resolve it is:\n\nTo design the research so that the design eliminates the selection bias.\n\n\nUsing econometrics enhanced by statistics and appropriate research design, we can:\n\n\n\nensure the Ceteris Paribus by\n“making” groups of comparison as similar as possible\nand controlling the differences."
  },
  {
    "objectID": "slides/w01b-selection-bias.html#furious-five-econometric-methods",
    "href": "slides/w01b-selection-bias.html#furious-five-econometric-methods",
    "title": "Selection Bias and how to fight it",
    "section": "Furious Five econometric methods",
    "text": "Furious Five econometric methods\nFor ensuring ceteris paribus econometricians use:\n\n\nRandom assignment (RCT)\nRegression\nInstrumental Variable\nDifference-in-difference\nRegression Discontinuity Design"
  },
  {
    "objectID": "slides/w01b-selection-bias.html#random-assignment-13",
    "href": "slides/w01b-selection-bias.html#random-assignment-13",
    "title": "Selection Bias and how to fight it",
    "section": "Random assignment (1/3)",
    "text": "Random assignment (1/3)\nRemember, we had to non randomly assigned groups:\n\\[\n\\text{Diff. in group means}  =  \\color{Blue}{E[Y_{i}|D_{i}=1]} - \\color{Red}{E[Y_{i}|D_{i}=0]} \\\\\n= \\underbrace{E[Y_{1i}|\\color{Blue}{D_{i}=1}] - E[Y_{0i}|\\color{Green}{D_{i}=1}]}_\\text{Average causal effect}\n+ \\underbrace{E[Y_{0i}|\\color{Green}{D_{i}=1}] - E[Y_{0i}|\\color{Red}{D_{i}=0}]}_\\text{Selection bias},\n\\]\n\n\\[\n\\text{where} \\; \\color{Red}{E[Y_{i}|D_{i}=0]} = E[Y_{0i}|\\color{Red}{D_{i}=0}], \\; \\text{but} \\\\ E[Y_{0i}|\\color{Green}{D_{i}=1}] \\ne E[Y_{0i}|\\color{Red}{D_{i}=0}]\n\\]\n\n\nThe random assignment of \\(D_i\\) makes:\n\\[\nE[Y_{0i}|\\color{Green}{D_{i}=1}] = E[Y_{0i}|\\color{Red}{D_{i}=0}]\n\\]"
  },
  {
    "objectID": "slides/w01b-selection-bias.html#random-assignment-23",
    "href": "slides/w01b-selection-bias.html#random-assignment-23",
    "title": "Selection Bias and how to fight it",
    "section": "Random assignment (2/3)",
    "text": "Random assignment (2/3)\nNow with \\(E[Y_{0i}|\\color{Green}{D_{i}=1}] = E[Y_{0i}|\\color{Red}{D_{i}=0}]\\), we have:\n\n\\[\n\\begin{align}\n\\text{Diff. in group means}  & =  \\underbrace{E[Y_{1i}|\\color{Blue}{D_{i}=1}] - E[Y_{0i}|\\color{Red}{D_{i}=0}]}_{\\rho} \\\\\n& + \\underbrace{E[Y_{0i}|\\color{Green}{D_{i}=1}] - E[Y_{0i}|\\color{Red}{D_{i}=0}]}_{0}\n\\end{align}\n\\]\n\n\n\\[\n\\begin{align}\n& = \\overbrace{\\underbrace{E[Y_{0i} + \\rho|\\color{Blue}{D_{i}=1}]}_{E[Y_{1i}|\\color{Blue}{D_{i}=1}]} - E[Y_{0i}|\\color{Red}{D_{i}=0}]}^{\\rho}\n\\end{align}\n\\]\n\n\n\\[\n\\begin{align}\n& = \\rho + \\underbrace{E[Y_{0i}|\\color{Green}{D_{i}=1}] - E[Y_{0i}|\\color{Red}{D_{i}=0}]}_{0} \\\\\n\\end{align}\n\\]\n\n\n\\[\n\\begin{align}\n& = \\rho = \\text{ATE}\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/w01b-selection-bias.html#random-assignment-33",
    "href": "slides/w01b-selection-bias.html#random-assignment-33",
    "title": "Selection Bias and how to fight it",
    "section": "Random assignment (3/3)",
    "text": "Random assignment (3/3)\nWith random assignment,\n\n\\(\\text{Diff. in group means} = \\text{ATE}\\) works because of\nthe Law of Large Numbers (LLN).\n\n\n\nProvided that two samples are large enough for the LLN to work\n\n\n\n\nLLN ensures that such large random samples asymptotically approximate the population.\n\n\n\n\nThus, samples are also same between each other.\n\n\n\n\nResearch Design + LLN, ensures that \\(E[Y_{0i}|\\color{Green}{D_{i}=1}] = E[Y_{0i}|\\color{Red}{D_{i}=0}]\\)."
  },
  {
    "objectID": "slides/w01b-selection-bias.html#key-redings-on-rct",
    "href": "slides/w01b-selection-bias.html#key-redings-on-rct",
    "title": "Selection Bias and how to fight it",
    "section": "Key redings on RCT",
    "text": "Key redings on RCT\nKey papers:\n\n\nUsing Randomization in Development Economics Research: A Toolkit\n\n\nThe Econometrics of Randomized Experiments\n\n\nRCT in development:\n\n\nConditional Cash Transfers : Reducing Present and Future Poverty (book)\n\n\nCausal Inference in Statistics, Social, and Biomedical Sciences An Introduction (book)\n\n\nRCT in economics:\n\nSee reference in (Angrist & Pischke, 2009, Chapter 2; 2014, Chapter 1; Athey & Imbens, 2017)"
  },
  {
    "objectID": "slides/w01b-selection-bias.html#rcts-criticism",
    "href": "slides/w01b-selection-bias.html#rcts-criticism",
    "title": "Selection Bias and how to fight it",
    "section": "RCT’s criticism",
    "text": "RCT’s criticism\nThis research design is not ultimate and one needs to be critical to it as well!\nSee (Deaton & Cartwright, 2018):\n\nRCT does not ultimately equalize everything because of the sample size and randomization strategy. Other factors (covariates) must be controlled for.\nExternal validity of the RCT could be very limited.\nBuilding RCT should include prior knowledge.\nRCT’s finding may be contemporary."
  },
  {
    "objectID": "slides/w01b-selection-bias.html#homework",
    "href": "slides/w01b-selection-bias.html#homework",
    "title": "Selection Bias and how to fight it",
    "section": "Homework",
    "text": "Homework\nWatch these videos on youtube and read\n\n\nVideo 1: Selection Bias or this link: https://youtu.be/6YrIDhaUQOE\n\nVideo 2: Randomized Trials or this link: https://youtu.be/eGRd8jBdNYg\n\n\nRead:\n(Angrist & Pischke, 2014, Chapter 1; optional Angrist & Pischke, 2009, Chapter 2)\nFinish the in-class exercise"
  },
  {
    "objectID": "slides/w01b-selection-bias.html#homwork-discuss-the-following-causal-questions",
    "href": "slides/w01b-selection-bias.html#homwork-discuss-the-following-causal-questions",
    "title": "Selection Bias and how to fight it",
    "section": "Homwork: Discuss the following causal questions",
    "text": "Homwork: Discuss the following causal questions\n\nMany farms, particularly in Europe, are small and owned and run by families. In Eastern Europe, Soviet legacy left large scale farm. Are small scale farms more efficient and productive then the large one?\nWhat is the effect of Conditional Cash Transfers rather then support with goods in kind on the extreme poverty in developing countries?\nWhat is the effect of Global Food prices surge on the number of the food security in the low income countries?\n\n\nFor each of these questions answer the following:\n\nWhat is the outcome variable and what is the treatment?\nDefine the counterfactual outcomes \\(Y_{0i}\\) and \\(Y_{1i}\\) .\nWhat plausible causal channel(s) runs directly from the treatment to the outcome?\nWhat are possible sources of selection bias in the raw comparison of outcomes by treatment status?\nDoes the selection bias overestimate the difference or underestimates it?"
  },
  {
    "objectID": "slides/w01b-selection-bias.html#takeaways",
    "href": "slides/w01b-selection-bias.html#takeaways",
    "title": "Selection Bias and how to fight it",
    "section": "Takeaways:",
    "text": "Takeaways:\n\nAverage treatment effect (ATE) and Effect of Treatment on Treated (ETT);\nSelection bias of means comparison and the lack of Ceteris Paribus;\nWhat is the Ceteris Paribus (Watch a video is needed);\nActual and potential outcomes framework;\nFactual and Counterfactual;\nRole of research design in fighting with the selection bias;\nFurious Five econometric methods;\nRandom assignment;"
  },
  {
    "objectID": "slides/w01c-r-rstudio-intro.html#about-the-r-and-rstudio",
    "href": "slides/w01c-r-rstudio-intro.html#about-the-r-and-rstudio",
    "title": "Practical Exercise 1 + R and R Studio Setup",
    "section": "About the R and RStudio",
    "text": "About the R and RStudio\nTo learn more about R and RStudio, use R For Data Science (Second Edition), specifically R4DS Chapter “Intro”, section 1.4 Prerequisites\n\n\n\n\nR is an open-source statistical programming language\nR is also an environment for statistical computing and graphics\nIt’s easily extensible with packages, see: https://cran.r-project.org/\n\n\n\n\nRStudio is an IDE (integrated development environment) for R\nRStudio is not a requirement for programming with R, but it’s commonly used by R programmers and data scientists https://www.rstudio.com/"
  },
  {
    "objectID": "slides/w01c-r-rstudio-intro.html#r-rstudio-installation",
    "href": "slides/w01c-r-rstudio-intro.html#r-rstudio-installation",
    "title": "Practical Exercise 1 + R and R Studio Setup",
    "section": "R + RStudio Installation",
    "text": "R + RStudio Installation\n\nDownload R from here: cran.r-project.org\nInstall R by double click on the installation file and clicking next…\nDownload free version of R Studio from here: rstudio.com\nInstall RStudio by double click on the installation file and clicking next…\nCheck that RStudio has been installed by typing “RStudio” in the start menu or Windows search.\n(Optional) Check that R has been installed by typing “R x” in the start menu or Windows search."
  },
  {
    "objectID": "slides/w01c-r-rstudio-intro.html#r-and-r-studio-introduction-and-interface",
    "href": "slides/w01c-r-rstudio-intro.html#r-and-r-studio-introduction-and-interface",
    "title": "Practical Exercise 1 + R and R Studio Setup",
    "section": "R and R Studio: introduction and interface",
    "text": "R and R Studio: introduction and interface\n\n\n\n\n\nConsole (left bottom) - to type the R code into\nEditor (left top) - to write and SAVE scripts, analysis and documentation.\nEnvironment (right top) - overview of the r session and objects in there\nPlots, Files and Viewer (right bottom) - files navigation, plots export and inspection."
  },
  {
    "objectID": "slides/w01c-r-rstudio-intro.html#scripts-editor",
    "href": "slides/w01c-r-rstudio-intro.html#scripts-editor",
    "title": "Practical Exercise 1 + R and R Studio Setup",
    "section": "Scripts editor",
    "text": "Scripts editor"
  },
  {
    "objectID": "slides/w01c-r-rstudio-intro.html#r-markdown-editor",
    "href": "slides/w01c-r-rstudio-intro.html#r-markdown-editor",
    "title": "Practical Exercise 1 + R and R Studio Setup",
    "section": "R Markdown editor",
    "text": "R Markdown editor"
  },
  {
    "objectID": "slides/w01c-r-rstudio-intro.html#application-exercise-01",
    "href": "slides/w01c-r-rstudio-intro.html#application-exercise-01",
    "title": "Practical Exercise 1 + R and R Studio Setup",
    "section": "Application Exercise 01",
    "text": "Application Exercise 01\n\n\nIn the classroom:\nTurn on your PC\nUse these log in and password.\n\n\n\n\n\n\nImportant\n\n\nLogin: ZH-user-pcl\nPassword: V5-senc!3ken\n\n\n\n\n\nGo to: bit.ly/3GD8Oap\nScroll and download ex01-rct.zip.\nDON’T OPEN IT! setup your working folders first (next slide)."
  },
  {
    "objectID": "slides/w01c-r-rstudio-intro.html#setup-working-folders",
    "href": "slides/w01c-r-rstudio-intro.html#setup-working-folders",
    "title": "Practical Exercise 1 + R and R Studio Setup",
    "section": "Setup working folders",
    "text": "Setup working folders\nNavigate to your user folder: C > Users > Name of your user account;\n\n\nCreate there a course folder names {your initial}-mk223-2023.\n\nUse it for your course for all in-class work;\non my pc the course folder is called eb-mk223-2023;\nthe full path is C:\\Users\\ZH-user-pcl\\eb-mk223-2023;\n\n\n\n\n\nPaste ex01-rct.zip from downloads to the course folder;\n\n\n\n\nUnzip ex01-rct.zip into ex01-rct;"
  },
  {
    "objectID": "slides/w01c-r-rstudio-intro.html#launch-the-r-studio-from-the-project-ae01-soft-intro-to-r",
    "href": "slides/w01c-r-rstudio-intro.html#launch-the-r-studio-from-the-project-ae01-soft-intro-to-r",
    "title": "Practical Exercise 1 + R and R Studio Setup",
    "section": "Launch the R Studio from the project “ae01-soft-intro-to-R”",
    "text": "Launch the R Studio from the project “ae01-soft-intro-to-R”\n\n\nNavigate to ex01-rct in your course folder\n\n\n\n\nOpen ex01-rct.Rproj that has R studio icon and .Rproj extension:\n\n\n\n\n\n\n\n\n\n\n\nMP223-Applied Econometrics (SoSe 2023). Author: Eduard Bukin (2023). | GitHub | StudIP | ILIAS."
  },
  {
    "objectID": "slides/w02-homework-supplement.html#table-1.3-check-and-balance",
    "href": "slides/w02-homework-supplement.html#table-1.3-check-and-balance",
    "title": "Week 02. Homework supplementary materials",
    "section": "Table 1.3 Check and balance",
    "text": "Table 1.3 Check and balance"
  },
  {
    "objectID": "slides/w02-homework-supplement.html#table-1.4-intervention-evaluation",
    "href": "slides/w02-homework-supplement.html#table-1.4-intervention-evaluation",
    "title": "Week 02. Homework supplementary materials",
    "section": "Table 1.4 Intervention evaluation",
    "text": "Table 1.4 Intervention evaluation"
  },
  {
    "objectID": "slides/w02-homework-supplement.html#references",
    "href": "slides/w02-homework-supplement.html#references",
    "title": "Week 02. Homework supplementary materials",
    "section": "References",
    "text": "References\n\n\n\nMP223-Applied Econometrics (SoSe 2023). Author: Eduard Bukin (2023). | GitHub | StudIP | ILIAS.\n\n\n\nAngrist, J. D., & Pischke, J.-S. (2014). Mastering’metrics: The path from cause to effect. Princeton University Press."
  },
  {
    "objectID": "slides/w03-mlr-part1.html#recap-ceteris-paribus",
    "href": "slides/w03-mlr-part1.html#recap-ceteris-paribus",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Recap: Ceteris Paribus",
    "text": "Recap: Ceteris Paribus\nFill in page 1 here: bit.ly/41R1YpL"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#recap-multiple-regression",
    "href": "slides/w03-mlr-part1.html#recap-multiple-regression",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Recap: Multiple regression",
    "text": "Recap: Multiple regression\nFill in page 2 here: bit.ly/41R1YpL"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#hedonic-model",
    "href": "slides/w03-mlr-part1.html#hedonic-model",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Hedonic Model",
    "text": "Hedonic Model\nTo understand where the regression equation comes from, let us follow an example of:\nHedonic Model\n\nAny idea what this is?"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#hedonic-model-overview",
    "href": "slides/w03-mlr-part1.html#hedonic-model-overview",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Hedonic Model overview",
    "text": "Hedonic Model overview\nHedonic prices is an econometric approach of quantifying monetary values of differentiated characteristics (\\(x_i\\)) of goods and services, which are subjects of economic exchange (and stochastic variation \\(u\\)).\n\\[\n\\text{Price} = f(x_1, x_2, \\cdots , x_i, u)\n\\]\n\nFor example, agricultural land has such characteristics as: …\n\n\nLand quality (location, slope, soil salinity, nutrient content, irrigation availability, rainfall, climate) environmental limitation, farmers’ accessibility, and other.\n\n\nHedonic equation is based on the theory that takes its roots to supply and demand."
  },
  {
    "objectID": "slides/w03-mlr-part1.html#supply-and-demand-theory-a-structural-approach",
    "href": "slides/w03-mlr-part1.html#supply-and-demand-theory-a-structural-approach",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Supply and demand theory: a structural approach",
    "text": "Supply and demand theory: a structural approach\n\n\n\nDemand function:\n\\[Q_{t}^D = \\alpha_0 + \\alpha_1 P_t + u_{1t}, \\;\\; \\text{with} \\; \\alpha_1 < 0\\]\n\n\nSupply function:\n\\[Q_{t}^S = \\beta_0 + \\beta_1 P_t + u_{2t}, \\;\\; \\text{with} \\; \\beta_1 >0\\]\n\n\nEquilibrium condition:\n\\[\nQ_{t}^S = Q_{t}^D\n\\]"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#hedonic-land-prices-model",
    "href": "slides/w03-mlr-part1.html#hedonic-land-prices-model",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Hedonic land prices model",
    "text": "Hedonic land prices model\nRelies on the partial equilibrium framework (Palmquist, 1989), where\n\n\\(R(\\cdot)\\) - realized land price (rental of sales), are modeled from two sides:\n\n\n\n\nSupply\n\nLand owner whats to maximize own profit from renting land out.\nOwner’s offer function:\n\n\n\n\\[\n\\phi(\\hat{z}, \\tilde{z}, \\pi^{S^{'}}, r, \\beta)\n\\]\n\n\n\\(\\hat{z}\\) - land characteristics exogenous to land owner;\n\\(\\tilde{z}\\) - land characteristics in control of land owner;\n\n\n\nDemand\n\nFarmer whats to maximize agricultural profit from land\nFarmer’s bid function”\n\n\n\n\\[\n\\pi^{S^{'}} + C(\\hat{z}, \\tilde{z}, r, \\beta)\n\\]\n\n\n\\(r\\) - inputs prices;\n\\(\\beta\\) - technologies and opportunities such as credit availability;\n\\(\\pi^{S^{'}}\\) - expected profit of agricultural producers from land;"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#structural-model-of-the-realized-prices",
    "href": "slides/w03-mlr-part1.html#structural-model-of-the-realized-prices",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Structural model of the realized prices",
    "text": "Structural model of the realized prices\n\nSupply-demand equilibrium:\n\\[\\phi(\\hat{z}, \\tilde{z}, \\pi^{S^{'}}, r, \\beta) = R = \\pi^{S^{'}} + C(\\hat{z}, \\tilde{z}, r, \\beta)\\]\n\n\nObserved prices \\(R\\) are the equilibrium between bid and offer (demand and supply).\n\n\n\nThis is called a Structural Model"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#econometric-modell",
    "href": "slides/w03-mlr-part1.html#econometric-modell",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Econometric modell",
    "text": "Econometric modell\nTo explain causes behind price changes, we can deconstruct structural model into reduced form equations, which can be estimated:\n\n\nReduced form supply side\n\\[\nR = \\phi(\\hat{z}, \\tilde{z}, \\pi^{S^{'}}, r, \\beta) + e\n\\]\n\nReduced form demand side\n\\[\nR = \\pi^{S^{'}} + C(\\hat{z}, \\tilde{z}, r, \\beta) + e\n\\]\n\n\n\nWhen we run a hedonic prices model\n\\[R = R(\\hat{z}, \\tilde{z}, \\pi^{S^{'}}, r, \\beta)\\]\nwe estimate one of the reduced forms and select independent variables based on the hedonic prices theory (differentiated land qualities)."
  },
  {
    "objectID": "slides/w03-mlr-part1.html#relevance-of-the-theory",
    "href": "slides/w03-mlr-part1.html#relevance-of-the-theory",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Relevance of the theory",
    "text": "Relevance of the theory\n\n\nTheory provides a rationale behind causal relationship\n\n\\[R = R(\\hat{z}, \\tilde{z}, \\pi^{S^{'}}, r, \\beta)\\]\n\n\n\nTheory suggests a functional form.\n\n\n\n\nTheory stipulates the dependent variable.\n\n\n\n\nTheory specifies key determinants of the outcome:\n\nAKA what our regressors/independnet variables."
  },
  {
    "objectID": "slides/w03-mlr-part1.html#what-are-the-differentiated-land-characteristics",
    "href": "slides/w03-mlr-part1.html#what-are-the-differentiated-land-characteristics",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "What are the differentiated land characteristics?",
    "text": "What are the differentiated land characteristics?\nIn \\(R = R(\\hat{z}, \\tilde{z}, \\pi^{S^{'}}, r, \\beta)\\), what are these independent and dependent variables?\n\\(\\tilde{z}\\) Affected by land owner:\n\n\nTo enroll for subsidies or not.\nTo install irrigation or not.\nFertilize land\nImprove the landscape\n\n\n\\(\\hat{z}\\) Not affected by land owner:\n\n\nWeather\nLocation\nRestrictions"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#problem",
    "href": "slides/w03-mlr-part1.html#problem",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Problem",
    "text": "Problem\nWe would like to assess the effect of the “Conservation Reserve Program” (CPR) on the agricultural land prices in Minnesota in 2002-2011.\nConservation Reserve Program\n\nis a subsidy\nobligates farms NOT TO GROW ANY CROPS on the enrolled land\npays monetary compensation in exchange;"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#regression-equation",
    "href": "slides/w03-mlr-part1.html#regression-equation",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Regression equation",
    "text": "Regression equation\n\\[\n\\log (\\text{acrePrice}) = \\beta_0 +  \\beta_1 \\text{crpPct} + \\log(\\beta_2 \\text{acres}) + \\beta_3 \\text{region} \\\\\n+ \\beta_4 \\text{year} + \\beta_4 \\text{tillable} + \\beta_5 \\text{productivity} + \\beta_6 \\text{improvements}  + e\n\\]\n\nacrePrice - sale price in dollars per acre;\nacres - size of the farm in acres;\nregion - region in the state Minnesota;\nyear - year of the land sales translation;\ncrpPct - the percentage of all farm acres enrolled in CRP;\ntillable - percentage of farm acreage that is rated arable by the assessor;\nproductivity - average agronomic productivity scaled 1 to 100, with larger numbers for more productive land;\nimprovements - percentage of property value due to improvements (infrastructure)"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#regression-results",
    "href": "slides/w03-mlr-part1.html#regression-results",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Regression results",
    "text": "Regression results\n\n\n\n\n\n\n\n\n\n\n\nlog(Price per acre)\nlog(Price per acre)\n\n\n\n\nIntercept\n6.099 *** (0.051)\n6.507 *** (0.034)\n\n\nSubsidy (0|1)\n-0.00370 *** (0.00027)\n-0.00488 *** (0.00017)\n\n\nArea (log), acres\n-0.0587 *** (0.0058)\n-0.0913 *** (0.0051)\n\n\nTillable area, % (0-100)\n0.00421 *** (0.00036)\n0.00550 *** (0.00018)\n\n\nImprovements, (0-100)\n0.01567 *** (0.00062)\n0.01414 *** (0.00033)\n\n\nProductivity, (0-100)\n0.0094 *** (0.0004)\n\n\n\nWest Central (0|1)\n0.633 *** (0.016)\n0.746 *** (0.011)\n\n\nCentral (0|1)\n0.885 *** (0.018)\n1.041 *** (0.013)\n\n\nSouth West (0|1)\n0.734 *** (0.016)\n1.019 *** (0.011)\n\n\nSouth Central (0|1)\n0.854 *** (0.017)\n1.191 *** (0.011)\n\n\nSouth East (0|1)\n0.897 *** (0.018)\n1.234 *** (0.012)\n\n\nNum.Obs.\n8770\n17441\n\n\nR2 Adj.\n0.700\n0.646\n\n\n\nNote: ^^ Year-specific dummy variables are omitted. Heteroscedasticity consistent standard errors are reported in parentheses. P-values are coded as: * p=0.05, ** p=0.01, *** p<0.001"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#goodness-of-fit-r2-adjusted",
    "href": "slides/w03-mlr-part1.html#goodness-of-fit-r2-adjusted",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Goodness of fit: \\(R^2\\) adjusted",
    "text": "Goodness of fit: \\(R^2\\) adjusted\nShows the share of variance explained by a model adjusted to the number of independent variables.\n\n\nIf the number of independent variables increases, but variables do not explain \\(y\\) better, \\(R^2\\) adjusted could shrink to 0 or a negative number.\nWe generally want to have it as high as possible, however!\n\\(R^2\\) adjusted has nothing to do with the coefficients’ significance and their causal meaning.\nIf the goal of regression is to explain causes, rather than predict outcomes, \\(R^2\\) adjusted has not much relevance."
  },
  {
    "objectID": "slides/w03-mlr-part1.html#let-us-summarize-the-causes-of-wine-prices",
    "href": "slides/w03-mlr-part1.html#let-us-summarize-the-causes-of-wine-prices",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Let us summarize the causes of wine prices",
    "text": "Let us summarize the causes of wine prices\n\n\nAny guesses?\nWeather\nAny unobserved characteristics?\n\nArt of the winemaker\nStorage\nWay of drinking"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#let-us-see-what-regression-tells-us-about-wine",
    "href": "slides/w03-mlr-part1.html#let-us-see-what-regression-tells-us-about-wine",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Let us see what regression tells us about wine",
    "text": "Let us see what regression tells us about wine\n\\[\nPrice = \\beta_0 + \\beta_1 \\text{Age} + \\beta_2 \\text{AGST} + \\beta_3 \\text{HarvestRain} + \\beta_4 \\text{WinterRain} + e\n\\]\nWhat are our expectations about signs of \\(\\beta\\) s?\nVariable\n\nPrice: average market price for Bordeaux vintages according to a series of auctions (USD). The price is relative to the price of the 1961 vintage, regarded as the best one ever recorded.\nWinterRain: winter rainfall (in mm).\nAGST: Average Growing Season Temperature (in Celsius degrees).\nHarvestRain: harvest rainfall (in mm).\nAge: age of the wine, measured in 1983 as the number of years stored in a cask."
  },
  {
    "objectID": "slides/w03-mlr-part1.html#let-us-regress-wine-in-the-class",
    "href": "slides/w03-mlr-part1.html#let-us-regress-wine-in-the-class",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Let us regress wine in the class",
    "text": "Let us regress wine in the class\n\nExercise: 03-wine-regression.Rmd"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#wage-education-equation",
    "href": "slides/w03-mlr-part1.html#wage-education-equation",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Wage ~ Education: equation",
    "text": "Wage ~ Education: equation\n\\[\nY_i = \\alpha + \\beta \\, P_i + e_i,\n\\]\nwhere:\n\n\\(Y_i\\) is wage in Euro per week\n\\(P_i\\) is education in years\n\\(\\alpha\\) is the intercept\n\\(\\beta\\) is the slope or causal effect of interest"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#wage-education.",
    "href": "slides/w03-mlr-part1.html#wage-education.",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Wage ~ Education.",
    "text": "Wage ~ Education.\n\nCausal directed acyclic graph."
  },
  {
    "objectID": "slides/w03-mlr-part1.html#relationship",
    "href": "slides/w03-mlr-part1.html#relationship",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Relationship",
    "text": "Relationship\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpret the effect of education on wage.\nIs this a causal effect on education on wage?\n\nExplain why?"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#wage-education-is-there-a-ceteris-paribus",
    "href": "slides/w03-mlr-part1.html#wage-education-is-there-a-ceteris-paribus",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Wage ~ Education: Is there a ceteris paribus?",
    "text": "Wage ~ Education: Is there a ceteris paribus?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression accounts for the observed (included) confounders by attributing variance in \\(y\\) to the variance in \\(x\\) (variable of interest) and \\(u\\) (control variables)."
  },
  {
    "objectID": "slides/w03-mlr-part1.html#why-regression-1",
    "href": "slides/w03-mlr-part1.html#why-regression-1",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Why regression?",
    "text": "Why regression?\n\n\nIn multiple regression, Ceteris Paribus is achieved by introducing control variables (\\(A_i\\)).\n\n\n\n\\[\nY_i = \\alpha + \\beta \\, P_i + \\gamma \\, A_i + e_i,\n\\]\n\n\n\nRegression controls the variance in \\(Y_i\\) with observed \\(P_i\\) and \\(A_i\\).\nIn the context of the variable of interest (\\(P_i\\)):\n\nRegression controls other variables (\\(A_i\\)) fixed,\nensuring that \\(\\beta\\) reviels causal effect Ceteris Paribus."
  },
  {
    "objectID": "slides/w03-mlr-part1.html#wage-education-really-is-there-a-ceteris-paribus",
    "href": "slides/w03-mlr-part1.html#wage-education-really-is-there-a-ceteris-paribus",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Wage ~ Education: Really? Is there a ceteris paribus?",
    "text": "Wage ~ Education: Really? Is there a ceteris paribus?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNot every confounded could be observed or measures.\nThere are unobserved ones:\n\n…\n\n\n\n\n\nAbility, attitude, effort.\n\n\n\n\n\nWhen a confounded correlated with outcome \\(Cov(c,y) \\ne 0\\) and other regressors \\(Cov(c,u) \\ne 0\\) and \\(Cov(c,x) \\ne 0\\).\n\n\n\nEstimates of \\(\\beta\\) and \\(\\gamma\\) are no longer ceteris paribus!\n\nThey are biased: \\(\\tilde{\\beta}\\) and \\(\\tilde{\\gamma}\\)"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#ovb-the-long-model",
    "href": "slides/w03-mlr-part1.html#ovb-the-long-model",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "OVB: The long model",
    "text": "OVB: The long model\nSupposed that our ideal regression\n\nthe true model / population regression / long model is:\n\n\n\\[\nY_i = \\alpha ^ l + \\beta ^ l P_i + \\gamma A_i + e^l_i,\n\\]\n\n\nWe cannot measure \\(A_i\\), but:\n\n\n\n\\(A_i\\) has a causal effect on \\(Y_i\\): \\((E[Y_i|A_i] \\ne 0)\\), and\n\n\n\n\n\\(A_i\\) correlated with \\(P_i\\): \\((E[P_i|A_i] \\ne 0)\\):"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#ovb-the-short-model",
    "href": "slides/w03-mlr-part1.html#ovb-the-short-model",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "OVB: The short model",
    "text": "OVB: The short model\nBecause of the omitted variable,\n\nwe cannot estimate the long model.\n\n\nInstead, we estimate a short model:\n\n\n\\[\nY_i = \\alpha ^ s + \\beta^s P_i + e^s_i\n\\]\n\n\nwhere omitted variable is implicit in the residuals:\n\n\n\\[\ne^s_i = e^l_i + A_i\n\\]"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#bias-of-variable-omission",
    "href": "slides/w03-mlr-part1.html#bias-of-variable-omission",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Bias of variable omission",
    "text": "Bias of variable omission\nOmitted variable causes bias of all estimates!\n\nThis bias can be measured as \\(\\text{OVB}\\):\n\n\n\\[\n\\text{OVB} = \\beta^s - \\beta^l\n\\]\nTo be continued on the OVB in another week"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#how-does-regression-fights-selection-bias",
    "href": "slides/w03-mlr-part1.html#how-does-regression-fights-selection-bias",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "How does regression fights selection bias?",
    "text": "How does regression fights selection bias?\n\nAny ideas?\n\n\nAny ideas?\n\n\nWe include control variables to reduce or defeat the omitted variable bias."
  },
  {
    "objectID": "slides/w03-mlr-part1.html#introduction",
    "href": "slides/w03-mlr-part1.html#introduction",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Introduction",
    "text": "Introduction\n\nSee Wooldridge (2020)\n\nWe use data from (Blackburn and Neumark, 1992) on wage determinants. Variables present are:\n\n\\(wage\\) - monthly earnings in USD;\n\\(educ\\) - years of education;\n\\(exper\\) - years of experiences;\n\\(black\\) - dummy variable representing individuals which are not Caucasian;\n\\(female\\) - dummy variable representing females;\n\n\nOur goal is to identify the causal effect of education on wage estimating following equation:\n\\[\n\\text{wage} = \\beta_0 + \\beta_1 \\text{educ} + \\beta_2 \\text{exper} + \\beta_3 \\text{black} + \\beta_4 \\text{female} + e\n\\]"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#loading-data",
    "href": "slides/w03-mlr-part1.html#loading-data",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Loading data",
    "text": "Loading data\n\n\n\n\n\nRows: 526\nColumns: 7\n$ wage      <dbl> 3.10, 3.24, 3.00, 6.00, 5.30, 8.75, 11.25, 5.00, 3.60, 18.18…\n$ educ      <dbl> 11, 12, 11, 8, 12, 16, 18, 12, 12, 17, 16, 13, 12, 12, 12, 1…\n$ exper     <dbl> 2, 22, 2, 44, 7, 9, 15, 5, 26, 22, 8, 3, 15, 18, 31, 14, 10,…\n$ black     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ white     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ female    <dbl> 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, …\n$ caucasian <fct> yes, yes, yes, yes, yes, yes, yes, yes, yes, yes, yes, yes, …"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#exploratory-data-analysis-13",
    "href": "slides/w03-mlr-part1.html#exploratory-data-analysis-13",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Exploratory data analysis (1/3)",
    "text": "Exploratory data analysis (1/3)\n\n\n\n\n\n\nUnique (#)\nMissing (%)\nMean\nSD\nMin\nMedian\nMax\n\n\n\n\nwage\n241\n0\n5.9\n3.7\n0.5\n4.7\n25.0\n\n\neduc\n18\n0\n12.6\n2.8\n0.0\n12.0\n18.0\n\n\nexper\n51\n0\n17.0\n13.6\n1.0\n13.5\n51.0\n\n\nblack\n2\n0\n0.1\n0.3\n0.0\n0.0\n1.0\n\n\nwhite\n2\n0\n0.9\n0.3\n0.0\n1.0\n1.0\n\n\nfemale\n2\n0\n0.5\n0.5\n0.0\n0.0\n1.0"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#exploratory-data-analysis-23",
    "href": "slides/w03-mlr-part1.html#exploratory-data-analysis-23",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Exploratory data analysis (2/3)",
    "text": "Exploratory data analysis (2/3)"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#exploratory-data-analysis-33",
    "href": "slides/w03-mlr-part1.html#exploratory-data-analysis-33",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Exploratory data analysis (3/3)",
    "text": "Exploratory data analysis (3/3)"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#estimating-regression",
    "href": "slides/w03-mlr-part1.html#estimating-regression",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Estimating regression",
    "text": "Estimating regression\n\nmod1 <- lm(wage ~ educ + exper + black + female, data = wage_dta)\nmod1\n\n\n\n\nCall:\nlm(formula = wage ~ educ + exper + black + female, data = wage_dta)\n\nCoefficients:\n(Intercept)         educ        exper        black       female  \n   -1.71453      0.60175      0.06422     -0.08389     -2.15649"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#interpreting-the-results-1",
    "href": "slides/w03-mlr-part1.html#interpreting-the-results-1",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Interpreting the results (1)",
    "text": "Interpreting the results (1)\n\nsummary(mod1)\n\n\n\n\nCall:\nlm(formula = wage ~ educ + exper + black + female, data = wage_dta)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.3666 -1.9740 -0.4936  1.1248 14.8123 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.71453    0.76169  -2.251   0.0248 *  \neduc         0.60175    0.05135  11.718  < 2e-16 ***\nexper        0.06422    0.01041   6.168 1.39e-09 ***\nblack       -0.08389    0.44430  -0.189   0.8503    \nfemale      -2.15649    0.27060  -7.969 1.01e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.081 on 521 degrees of freedom\nMultiple R-squared:  0.3094,    Adjusted R-squared:  0.304 \nF-statistic: 58.34 on 4 and 521 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#interpreting-the-results-2-fancy-summary",
    "href": "slides/w03-mlr-part1.html#interpreting-the-results-2-fancy-summary",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Interpreting the results (2): fancy summary",
    "text": "Interpreting the results (2): fancy summary\n\n\n\n\n\n\nModel 1\n\n\n\n\n(Intercept)\n-1.715 (0.762)*\n\n\neduc\n0.602 (0.051)***\n\n\nexper\n0.064 (0.010)***\n\n\nblack\n-0.084 (0.444)\n\n\nfemale\n-2.156 (0.271)***\n\n\nNum.Obs.\n526\n\n\nR2 Adj.\n0.304\n\n\nF\n58.341"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#interpreting-the-results-3-effect-of-a-dummy-variables",
    "href": "slides/w03-mlr-part1.html#interpreting-the-results-3-effect-of-a-dummy-variables",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Interpreting the results (3): Effect of a dummy variables",
    "text": "Interpreting the results (3): Effect of a dummy variables\n\n\n\n\nggpredict(mod1, term = c(\"educ\")) \n\n# Predicted values of wage\n\neduc | Predicted |         95% CI\n---------------------------------\n   0 |     -1.66 | [-2.96, -0.37]\n   3 |      0.14 | [-0.86,  1.14]\n   5 |      1.35 | [ 0.54,  2.15]\n   7 |      2.55 | [ 1.93,  3.17]\n  10 |      4.35 | [ 3.99,  4.72]\n  12 |      5.56 | [ 5.29,  5.83]\n  14 |      6.76 | [ 6.46,  7.06]\n  18 |      9.17 | [ 8.56,  9.78]\n\nAdjusted for:\n*  exper = 17.02\n*  black =  0.10\n* female =  0.48\n\n\n\n\n\nggpredict(mod1, term = c(\"educ\", \"female\")) \n\n# Predicted values of wage\n\n# female = 0\n\neduc | Predicted |         95% CI\n---------------------------------\n   0 |     -0.63 | [-1.97,  0.71]\n   4 |      1.78 | [ 0.82,  2.74]\n   7 |      3.58 | [ 2.89,  4.27]\n  10 |      5.39 | [ 4.92,  5.85]\n  12 |      6.59 | [ 6.22,  6.96]\n  18 |     10.20 | [ 9.57, 10.84]\n\n# female = 1\n\neduc | Predicted |         95% CI\n---------------------------------\n   0 |     -2.79 | [-4.08, -1.49]\n   4 |     -0.38 | [-1.30,  0.54]\n   7 |      1.43 | [ 0.77,  2.08]\n  10 |      3.23 | [ 2.79,  3.67]\n  12 |      4.43 | [ 4.05,  4.82]\n  18 |      8.04 | [ 7.35,  8.73]\n\nAdjusted for:\n* exper = 17.02\n* black =  0.10"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#conclude",
    "href": "slides/w03-mlr-part1.html#conclude",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Conclude",
    "text": "Conclude\n\n\n\nIs model 1 a good predictor of wage based on education?\nIs the effect of education causal?\n\n\n\n\n\n\n\n\nModel 1\n\n\n\n\n(Intercept)\n-1.715 (0.762)*\n\n\neduc\n0.602 (0.051)***\n\n\nexper\n0.064 (0.010)***\n\n\nblack\n-0.084 (0.444)\n\n\nfemale\n-2.156 (0.271)***\n\n\nNum.Obs.\n526\n\n\nR2 Adj.\n0.304\n\n\nF\n58.341"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#elasticity-1",
    "href": "slides/w03-mlr-part1.html#elasticity-1",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Elasticity",
    "text": "Elasticity\nElasticity of \\(y\\) with response to \\(x\\) (\\(x\\) elasticity of \\(y\\)):\n\\[\n\\epsilon = \\frac{\\partial y / y}{\\partial x / x} =  \\frac{\\partial y}{\\partial x}  \\frac{x}{y}\n\\]\n\n\\[\n\\epsilon = \\frac{ \\frac{y_2 - y_1}{y_1} }{ \\frac{x_2 - x_1}{x_1}}\n\\]"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#elasticity-in-a-linear-model",
    "href": "slides/w03-mlr-part1.html#elasticity-in-a-linear-model",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Elasticity in a linear model",
    "text": "Elasticity in a linear model\n\\[\n\\text{wage} = \\beta_0 + \\beta_1 \\text{educ} + \\beta_2 \\text{exper} + \\beta_3 \\text{black} + e\n\\]\n\nLet us compute elasticity of \\(\\text{wage}\\) in response to \\(\\text{educ}\\):\n\n\n\\[\n\\epsilon_{\\text{wage},\\text{educ}} = \\frac{\\partial y}{\\partial x}  \\frac{x}{y},\n\\]\n\n\nwhere: \\(\\beta_1 = \\frac{\\partial y}{\\partial x}\\)\n\n\nTherefore, elasticity of wage depends on on the value of \\(x\\) and \\(y\\).\n\n\nWhen elasticity depends on a valued of another variable, we evaluate it at mean (or other quantiles) values of these variables."
  },
  {
    "objectID": "slides/w03-mlr-part1.html#eslimating-elasticity-in-a-linear-model-12",
    "href": "slides/w03-mlr-part1.html#eslimating-elasticity-in-a-linear-model-12",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Eslimating elasticity in a linear model (1/2)",
    "text": "Eslimating elasticity in a linear model (1/2)\n\n\n\n\n\n\n\n\nwage\neduc\n\n\n\n\nmean\n5.90\n12.56\n\n\nq2\n3.33\n12.00\n\n\nmedian\n4.65\n12.00\n\n\nq4\n6.88\n14.00"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#eslimating-elasticity-in-a-linear-model-12-1",
    "href": "slides/w03-mlr-part1.html#eslimating-elasticity-in-a-linear-model-12-1",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Eslimating elasticity in a linear model (1/2)",
    "text": "Eslimating elasticity in a linear model (1/2)\n\n\n\n\n\n\n\n\n\n\n\n\nElasticity at mean:\n\ncoef(mod1)[2] * (mean(wage_dta$educ) / mean(wage_dta$wage))\n\n    educ \n1.282137 \n\n\nElasticity at 2nd, 3rd and 4th quartiles:\n\ncoef(mod1)[2] * (fivenum(wage_dta$educ)[[2]] / fivenum(wage_dta$wage)[[2]])\n\n    educ \n2.168464 \n\ncoef(mod1)[2] * (fivenum(wage_dta$educ)[[3]] / fivenum(wage_dta$wage)[[3]])\n\n  educ \n1.5529 \n\ncoef(mod1)[2] * (fivenum(wage_dta$educ)[[4]] / fivenum(wage_dta$wage)[[4]])\n\n    educ \n1.224489"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#asymptotic-properties-of-the-ols-simplified",
    "href": "slides/w03-mlr-part1.html#asymptotic-properties-of-the-ols-simplified",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Asymptotic properties of the OLS (simplified)",
    "text": "Asymptotic properties of the OLS (simplified)\nOLS results with consistent (unbiased) and efficient estimates of population parameters when sample size is finite (\\(n \\rightarrow \\infty\\))\n\n\\(\\hat \\beta \\rightarrow \\beta\\)\n\\(Var(\\hat \\beta) \\rightarrow 0\\)\n\n\nWhen the sample size is finite and all Gauss-Markov assumptions are satisfied:\n\n\\(\\hat \\beta\\) - estimates vary from sample to sample, but\n\\(Var(\\hat \\beta)\\) is distributed according to the t - distribution.\nVariances of two estimates \\(Var(\\hat \\beta_1)\\) and \\(Var(\\hat \\beta_2)\\) are distributed according to the F - distribution.\n\n\n\nWhen GM assumptions are not satisfied:\n\nt and F distributions are no longer relevant and we cannot perform conduct inference."
  },
  {
    "objectID": "slides/w03-mlr-part1.html#linearity-meaning",
    "href": "slides/w03-mlr-part1.html#linearity-meaning",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Linearity: meaning",
    "text": "Linearity: meaning\n\n\nthe expected value of a dependent variable is a straight-line function of the independent variable\nIf linearity is violated:\n\nestimates are biased\ninappropriate representation of the dependent variable"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#linearity-detection",
    "href": "slides/w03-mlr-part1.html#linearity-detection",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Linearity: detection",
    "text": "Linearity: detection\n\n\nHow to detect a non-linearity?\n\nno accepted statistical tests, but\nthe visual inspection\n\nTypical plots:\n\nScatter plots of dependent and independent variables;\nobserved versus predicted/fitted values;\nresiduals versus predicted/fitted values;"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#linearity-resolutions",
    "href": "slides/w03-mlr-part1.html#linearity-resolutions",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Linearity: resolutions",
    "text": "Linearity: resolutions\n\n(non) linear transformation to the dependent and/or independent variables;\n\nit does change the way how we must interpret coefficients;\n\nfind a different independent variable;\npropose a different functional form;"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#common-linear-transformations",
    "href": "slides/w03-mlr-part1.html#common-linear-transformations",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Common linear transformations",
    "text": "Common linear transformations\n\n\nInteraction term: \\(y = \\beta_0 + \\beta_1 x_1 \\cdot x_2 + \\beta_2 x_3 + e\\)\nNatural logarithm: \\(\\log y = \\beta_0 + \\beta_2 \\log x_1 + \\beta_2 x_2 + \\beta_3 \\log x_3 + e\\)\nPower transformation and polynomial: \\(y = \\beta_0 + \\beta_2 x_1 ^ 2 + \\beta_2 x_2 ^ 3 + \\beta_3 \\sqrt x_3 + e\\)\n\nBox-Cox transformation.\nTailor expansion (Cobb-Douglas, Trans-log).\n\nReciprocal: \\(\\log y = \\beta_0 + \\beta_2 \\frac{1}{x_1} + \\beta_2 x_2 + \\beta_3 \\log x_3 + e\\)\nStandardized variables \\(\\frac{y - \\bar y}{S_y} = \\beta_0 + \\beta_1 \\frac{x_1 - \\bar x_1}{S_{x_1}} + \\beta_2 \\frac{x_2 - \\bar x_2}{S_{x_2}} + e\\)"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#log",
    "href": "slides/w03-mlr-part1.html#log",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Log",
    "text": "Log\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nDep. var.\nIndep. var.\nEquation\nSlope\nInterpretation\nElasticity\n\n\n\n\n\n\n\n\n\\(\\frac{\\partial y}{\\partial x}\\)\n\n\\(\\frac{\\partial y}{\\partial x} \\cdot \\frac{x}{y}\\)\n\n\nLevel - level\n\\(y\\)\n\\(y\\)\n\\(y=\\beta_0 + \\beta_1 x\\)\n\\(\\beta_1\\)\n\\(\\Delta y = \\beta_1 \\Delta x\\)\n\\(\\beta_1 \\frac{x}{y}\\)\n\n\nLevel - log\n\\(y\\)\n\\(\\log x\\)\n\\(\\log y=\\beta_0 + \\beta_1 x\\)\n\\(\\beta_1 y\\)\n\\(\\Delta y = (\\beta_1/100)\\% \\Delta x\\)\n\\(\\beta_1 x\\)\n\n\nLog - level\n\\(\\log y\\)\n\\(x\\)\n\\(y=\\beta_0 + \\beta_1 \\log x\\)\n\\(\\beta_1 \\frac{1}{x}\\)\n\\(\\% \\Delta y = 100 \\beta_1 \\Delta x\\)\n\\(\\beta_1 \\frac{1}{y}\\)\n\n\nLog - log\n\\(\\log y\\)\n\\(\\log x\\)\n\\(\\log y=\\beta_0 + \\beta_1 \\log x\\)\n\\(\\beta_1 \\frac{y}{x}\\)\n\\(\\% \\Delta y = \\% \\beta_1 \\Delta x\\)\n\\(\\beta_1\\)"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#log-key-limitations",
    "href": "slides/w03-mlr-part1.html#log-key-limitations",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Log: Key limitations",
    "text": "Log: Key limitations\n\n\\(\\log(0) = - \\infty\\);\nwhat is the \\(\\log(x)\\), when \\(x < 0\\)?"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#variables-standardiztion-to-the-standard-normal-distribution",
    "href": "slides/w03-mlr-part1.html#variables-standardiztion-to-the-standard-normal-distribution",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Variables standardiztion to the standard normal distribution",
    "text": "Variables standardiztion to the standard normal distribution\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nDep. var.\nIndep. var.\nEquation\nSlope\nInterpretation\nElasticity\n\n\n\n\n\n\n\n\n\\(\\frac{\\partial y}{\\partial x}\\)\n\n\\(\\frac{\\partial y}{\\partial x} \\cdot \\frac{x}{y}\\)\n\n\nStandardized variables\n\\(y^* = \\frac{y - \\bar y}{S_y}\\)\n\\(x^* = \\frac{x - \\bar x}{S_x}\\)\n\\(y^*=\\beta_0 + \\beta_1 x^*\\)\n\\(\\frac{\\partial y^*}{\\partial x^*}\\)\n\\(\\text{SD} \\Delta y = \\text{SD} \\beta_1 \\Delta x\\)\nDIY\n\n\n\n\nKey limitations:\n\nNot intuitive interpretation"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#reciprocal",
    "href": "slides/w03-mlr-part1.html#reciprocal",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Reciprocal",
    "text": "Reciprocal\n\n\n\n\n\n\n\n\n\n\n\nModel\nDep. var.\nIndep. var.\nEquation\nSlope\nElasticity\n\n\n\n\n\n\n\n\n\\(\\frac{\\partial y}{\\partial x}\\)\n\\(\\frac{\\partial y}{\\partial x} \\cdot \\frac{x}{y}\\)\n\n\nReciprocal\n\\(y\\)\n\\(\\frac{1}{x}\\)\n\\(y=\\beta_0 + \\beta_1 \\frac{1}{x}\\)\n\\(-\\beta_1 \\frac{1}{x^2}\\)\n\\(-\\beta_1 \\frac{1}{xy}\\)\n\n\n\nInterpretation:\n\nWhen \\(x\\) increases to infinity, \\(y\\) reaches asymptotically \\(\\beta_0\\)\n\nSee Gujarati (2004) Chapter 6.7 for more details on interpreting the reciprocal relationship."
  },
  {
    "objectID": "slides/w03-mlr-part1.html#linearity-in-the-wage-equation",
    "href": "slides/w03-mlr-part1.html#linearity-in-the-wage-equation",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Linearity in the wage equation",
    "text": "Linearity in the wage equation"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#wage-equaition-update",
    "href": "slides/w03-mlr-part1.html#wage-equaition-update",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Wage equaition update",
    "text": "Wage equaition update\n\n\n\nCall:\nlm(formula = log(wage) ~ educ + exper + black + female, data = wage_dta)\n\nCoefficients:\n(Intercept)         educ        exper        black       female  \n   0.483188     0.091192     0.009411    -0.009889    -0.343712  \n\n\n\nCall:\nlm(formula = log(wage) ~ educ + exper + black + female, data = wage_dta)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.89689 -0.26333 -0.03394  0.26654  1.28131 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.483188   0.106141   4.552 6.61e-06 ***\neduc         0.091192   0.007156  12.743  < 2e-16 ***\nexper        0.009411   0.001451   6.487 2.04e-10 ***\nblack       -0.009889   0.061913  -0.160    0.873    \nfemale      -0.343712   0.037709  -9.115  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4293 on 521 degrees of freedom\nMultiple R-squared:  0.3526,    Adjusted R-squared:  0.3476 \nF-statistic: 70.93 on 4 and 521 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#non-linearity-change",
    "href": "slides/w03-mlr-part1.html#non-linearity-change",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Non-linearity change",
    "text": "Non-linearity change"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#interpretation",
    "href": "slides/w03-mlr-part1.html#interpretation",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Interpretation",
    "text": "Interpretation\n\n\n\n\n\n\nModel 1 (level-level)\nModel 2 (log(wage)-level)\n\n\n\n\n(Intercept)\n-1.715 (0.762)*\n0.483 (0.106)***\n\n\neduc\n0.602 (0.051)***\n0.091 (0.007)***\n\n\nexper\n0.064 (0.010)***\n0.009 (0.001)***\n\n\nblack\n-0.084 (0.444)\n-0.010 (0.062)\n\n\nfemale\n-2.156 (0.271)***\n-0.344 (0.038)***\n\n\nNum.Obs.\n526\n526\n\n\nR2 Adj.\n0.304\n0.348\n\n\nF\n58.341\n70.934"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#collinearity-or-muticollinearity",
    "href": "slides/w03-mlr-part1.html#collinearity-or-muticollinearity",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Collinearity or Muticollinearity",
    "text": "Collinearity or Muticollinearity\n\nNo collinearity means\n\nnone of the regressors can be written as an exact linear combinations of some other regressors in the model.\n\nFor example:\n\nin \\(Y = \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3\\) ,\nwhere \\(X_3 = X_2 + X_1\\) ,\nall \\(X\\) are collinear."
  },
  {
    "objectID": "slides/w03-mlr-part1.html#consequence-of-collinearity",
    "href": "slides/w03-mlr-part1.html#consequence-of-collinearity",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Consequence of collinearity:",
    "text": "Consequence of collinearity:\n\nbiased estimates of the collinear variables\nover-significant results;"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#detection-of-collinearity",
    "href": "slides/w03-mlr-part1.html#detection-of-collinearity",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Detection of collinearity:",
    "text": "Detection of collinearity:\n\nScatter plot; Correlation matrix;\nModel specification;\nStep-wise regression approach;\nVariance Inflation Factor;"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#solution-to-collinearity",
    "href": "slides/w03-mlr-part1.html#solution-to-collinearity",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Solution to collinearity:",
    "text": "Solution to collinearity:\n\nRe specify the model;\nChoose different regressors;\nSee also:\n\nOverview: “Assumption AMLR.3 No Perfect Collinearity” in (Wooldridge, 2020) ;\nExamples of causes in Chapter 9.5 (Wooldridge, 2020) ;\nChapter 9.4-9.5 in (Weisberg, 2005);"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#perfect-collinearity-with-dummy-variables",
    "href": "slides/w03-mlr-part1.html#perfect-collinearity-with-dummy-variables",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Perfect collinearity with dummy variables",
    "text": "Perfect collinearity with dummy variables\n\nWe want to build a naive regression, where the wage is a function of sex (female and male):\n\\(\\text{wage} = \\beta_0 + \\beta_1 \\cdot \\text{female} + \\beta_2 \\cdot \\text{male}\\)\nThe data is fictional:\n\n\n\n\nRows: 14\nColumns: 3\n$ female <int> 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1\n$ male   <int> 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0\n$ wage   <dbl> 10.847522, 7.167989, 4.941890, 7.477957, 9.391538, 8.087289, 9.…"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#perfect-collinearity-with-dummy-variable-2",
    "href": "slides/w03-mlr-part1.html#perfect-collinearity-with-dummy-variable-2",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Perfect collinearity with dummy variable (2)",
    "text": "Perfect collinearity with dummy variable (2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel 1\nModel 2\nModel 3\nModel 4\nModel 5\n\n\n\n\n(Intercept)\n8.747*** (0.468)\n6.094*** (0.628)\n6.094*** (0.628)\n\n\n\n\nmale\n-2.652** (0.784)\n\n\n6.094*** (0.628)\n6.094*** (0.628)\n\n\nfemale\n\n2.652** (0.784)\n2.652** (0.784)\n8.747*** (0.468)\n8.747*** (0.468)\n\n\nNum.Obs.\n14\n14\n14\n14\n14\n\n\nR2\n0.488\n0.488\n0.488\n0.974\n0.974\n\n\nR2 Adj.\n0.446\n0.446\n0.446\n0.969\n0.969\n\n\n\nNote: ^^ Model 1: wage ~ maleModel 2: wage ~ femaleModel 3: wage ~ female + maleModel 4: wage ~ 0 + female + maleModel 5: wage ~ 0 + male + female"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#perfect-collinearity-with-dummy-variable-2-1",
    "href": "slides/w03-mlr-part1.html#perfect-collinearity-with-dummy-variable-2-1",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Perfect collinearity with dummy variable (2)",
    "text": "Perfect collinearity with dummy variable (2)\n\n\n\n\n\n\nModel 1\nModel 2\nModel 3\n\n\n\n\n(Intercept)\n8.747*** (0.468)\n6.094*** (0.628)\n6.094*** (0.628)\n\n\nmale\n-2.652** (0.784)\n\n\n\n\nfemale\n\n2.652** (0.784)\n2.652** (0.784)\n\n\nNum.Obs.\n14\n14\n14\n\n\nR2\n0.488\n0.488\n0.488\n\n\nR2 Adj.\n0.446\n0.446\n0.446\n\n\n\nNote: ^^ Model 1: wage ~ maleModel 2: wage ~ femaleModel 3: wage ~ female + male"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#perfect-collinearity-with-dummy-variable-2-2",
    "href": "slides/w03-mlr-part1.html#perfect-collinearity-with-dummy-variable-2-2",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Perfect collinearity with dummy variable (2)",
    "text": "Perfect collinearity with dummy variable (2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel 1\nModel 2\nModel 3\nModel 4\n\n\n\n\n(Intercept)\n8.747*** (0.468)\n6.094*** (0.628)\n6.094*** (0.628)\n\n\n\nmale\n-2.652** (0.784)\n\n\n6.094*** (0.628)\n\n\nfemale\n\n2.652** (0.784)\n2.652** (0.784)\n8.747*** (0.468)\n\n\nNum.Obs.\n14\n14\n14\n14\n\n\nR2\n0.488\n0.488\n0.488\n0.974\n\n\nR2 Adj.\n0.446\n0.446\n0.446\n0.969\n\n\n\nNote: ^^ Model 1: wage ~ maleModel 2: wage ~ femaleModel 3: wage ~ female + maleModel 4: wage ~ 0 + female + male"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#homeworks-1",
    "href": "slides/w03-mlr-part1.html#homeworks-1",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Homeworks:",
    "text": "Homeworks:\nWatch these videos on youtube and read\n\n\nVideo 1: Ceteris Paribus: Public vs. Private University or this link: https://youtu.be/iPBV3BlV7jk\nRe watch video 2: Selection Bias or this link: https://youtu.be/6YrIDhaUQOE\n\nRead:\n(Angrist & Pischke, 2014, Chapter 2; optional Angrist & Pischke, 2009, Chapter 3)\nDo:\nFollow pre-recorded videos in the order below. Please note that slides below supplement some of those practical works.\n\nEx.03a Regression basics\nEx.03b Wage education\nEx.03c Hedonic Land Prices Model"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#hw03a-regression-basics",
    "href": "slides/w03-mlr-part1.html#hw03a-regression-basics",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "HW03a Regression basics",
    "text": "HW03a Regression basics\n\\[\n\\pmb{y} = \\pmb{x}\\beta+\\pmb{e}\n\\]\nwhere\n\\[\n\\pmb{e} = \\pmb{y} - \\pmb{x}\\hat \\beta\n\\]\n\n\n\nDependent variable:\n\\[\n\\pmb{y} = \\begin{bmatrix}\ny_1 \\\\ y_2 \\\\ \\vdots \\\\ y_k\n\\end{bmatrix}\n\\]\n\n\n\nIndependent variables:\n\\[\n\\pmb{x} = \\begin{bmatrix}\n1 & x_{11} & x_{12} & \\dots & x_{1n} \\\\\n1 & x_{21} & x_{22} & \\dots & x_{2n} \\\\\n\\vdots  & \\vdots & \\vdots  & \\ddots & \\vdots  \\\\\n1 & x_{k1} & x_{k2} & \\dots&  x_{kn} \\\\\n\\end{bmatrix}\n\\]\n\n\n\n\\[\n\\hat \\beta = \\begin{bmatrix} \\hat \\beta_0 & \\hat \\beta_1 & \\hat \\beta_2 & \\cdots & \\hat \\beta_n \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#where-do-beta-come-from",
    "href": "slides/w03-mlr-part1.html#where-do-beta-come-from",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Where do \\(\\beta\\) come from?",
    "text": "Where do \\(\\beta\\) come from?\n\n\n\n\\[\n\\pmb{y} = \\pmb{x}\\hat\\beta\n\\]\n\n\n\\[\n\\pmb{x}^{T} \\pmb{y} = \\pmb{x}^{T} \\pmb{x_i}\\hat\\beta\n\\]\n\n\n\\[\n\\frac{1}{ \\pmb{x}^{T} \\pmb{x_i}} \\pmb{x}^{T} \\pmb{y} = \\frac{1}{ \\pmb{x}^{T} \\pmb{x_i}}\\pmb{x}^{T} \\pmb{x_i}\\hat\\beta\n\\]\n\n\n\\[\n(\\pmb{x}^{T} \\pmb{x_i}) ^ {-1} \\pmb{x}^{T} \\pmb{y} = \\hat\\beta\n\\]\n\n\n\nwhere:\n\n\\(\\pmb{x}^{T}\\) is the transposed matrix \\(\\pmb{x}\\)\n\\((\\cdot) ^ {-1}\\) is the inverse of a matrix"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#fitted-values",
    "href": "slides/w03-mlr-part1.html#fitted-values",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Fitted values",
    "text": "Fitted values\n\\[\n\\pmb{\\hat y} =\n\\pmb{x} \\hat\\beta =\n\\begin{bmatrix}\n1 & x_{11} & x_{12} & \\dots & x_{1n} \\\\\n1 & x_{21} & x_{22} & \\dots & x_{2n} \\\\\n\\vdots  & \\vdots & \\vdots  & \\ddots & \\vdots  \\\\\n1 & x_{k1} & x_{k2} & \\dots&  x_{kn} \\\\\n\\end{bmatrix}\n\\begin{bmatrix} \\hat \\beta_0 \\\\ \\hat \\beta_1 \\\\ \\hat \\beta_2 \\\\ \\vdots \\\\ \\hat \\beta_n \\end{bmatrix} =\n\\]\n\\[\n\\begin{bmatrix}\n\\beta_0 +  \\beta_1 x_{11} +  \\beta_2 x_{12} + \\dots + \\beta_n x_{1n} \\\\\n\\beta_0 +  \\beta_1 x_{21} +  \\beta_2 x_{22} + \\dots + \\beta_n x_{2n} \\\\\n\\vdots  \\\\\n\\beta_0 +  \\beta_1 x_{k1} +  \\beta_2 x_{k2} + \\dots + \\beta_n x_{kn} \\\\\n\\end{bmatrix} =\n\\begin{bmatrix} \\hat y_1 \\\\ \\hat y_2 \\\\ \\vdots \\\\ \\hat y_k\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#error-terms",
    "href": "slides/w03-mlr-part1.html#error-terms",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Error terms",
    "text": "Error terms\n\\[\n\\pmb{\\hat e} = \\pmb{y} - \\pmb{\\hat y} =\n\\begin{bmatrix} y_1 - \\hat y_1 \\\\ y_2 - \\hat y_2 \\\\ \\vdots \\\\ y_k - \\hat y_k \\end{bmatrix} =\n\\begin{bmatrix} \\hat e_1 \\\\ \\hat e_2 \\\\ \\vdots \\\\ \\hat e_k \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#standard-errors",
    "href": "slides/w03-mlr-part1.html#standard-errors",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Standard Errors",
    "text": "Standard Errors\n\n\nMeasure of variance in the estimated parameters \\(\\beta\\). Computed based on the Variance Covariance matrix\n\\[\nVar(\\hat \\beta) = (\\pmb{x}^T \\pmb{x})^{-1} \\hat \\sigma_e\n\\]\nwhere \\(\\hat \\sigma_e\\) is the estimate of the variance in error terms:\n\\[\n\\hat \\sigma_e = \\frac{\\pmb{\\hat e}^T\\pmb{\\hat e}}{n-r}\n\\]\n\\(n\\) - number of observations and \\(r\\) number of regressors including intercept.\n\nStandard Errors:\n\\[\n\\text{SE} = \\sqrt{\\text{diag}(Var(\\hat \\beta) )}\n\\]"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#why-do-we-need-standard-errors",
    "href": "slides/w03-mlr-part1.html#why-do-we-need-standard-errors",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Why do we need standard errors?",
    "text": "Why do we need standard errors?\n\n\nSE are needed for the inference!\nTo conclude about the population based on the sample regression results."
  },
  {
    "objectID": "slides/w03-mlr-part1.html#takeaways-1",
    "href": "slides/w03-mlr-part1.html#takeaways-1",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Takeaways",
    "text": "Takeaways\nGet comfortable with the terminology:\n\nControl variables for creating Ceteris Paribus\nSelection Bias in Regression:\n\nOVB;\nLong and Short models;\n\n\nRegression components and how do one produce them:\n\n\\(x\\), \\(y\\), \\(\\beta\\), standard errors.\n\nWhy assumptions are important?\nLinearity and how to detect it?\n\nLog transformation and its interpretation.\n\nWhat is perfect collinearity?"
  },
  {
    "objectID": "slides/w03-mlr-part1.html#references",
    "href": "slides/w03-mlr-part1.html#references",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "References",
    "text": "References\n\n\n\nMP223-Applied Econometrics (SoSe 2023). Author: Eduard Bukin (2023). | GitHub | StudIP | ILIAS.\n\n\n\nAngrist, J. D., & Pischke, J.-S. (2009). Mostly harmless econometrics. Princeton University Press. http://doi.org/10.1515/9781400829828\n\n\nAngrist, J. D., & Pischke, J.-S. (2014). Mastering’metrics: The path from cause to effect. Princeton University Press.\n\n\nAshenfelter, O., Ashmore, D., & Lalonde, R. (1995). Bordeaux wine vintage quality and the weather. CHANCE, 8(4), 7–14. http://doi.org/10.1080/09332480.1995.10542468\n\n\nGujarati, D. N. (2004). Basic econometrics. (F. edition, Ed.). McGraw Hill.\n\n\nPalmquist, R. B. (1989). Land as a differentiated factor of production: A hedonic model and its implications for welfare measurement. Land Economics, 65(1), 23. http://doi.org/10.2307/3146260\n\n\nWeisberg, S. (2005). Applied linear regression (Vol. 528). John Wiley & Sons.\n\n\nWooldridge, J. M. (2020). Introductory econometrics: A modern approach. South-Western. Retrieved from https://www.cengage.uk/shop/isbn/9781337558860"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#the-basic-theory-of-production-sadoulet1995-chapter-3",
    "href": "slides/w04-mlr-part2.html#the-basic-theory-of-production-sadoulet1995-chapter-3",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "The Basic Theory of Production (Sadoulet & de Janvry, 1995, Chapter 3)",
    "text": "The Basic Theory of Production (Sadoulet & de Janvry, 1995, Chapter 3)\n\nExplain why production function should be zero. - Intensive to use all variables factors to produce goods. - Leaving underutilized production factors makes production inefficient.\n\nFarm’s production function\n\n\\[\nh(q,x,z) = 0,\n\\]\nwhere:\n\n\n\n\\(q\\) - output quantities (agricultural produce)\n\\(x\\) - variable input quantities (fertilizers, labor, seed, water, …)\n\\(z\\) - fixed factors (private: land, equipment; public: infrastructure, extension; exogenous features …)"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#farms-optimization-problem",
    "href": "slides/w04-mlr-part2.html#farms-optimization-problem",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Farm’s optimization problem",
    "text": "Farm’s optimization problem\n\nAsk what is the optimization problem. Ask how the profit is being defined on the farm.\n\nPrices of inputs \\(w^{'}\\) and outputs \\(p^{'}\\) affect farmers decisions of:\n\nwhat and how much to produce \\(q\\), and\nwhat and how much to use as the input \\(x\\).\n\n\nFarm’s objective function is to maximize own profit:\n\n\n\\[\n\\max_{x,q} \\;\\;\\; p^{'}q-w^{'}x, \\\\ \\text{s.t.} \\;\\; h(q,x,z) = 0\n\\]"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#solution-to-the-producers-optimization-problem",
    "href": "slides/w04-mlr-part2.html#solution-to-the-producers-optimization-problem",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Solution to the producer’s optimization problem",
    "text": "Solution to the producer’s optimization problem\n\\[\n\\max_{x,q} \\;\\;\\; p^{'}q-w^{'}x, \\\\ \\text{s.t.} \\;\\; h(q,x,z) = 0\n\\]\nIs a set of input demand and output supply functions:\n\\[\n\\begin{cases}\nx = x(p,w,z), & \\text{factor demand function} \\\\\nq = q(p,w,z), & \\text{supply function}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#farms-optimization-problem-is",
    "href": "slides/w04-mlr-part2.html#farms-optimization-problem-is",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Farm’s optimization problem is",
    "text": "Farm’s optimization problem is\n\n\nStructural form.\n\\[\n\\max_{x,q} \\;\\;\\; p^{'}q-w^{'}x, \\\\ \\text{s.t.} \\;\\; h(q,x,z) = 0\n\\]\n\nModels that we can simulate with linear programming, structural equation modelling\n\n\nReduced forms.\n\\[\n\\begin{cases}\nx = x(p,w,z), & \\text{factor demand function} \\\\\nq = q(p,w,z), & \\text{supply function}\n\\end{cases}\n\\]\n\nModels that we estimate with regressions"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#taylor-expansion",
    "href": "slides/w04-mlr-part2.html#taylor-expansion",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Taylor Expansion",
    "text": "Taylor Expansion\nValue of any function could be approximated at an arbitrary point \\(x_0\\) with a Taylor expansion:\n\\[\nf(x) \\approx f(x_0) + f^{'}(x_0)(x - x_0) + \\frac{f^{''}(x_0)}{2!}(x - x_0)^2 + \\cdots .\n\\]\n\nFor example:\n\\[\ne^x = \\sum_{n=0}^{\\infty} \\frac{x^n}{n!} \\approx 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\cdots\n\\]\n\n\n\\[\ne^{0.4} \\approx 1 + .4 + \\frac{.4^2}{2!} + \\frac{.4^3}{3!} + \\frac{.4^4}{4!} + \\frac{.4^5}{5!} \\\\\n\\approx 1 + .4 + .16/2 + 0.064/6 + 0.0256/24 + 0.01024 / 120 \\\\\n\\approx  1 + .4 + .08 + 0.0106(6) + 0.00106(6) + 0.000106(6)\n\\]"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#first-order-taylor-approximation",
    "href": "slides/w04-mlr-part2.html#first-order-taylor-approximation",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "First order Taylor approximation",
    "text": "First order Taylor approximation\n\\(q \\approx f(x)\\) at \\(x_0\\).\n\\[\nf(x) \\approx  f(x_0) + f^{'}(x_0)(x-x_0) \\\\ \\approx\n\\underbrace{f(x_0) - f^{'}(x_0) x_0}_{\\alpha} + \\underbrace{f^{'}(x_0)}_{\\beta}\\, x\n\\]\n\nSimple regression: \\(q \\approx \\alpha + \\beta \\, x\\)\n\n\n\\(q \\approx f(x_1, x_2)\\) at \\(a, b\\)\n\\[\nf(x_1, x_2) \\approx\n\\underbrace{f(a,b) - f_{x_1}^{'}(a,b)a - f_{x_2}^{'}(a,b)b}_{\\alpha} + \\underbrace{f_{x_1}^{'}(a,b)}_{\\beta_1}\\, x_1 + \\underbrace{f_{x_2}^{'}(a,b)}_{\\beta_2}\\, x_2\n\\]\n\n\nMultiple regression: \\(q \\approx \\alpha + \\beta_1 \\, x_1 + \\beta_2 \\, x_2\\)"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#second-and-greater-order-taylor-approximation",
    "href": "slides/w04-mlr-part2.html#second-and-greater-order-taylor-approximation",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Second and greater order Taylor approximation",
    "text": "Second and greater order Taylor approximation\nResults in polynomial functions with interaction terms:\n\\[\nq \\approx \\alpha + \\beta_1 \\, x_1 + \\beta_2 \\, x_2 + \\beta_3 \\, x_1^2 + \\beta_4 \\, x_2^2 + \\beta_5 x_1 x_2\n\\]"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#non-linear-production-functions",
    "href": "slides/w04-mlr-part2.html#non-linear-production-functions",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Non-linear production functions",
    "text": "Non-linear production functions\nConsider\n\\[q = f(x_i)\\]\n(function of several variables \\(x_i\\)), where\n\\[x_i = e^{\\ln x_i}\\]\n\nThen,\n\\[\n\\ln q = \\ln f(e^{\\ln x_i}) \\; \\; \\; \\; \\text{or} \\; \\; \\; \\; \\ln q = g(\\ln x_i)\n\\]\n\n\nLet us approximate \\(\\ln q = g(\\ln x_i)\\) using the Taylor expansion."
  },
  {
    "objectID": "slides/w04-mlr-part2.html#fist-order-taylor-approximation",
    "href": "slides/w04-mlr-part2.html#fist-order-taylor-approximation",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Fist order Taylor approximation",
    "text": "Fist order Taylor approximation\n\n\\[\n\\ln q \\approx \\alpha + \\sum_i \\beta_i \\ln x_i\n\\]\n\n\nThis is a Cobb-Douglas production function: \\(q \\approx e^\\alpha \\prod_i x_i^{\\beta_i}\\)"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#second-order-taylor-approximation",
    "href": "slides/w04-mlr-part2.html#second-order-taylor-approximation",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Second order Taylor approximation",
    "text": "Second order Taylor approximation\nof \\(\\ln q = g(\\ln x_i)\\) around an arbitrary point \\(x_{i0} \\ne 0\\)\n\n\\[\n\\ln q \\approx \\alpha + \\sum_i \\beta_i  \\ln x_i + \\frac{1}{2}\\sum_{ij} \\gamma_{ij} \\ln x_{i} \\ln x_{j}\n\\]\n\n\nThis is a trans-log production function"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#problem-and-research-question",
    "href": "slides/w04-mlr-part2.html#problem-and-research-question",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Problem and research question",
    "text": "Problem and research question\n\n\n\n\nPolicy makers are considering to abolish the share-cropping land tenure system.\nAs an alternative either owner-cultivation with hired labor or pure rental relationship should be established.\nMain rationale is in the claim that sharecropping is an inefficient farm structure.\n\n\n\n\n\nTheoretical debate\nContract theory (Dasgupta, Knight, & Love, 1999) sees sharecropping as a form of the optimal risks and incentives sharing contract.\nIt is a second best contract in agriculture after pure rental of hired labor contract.\nSharecropping can be as efficient as rental/labor contracts, when interlinked labor, resources, credit and insurance markets fail.\n\n\n\n\n\nResearch question: Is there any negative effect of sharecropping on farms productivity?"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#how-to-answer-this-question",
    "href": "slides/w04-mlr-part2.html#how-to-answer-this-question",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "How to answer this question?",
    "text": "How to answer this question?\n\nLet us estimate a simple production function of a rice-producing farm.\n\\[\\text{output} = \\alpha + \\beta_1 \\text{status : share} + \\beta_2 \\text{status : mixed} + \\beta_3 \\text{land} + \\beta_4 \\text{labor} + e\\]\n\n\nwhere:\n\\(\\text{output}\\) is gross output of rice in kg\n\\(\\text{land}\\) the total area cultivated with rice, measured in hectares\n\\(\\text{labor}\\) total labor inputs (excluding harvest labor) in hours\n\\(\\text{status}\\) land tenure system on the farm: owner-operated, Share-cropping and mixed\n\n\nData set covers a sample of the rice-cultivating farmers in India. Source: (Feng & Horrace, 2010)."
  },
  {
    "objectID": "slides/w04-mlr-part2.html#data-1",
    "href": "slides/w04-mlr-part2.html#data-1",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Data (1)",
    "text": "Data (1)\n\n\n\n\nlibrary(tidyverse)\nlibrary(GGally)\nlibrary(modelsummary)\nglimpse(farm_dta)\n\nRows: 1,026\nColumns: 4\n$ output <int> 7980, 4083, 2650, 4500, 16300, 17424, 3840, 2800, 950, 240, 150…\n$ land   <dbl> 3.000, 2.000, 1.000, 2.000, 3.572, 3.572, 1.420, 1.420, 0.428, …\n$ labor  <int> 2915, 2155, 1075, 2091, 3889, 3519, 810, 855, 460, 109, 230, 18…\n$ status <fct> owner, owner, owner, owner, share, share, mixed, mixed, mixed, …"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#exploratory-staistics-1",
    "href": "slides/w04-mlr-part2.html#exploratory-staistics-1",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Exploratory staistics (1)",
    "text": "Exploratory staistics (1)\n\ndatasummary_skim(farm_dta, output = \"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnique (#)\nMissing (%)\nMean\nSD\nMin\nMedian\nMax\n\n\n\n\noutput\n482\n0\n1405.2\n1921.8\n42.0\n886.5\n20960.0\n\n\nland\n300\n0\n0.4\n0.5\n0.0\n0.3\n5.3\n\n\nlabor\n544\n0\n388.4\n484.2\n17.0\n252.0\n4774.0"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#exploratory-staistics-2",
    "href": "slides/w04-mlr-part2.html#exploratory-staistics-2",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Exploratory staistics (2)",
    "text": "Exploratory staistics (2)\n\ndatasummary_skim(farm_dta, output = \"markdown\", \"categorical\")\n\n\n\n\nstatus\nN\n%\n\n\n\n\nowner\n736\n71.7\n\n\nshare\n79\n7.7\n\n\nmixed\n211\n20.6"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#exploratory-staistics-3",
    "href": "slides/w04-mlr-part2.html#exploratory-staistics-3",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Exploratory staistics (3)",
    "text": "Exploratory staistics (3)\n\ndatasummary( (land  + labor  + output) ~ status * (mean + sd), farm_dta, output = \"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nowner / mean\nowner / sd\nshare / mean\nshare / sd\nmixed / mean\nmixed / sd\n\n\n\n\nland\n0.44\n0.58\n0.36\n0.56\n0.44\n0.39\n\n\nlabor\n395.42\n522.12\n315.51\n566.17\n391.44\n262.63\n\n\noutput\n1435.68\n2014.60\n1250.65\n2639.25\n1356.59\n1104.10"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#regression-and-interpretation",
    "href": "slides/w04-mlr-part2.html#regression-and-interpretation",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Regression and interpretation",
    "text": "Regression and interpretation\n\n\n\nfit1 <- lm(output ~ status + land + labor, \n           data = farm_dta)\n\nfit2_cd <- lm(log(output) ~ status + log(land) + log(labor), \n           data = farm_dta)\n\nmodelsummary(\n  list(`Linear prod.fn. (level-level)` = fit1,\n       `CD prod.fn. (log-log)` = fit2_cd),\n  estimate = \"{estimate} {stars} ({std.error})\", \n  statistic = NULL\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear prod.fn. (level-level)\nCD prod.fn. (log-log)\n\n\n\n\n(Intercept)\n6.203  (36.845)\n5.703 ***  (0.202)\n\n\nSharecropping\n87.849  (96.953)\n0.004  (0.047)\n\n\nMixed\n-90.608  (63.901)\n0.017  (0.031)\n\n\nLand, ha\n2145.659 ***  (111.521)\n0.679 ***  (0.027)\n\n\nLabor, hours\n1.248 ***  (0.126)\n0.344 ***  (0.030)\n\n\nNum.Obs.\n1026\n1026\n\n\nR2\n0.819\n0.843\n\n\nR2 Adj.\n0.819\n0.842\n\n\nF\n1158.652\n1369.377"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#regression-and-interpretation-output",
    "href": "slides/w04-mlr-part2.html#regression-and-interpretation-output",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Regression and interpretation",
    "text": "Regression and interpretation\n\n\n\n\n\nLinear prod.fn. (level-level)\n\n\n\n\n(Intercept)\n6.203 (36.845)\n\n\nstatusshare\n87.849 (96.953)\n\n\nstatusmixed\n-90.608 (63.901)\n\n\nland\n2145.659 *** (111.521)\n\n\nlabor\n1.248 *** (0.126)\n\n\nNum.Obs.\n1026\n\n\nR2\n0.819\n\n\nR2 Adj.\n0.819\n\n\nF\n1158.652"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#fitted-values-1",
    "href": "slides/w04-mlr-part2.html#fitted-values-1",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Fitted values (1)",
    "text": "Fitted values (1)\n\n\nfarm_dta %>% \n  mutate(fitted = fitted(fit1)) %>% \n  ggplot() + \n  aes(x = land,\n      y = output,\n      colour = \"Observed\") +\n  xlab(\"Land area, ha\") + \n  ylab(\"Rice output, kg\")"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#fitted-values-1-1",
    "href": "slides/w04-mlr-part2.html#fitted-values-1-1",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Fitted values (1)",
    "text": "Fitted values (1)\n\n\nfarm_dta %>% \n  mutate(fitted = fitted(fit1)) %>% \n  ggplot() + \n  aes(x = land,\n      y = output,\n      colour = \"Observed\") +\n  xlab(\"Land area, ha\") + \n  ylab(\"Rice output, kg\") +\n  geom_point(alpha = 0.3) + \n  theme(legend.position = c(0.1, 0.9)) + \n  scale_color_manual(\n    values = c('#BF382A', '#0C4B8E'))"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#fitted-values-1-2",
    "href": "slides/w04-mlr-part2.html#fitted-values-1-2",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Fitted values (1)",
    "text": "Fitted values (1)\n\n\nfarm_dta %>%\n  mutate(fitted = fitted(fit1)) %>%\n  ggplot() +\n  aes(x = land,\n      y = output,\n      colour = \"Observed\") +\n  xlab(\"Land area, ha\") + \n  ylab(\"Rice output, kg\") +\n  geom_point(alpha = 0.3) +\n  theme(legend.position = c(0.1, 0.9)) + \n  scale_color_manual(\n    values = c('#BF382A', '#0C4B8E')) +\n  geom_point(\n    aes(y = fitted, colour = \"Gitted\"),\n    alpha = 0.3)"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#fitted-values-1-3",
    "href": "slides/w04-mlr-part2.html#fitted-values-1-3",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Fitted values (1)",
    "text": "Fitted values (1)\n\n\nfarm_dta %>%\n  mutate(fitted = fitted(fit1)) %>%\n  ggplot() +\n  aes(x = land,\n      y = output,\n      colour = \"Observed\") +\n  xlab(\"Land area, ha\") + \n  ylab(\"Rice output, kg\") +\n  geom_point(alpha = 0.3) +\n  theme(legend.position = c(0.1, 0.9)) + \n  scale_color_manual(\n    values = c('#BF382A', '#0C4B8E')) +\n  geom_point(\n    aes(y = fitted, colour = \"Gitted\"),\n    alpha = 0.3) + \n  scale_x_log10() +\n  scale_y_log10()"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#actual-values-3d",
    "href": "slides/w04-mlr-part2.html#actual-values-3d",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Actual values 3D",
    "text": "Actual values 3D"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#actual-fitted-values-3d",
    "href": "slides/w04-mlr-part2.html#actual-fitted-values-3d",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Actual + Fitted values 3D",
    "text": "Actual + Fitted values 3D"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#fitted-values-3d-v.2",
    "href": "slides/w04-mlr-part2.html#fitted-values-3d-v.2",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Fitted values 3D v.2",
    "text": "Fitted values 3D v.2"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#linearity",
    "href": "slides/w04-mlr-part2.html#linearity",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Linearity",
    "text": "Linearity\n\nlibrary(performance)\ncheck_model(fit1, check = \"linearity\", panel = FALSE)"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#multicolinearity",
    "href": "slides/w04-mlr-part2.html#multicolinearity",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Multicolinearity",
    "text": "Multicolinearity"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#no-perfect-collinearity",
    "href": "slides/w04-mlr-part2.html#no-perfect-collinearity",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "No perfect collinearity",
    "text": "No perfect collinearity\n\n\n\n\nnone of the regressors can be written as an exact linear combinations of some other regressors in the model.\nFor example:\n\nin \\(Y = \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3\\) ,\nwhere \\(X_3 = X_2 + X_1\\) ,\nall \\(X\\) are collinear.\n\n\n\n\n\n\nConsequence of collinearity:\n\nbiased estimates of the collinear variables\nover-significant results;"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#no",
    "href": "slides/w04-mlr-part2.html#no",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "No",
    "text": "No"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#ne2-slide",
    "href": "slides/w04-mlr-part2.html#ne2-slide",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Ne2 slide",
    "text": "Ne2 slide"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#references-1",
    "href": "slides/w04-mlr-part2.html#references-1",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "References",
    "text": "References\n\n\n\nMP223-Applied Econometrics (SoSe 2023). Author: Eduard Bukin (2023). | GitHub | StudIP | ILIAS.\n\n\n\nAngrist, J. D., & Pischke, J.-S. (2009). Mostly harmless econometrics. Princeton University Press. http://doi.org/10.1515/9781400829828\n\n\nDasgupta, S., Knight, T. O., & Love, H. A. (1999). Evolution of Agricultural Land Leasing Models: A Survey of the Literature. Applied Economic Perspectives and Policy, 21(1), 148–176. http://doi.org/10.2307/1349978\n\n\nFeng, Q., & Horrace, W. C. (2010). Alternative technical efficiency measures: Skew, bias and scale. Journal of Applied Econometrics, 27(2), 253–268. http://doi.org/10.1002/jae.1190\n\n\nLong, J. S., & Ervin, L. H. (2000). Using heteroscedasticity consistent standard errors in the linear regression model. The American Statistician, 54(3), 217–224. http://doi.org/10.1080/00031305.2000.10474549\n\n\nSadoulet, E., & de Janvry, A. (1995). Quantitative Development Policy Analysis. THE JOHNS HOPKINS UNIVERSITY PRESS BALTIMORE AND LONDON.\n\n\nWooldridge, J. M. (2020). Introductory econometrics: A modern approach. South-Western. Retrieved from https://www.cengage.uk/shop/isbn/9781337558860"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#linearity-detection-scatter-plots",
    "href": "slides/w04-mlr-part2.html#linearity-detection-scatter-plots",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Linearity detection: scatter plots",
    "text": "Linearity detection: scatter plots\n\n\n\nfarm_dta %>% select(-status) %>% ggpairs()\n\n\n\n\n\n\n\n\n\n\n\nNote the magnitude of \\(x\\) and \\(y\\)\nFrom min to max \\(y\\) increases about 5000 times.\nIf a variable increases more than 10 times between min and max, \\(log\\) transformation may be needed."
  },
  {
    "objectID": "slides/w04-mlr-part2.html#linearity-detection-fitted-values-vs-residuals",
    "href": "slides/w04-mlr-part2.html#linearity-detection-fitted-values-vs-residuals",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Linearity detection: fitted values vs residuals",
    "text": "Linearity detection: fitted values vs residuals\n\nlibrary(performance)\ncheck_model(fit1, check = \"linearity\", panel = FALSE)"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#linearity-1",
    "href": "slides/w04-mlr-part2.html#linearity-1",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Linearity",
    "text": "Linearity\n\nlibrary(performance)\ncheck_model(fit1, check = \"linearity\", panel = FALSE)\n\n\n\n\n\nIs the linearity assumption fulfilled?"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#what-could-be-done-to-improve-the-model",
    "href": "slides/w04-mlr-part2.html#what-could-be-done-to-improve-the-model",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "What could be done to improve the model?",
    "text": "What could be done to improve the model?\n\nLinear transformation with log: partial (level-log or log-level) or full (log-log).\n\n\n\\[\\ln (\\text{output}) = \\alpha + \\beta_1 \\text{status : share} + \\beta_2 \\text{status : mixed} + \\beta_3 \\ln (\\text{land}) + \\beta_4 \\ln (\\text{labor}) + e\\]"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#linearity-detection-fitted-values-vs-residuals-1",
    "href": "slides/w04-mlr-part2.html#linearity-detection-fitted-values-vs-residuals-1",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Linearity detection: fitted values vs residuals",
    "text": "Linearity detection: fitted values vs residuals\n\nplot(fit1, which = 1)"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#linearity-detection-fitted-values-vs-residuals-v2",
    "href": "slides/w04-mlr-part2.html#linearity-detection-fitted-values-vs-residuals-v2",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Linearity detection: fitted values vs residuals (v2)",
    "text": "Linearity detection: fitted values vs residuals (v2)\n\nplot(fit1, which = 1)"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#taylor-expansion-sadoulet1995-appendix-a.2",
    "href": "slides/w04-mlr-part2.html#taylor-expansion-sadoulet1995-appendix-a.2",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Taylor expansion (Sadoulet & de Janvry, 1995, Appendix A.2)",
    "text": "Taylor expansion (Sadoulet & de Janvry, 1995, Appendix A.2)\nIntroduced by Brook Taylor in 1715 …\n\nValue of any function could be approximated at an arbitrary point \\(x_0\\) with a Taylor expansion:\n\n\n\\[\nf(x) \\approx f(x_0) + f^{'}(x_0)(x - x_0) + \\frac{f^{''}(x_0)}{2!}(x - x_0)^2 + \\cdots .\n\\]"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#taylor-expansion-example",
    "href": "slides/w04-mlr-part2.html#taylor-expansion-example",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Taylor expansion example:",
    "text": "Taylor expansion example:\nWhat is the value of \\(e^{0.4}\\)?\n\n\\[\ne^x = \\sum_{n=0}^{\\infty} \\frac{x^n}{n!} \\approx 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\cdots\n\\]\n\n\n\\[\ne^{0.4} \\approx 1 + .4 + \\frac{.4^2}{2!} + \\frac{.4^3}{3!} + \\frac{.4^4}{4!} + \\frac{.4^5}{5!}\n\\]\n\n\n\\[\n\\approx 1 + .4 + .16/2 + 0.064/6 + 0.0256/24 + 0.01024 / 120\n\\]\n\n\n\\[\n\\approx  1 + .4 + .08 + 0.0106(6) + 0.00106(6) + 0.000106(6)\n\\]\n\n\n\\[\n\\approx  1.491819  \n\\]\n\n\n\\[\n\\text{exp(0.4)} = 1.491825\n\\]"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#production-functions",
    "href": "slides/w04-mlr-part2.html#production-functions",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Production functions",
    "text": "Production functions\nConsider\n\\[q = f(x_i)\\]\nProduction function of several inputs \\(x_i\\), where\n\\[x_i = e^{\\ln x_i}\\]\nand \\(q\\) is the quantity of goods produced.\n\nThen,\n\\[\n\\ln q = \\ln f(e^{\\ln x_i}) \\; \\; \\; \\; \\text{or} \\; \\; \\; \\; \\ln q = g(\\ln x_i)\n\\]\n\n\nLet us approximate \\(\\ln q = g(\\ln x_i)\\) using Taylor expansion."
  },
  {
    "objectID": "slides/w04-mlr-part2.html#how-to-answer-this-question-any-ideas",
    "href": "slides/w04-mlr-part2.html#how-to-answer-this-question-any-ideas",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "How to answer this question? (Any ideas)",
    "text": "How to answer this question? (Any ideas)\n\nData that we have covers a sample of the rice-cultivating farmers in India (Feng & Horrace, 2010):\n\n\\(\\text{output}\\) is gross output of rice in kg\n\\(\\text{land}\\) the total area cultivated with rice, measured in hectares\n\\(\\text{labor}\\) total labor inputs (excluding harvest labor) in hours\n\\(\\text{status}\\) land tenure system on the farm: owner-operated, Share-cropping and mixed\n\n\n\nLet us estimate a simple production function of a rice-producing farm using linear form and Cobb-Douglas function.\n\n\n\\[\\text{output} = \\alpha + \\beta_1 \\text{status : share} + \\beta_2 \\text{status : mixed} + \\beta_3 \\text{land} + \\beta_4 \\text{labor} + e\\]\n\n\n\\[\\ln \\text{output} = \\alpha + \\beta_1 \\text{status : share} + \\beta_2 \\text{status : mixed} + \\beta_3 \\ln \\text{land} + \\beta_4 \\ln \\text{labor} + e\\]"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#linearity-conclusion",
    "href": "slides/w04-mlr-part2.html#linearity-conclusion",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Linearity Conclusion",
    "text": "Linearity Conclusion\nModel:\n\n\nlm(formula = output ~ status + land + labor, data = farm_dta)\n\n\n\nIs the linearity assumption fulfilled?\n\n\nWhat could be done to improve the model?\n\n\nLinear transformation with log: partial (level-log or log-level) or full (log-log).\n\n\n\\[\\ln (\\text{output}) = \\alpha + \\beta_1 \\text{status : share} + \\beta_2 \\text{status : mixed} \\\\ + \\beta_3 \\ln (\\text{land}) + \\beta_4 \\ln (\\text{labor}) + e\\]"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#linear-transformation-of-the-model",
    "href": "slides/w04-mlr-part2.html#linear-transformation-of-the-model",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Linear transformation of the model",
    "text": "Linear transformation of the model\n\nfit1 <- lm(output ~ status + land + labor, \n           data = farm_dta)\n\nfit2_cd <- lm(output ~ status + log(land) + log(labor), \n           data = farm_dta)\n\nmodelsummary(\n  list(`Linear prod.fn. (level-level)` = fit1,\n       `CD prod.fn. (log-log)` = fit2_cd),\n  estimate = \"{estimate} {stars} ({std.error})\", \n  statistic = NULL\n)"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#linear-transformation-of-the-model-output",
    "href": "slides/w04-mlr-part2.html#linear-transformation-of-the-model-output",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Linear transformation of the model",
    "text": "Linear transformation of the model\n\n\n\n\n\nLinear prod.fn. (level-level)\nCD prod.fn. (log-log)\n\n\n\n\n(Intercept)\n6.203 (36.845)\n5.703 *** (0.202)\n\n\nSharecropping\n87.849 (96.953)\n0.004 (0.047)\n\n\nMixed\n-90.608 (63.901)\n0.017 (0.031)\n\n\nLand, ha\n2145.659 *** (111.521)\n0.679 *** (0.027)\n\n\nLabor, hours\n1.248 *** (0.126)\n0.344 *** (0.030)\n\n\nNum.Obs.\n1026\n1026\n\n\nR2\n0.819\n0.843\n\n\nR2 Adj.\n0.819\n0.842\n\n\nF\n1158.652\n1369.377"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#change-in-the-lineariy-assumption",
    "href": "slides/w04-mlr-part2.html#change-in-the-lineariy-assumption",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Change in the lineariy assumption",
    "text": "Change in the lineariy assumption\n\n\nWithout log-log\n\n\n\n\n\n\n\n\n\n\nWith log-log"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#change-in-fitted-values-3d-v.2",
    "href": "slides/w04-mlr-part2.html#change-in-fitted-values-3d-v.2",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Change in Fitted values 3D v.2",
    "text": "Change in Fitted values 3D v.2"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#consequence-and-solutions",
    "href": "slides/w04-mlr-part2.html#consequence-and-solutions",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Consequence and Solutions",
    "text": "Consequence and Solutions\n\n\n\n\nConsequence\n\nbiased estimates of the collinear variables\n\nDetection\n\nScatter plot; Correlation matrix;\nModel specification (formula);\nStep-wise regression approach;\nVariance Inflation Factor;\n\n\n\n\n\n\nSolution to collinearity\n\nRe specify the model;\nChoose different regressors;\n\nSee also\n\nOverview: “Assumption AMLR.3 No Perfect Collinearity” in (Wooldridge, 2020) ;\nExamples of causes in Chapter 9.5 (Wooldridge, 2020) ;\nChapter 9.4-9.5 in (Weisberg, 2005);"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#perfect-collinearity-based-on-reg.-equation",
    "href": "slides/w04-mlr-part2.html#perfect-collinearity-based-on-reg.-equation",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Perfect collinearity based on reg. equation",
    "text": "Perfect collinearity based on reg. equation\n\\[\n\\begin{aligned}\n\\hat{output} & = \\alpha + \\hat\\beta_1 land + \\hat\\beta_2 seeds + \\hat\\beta_3 fertilizers \\\\ &  + \\hat\\beta_4 others + \\hat\\beta_5 total\n\\end{aligned}\n\\]\n\n\nwhere \\(total = seeds + fertilizers + others\\)\nIs there a multicollinearity problem here?\nCoefficient \\(\\hat\\beta_5\\) is aliased and wont be estimated"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#perfect-collinearity-based-on-reg.-equation-2",
    "href": "slides/w04-mlr-part2.html#perfect-collinearity-based-on-reg.-equation-2",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Perfect collinearity based on reg. equation (2)",
    "text": "Perfect collinearity based on reg. equation (2)\n\\[\n\\begin{aligned}\n\\hat{wage} & = \\alpha + \\hat\\beta_1 education + \\hat\\beta_2 experience \\\\\n& + \\hat\\beta_3 male  + \\hat\\beta_4 female\n\\end{aligned}\n\\]\n\n\n\\(male + female = 1\\)\nIs there a multicollinearity problem here?"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#variance-inflation-factor",
    "href": "slides/w04-mlr-part2.html#variance-inflation-factor",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Variance Inflation Factor",
    "text": "Variance Inflation Factor\n\n\n               GVIF Df GVIF^(1/(2*Df))\nstatus     1.019559  2        1.004854\nlog(land)  4.364616  1        2.089166\nlog(labor) 4.380305  1        2.092918\n\n\n# Check for Multicollinearity\n\nLow Correlation\n\n       Term  VIF   VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n     status 1.02 [1.00, 1.49]         1.01      0.98     [0.67, 1.00]\n  log(land) 4.36 [3.93, 4.87]         2.09      0.23     [0.21, 0.25]\n log(labor) 4.38 [3.94, 4.89]         2.09      0.23     [0.20, 0.25]"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#collinearity-example-0.2",
    "href": "slides/w04-mlr-part2.html#collinearity-example-0.2",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Collinearity Example 0.2",
    "text": "Collinearity Example 0.2\n\\[\n\\begin{aligned}\n\\hat{output} = \\hat\\beta_1 land + \\hat\\beta_2 seeds + \\hat\\beta_3 fertilizers + \\hat\\beta_4 others\n\\end{aligned}\n\\]\n::: incremental - where \\(seeds\\) and \\(fertilizers\\) highly correlate between each other (r=0.9), - VIF of \\(seeds\\) and \\(fertilizers\\) is > 12.2 - our key-interest variable is \\(land\\). - As \\(fertilizers\\) is a control variable and we may have OVB if we remove it,\n\nIf we really want to reduce VIF…:\n\nDis-aggregate fertilizers into mineral and organic, for example.\nAggregate fertilizers and seeds"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#example-0.3",
    "href": "slides/w04-mlr-part2.html#example-0.3",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Example 0.3",
    "text": "Example 0.3\nSame model but in log:\n\\[log(\\hat{output}) = \\hat\\beta_1 log(land) + \\hat\\beta_2 log(seeds) + \\hat\\beta_3 log(fertilizers) + \\\\ \\hat\\beta_4 log(others) + \\hat\\beta_5 log(total) \\]\nwhere \\(total = seeds + fertilizers + others\\)\n\n\n\n\n\n\n\nImportant\n\n\nIs there a multicollinearity problem here?\n\n\n\nThink!\n\\(log(a) + log(b) = log(a * b)\\)\n\n\n\n\n\n\nDanger\n\n\nNot really"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#example-0.4",
    "href": "slides/w04-mlr-part2.html#example-0.4",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Example 0.4",
    "text": "Example 0.4\nSame model but with a quadratic term:\n\\[\\hat{output} = \\hat\\beta_1 land + \\hat\\beta_2 land^2 + \\hat\\beta_3 seeds + \\hat\\beta_4 fertilizers + \\hat\\beta_5 others\\]\n\n\n\n\n\n\n\nImportant\n\n\nIs there a multicollinearity problem here?\n\n\n\nThink!\n\n\n\n\n\n\nDanger\n\n\nNot really\n\n\n\n\\(land^2\\) is not a linear combination of \\(land\\) ;\nLinear combination is when \\(land + land\\) not when \\(land \\times land\\) ;"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#no-perfect-collinearity-1",
    "href": "slides/w04-mlr-part2.html#no-perfect-collinearity-1",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "No perfect collinearity",
    "text": "No perfect collinearity"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#conclusion",
    "href": "slides/w04-mlr-part2.html#conclusion",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Conclusion",
    "text": "Conclusion\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear prod. fn. (level-level)\nCD prod. fn. (log-log)\n\n\n\n\n(Intercept)\n6.203  (36.845)\n5.703 ***  (0.202)\n\n\nSharecropping\n87.849  (96.953)\n0.004  (0.047)\n\n\nMixed\n-90.608  (63.901)\n0.017  (0.031)\n\n\nLand, ha\n2145.659 ***  (111.521)\n0.679 ***  (0.027)\n\n\nLabor, hours\n1.248 ***  (0.126)\n0.344 ***  (0.030)\n\n\nNum.Obs.\n1026\n1026\n\n\nR2\n0.819\n0.843\n\n\nR2 Adj.\n0.819\n0.842\n\n\nF\n1158.652\n1369.377\n\n\n\n\n\n\n\n\nInterpret coefficients in both models.\nWhat is the estimated effect of share-cropping?\nIs this model shows causal effects of \\(x\\) on \\(y\\)?\nIs there a ceteris paribus?"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#exploratory-staistics-3-1",
    "href": "slides/w04-mlr-part2.html#exploratory-staistics-3-1",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Exploratory staistics (3)",
    "text": "Exploratory staistics (3)\n\nggpairs(farm_dta, aes(colour = status, alpha = 0.2))"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#entertainment-fitted-values-1",
    "href": "slides/w04-mlr-part2.html#entertainment-fitted-values-1",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Entertainment: Fitted values (1)",
    "text": "Entertainment: Fitted values (1)\n\n\nfarm_dta %>% \n  mutate(fitted = fitted(fit1)) %>% \n  ggplot() + \n  aes(x = land,\n      y = output,\n      colour = \"Observed\") +\n  xlab(\"Land area, ha\") + \n  ylab(\"Rice output, kg\")"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#entertainment-fitted-values-1-1",
    "href": "slides/w04-mlr-part2.html#entertainment-fitted-values-1-1",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Entertainment: Fitted values (1)",
    "text": "Entertainment: Fitted values (1)\n\n\nfarm_dta %>% \n  mutate(fitted = fitted(fit1)) %>% \n  ggplot() + \n  aes(x = land,\n      y = output,\n      colour = \"Observed\") +\n  xlab(\"Land area, ha\") + \n  ylab(\"Rice output, kg\") +\n  geom_point(alpha = 0.3) + \n  theme(legend.position = c(0.1, 0.9)) + \n  scale_color_manual(\n    values = c('#BF382A', '#0C4B8E'))"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#entertainment-fitted-values-1-2",
    "href": "slides/w04-mlr-part2.html#entertainment-fitted-values-1-2",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Entertainment: Fitted values (1)",
    "text": "Entertainment: Fitted values (1)\n\n\nfarm_dta %>%\n  mutate(fitted = fitted(fit1)) %>%\n  ggplot() +\n  aes(x = land,\n      y = output,\n      colour = \"Observed\") +\n  xlab(\"Land area, ha\") + \n  ylab(\"Rice output, kg\") +\n  geom_point(alpha = 0.3) +\n  theme(legend.position = c(0.1, 0.9)) + \n  scale_color_manual(\n    values = c('#BF382A', '#0C4B8E')) +\n  geom_point(\n    aes(y = fitted, colour = \"Gitted\"),\n    alpha = 0.3)"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#entertainment-fitted-values-1-3",
    "href": "slides/w04-mlr-part2.html#entertainment-fitted-values-1-3",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Entertainment: Fitted values (1)",
    "text": "Entertainment: Fitted values (1)\n\n\nfarm_dta %>%\n  mutate(fitted = fitted(fit1)) %>%\n  ggplot() +\n  aes(x = land,\n      y = output,\n      colour = \"Observed\") +\n  xlab(\"Land area, ha\") + \n  ylab(\"Rice output, kg\") +\n  geom_point(alpha = 0.3) +\n  theme(legend.position = c(0.1, 0.9)) + \n  scale_color_manual(\n    values = c('#BF382A', '#0C4B8E')) +\n  geom_point(\n    aes(y = fitted, colour = \"Gitted\"),\n    alpha = 0.3) + \n  scale_x_log10() +\n  scale_y_log10()"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#entertainment-actual-values-3d",
    "href": "slides/w04-mlr-part2.html#entertainment-actual-values-3d",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Entertainment: Actual values 3D",
    "text": "Entertainment: Actual values 3D"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#entertainment-actual-fitted-values-3d",
    "href": "slides/w04-mlr-part2.html#entertainment-actual-fitted-values-3d",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Entertainment: Actual + Fitted values 3D",
    "text": "Entertainment: Actual + Fitted values 3D"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#entertainment-fitted-values-3d-v.2",
    "href": "slides/w04-mlr-part2.html#entertainment-fitted-values-3d-v.2",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Entertainment: Fitted values 3D v.2",
    "text": "Entertainment: Fitted values 3D v.2"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#partial-transformation-1",
    "href": "slides/w04-mlr-part2.html#partial-transformation-1",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Partial transformation (1)",
    "text": "Partial transformation (1)\n\nfit3_level_log <- lm(output ~ status + log(land) + log(labor), data = farm_dta)\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear prod.fn. (level-level)\nCD prod.fn. (log-log)\nLevel-log prod.fn. (level-log)\n\n\n\n\n(Intercept)\n6.203 (36.845)\n5.703 *** (0.202)\n-2549.390 *** (666.234)\n\n\nSharecropping\n87.849 (96.953)\n0.004 (0.047)\n132.761 (154.889)\n\n\nMixed\n-90.608 (63.901)\n0.017 (0.031)\n-480.042 *** (102.642)\n\n\nLand, ha\n2145.659 *** (111.521)\n0.679 *** (0.027)\n743.287 *** (89.588)\n\n\nLabor, hours\n1.248 *** (0.126)\n0.344 *** (0.030)\n901.025 *** (100.797)\n\n\nNum.Obs.\n1026\n1026\n1026\n\n\nR2\n0.819\n0.843\n0.540\n\n\nR2 Adj.\n0.819\n0.842\n0.538\n\n\nF\n1158.652\n1369.377\n299.783"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#partial-transformation-2",
    "href": "slides/w04-mlr-part2.html#partial-transformation-2",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Partial transformation (2)",
    "text": "Partial transformation (2)\n\ncheck_model(fit3_level_log, check = \"linearity\", panel = FALSE)"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#perfect-collinearity-example-1",
    "href": "slides/w04-mlr-part2.html#perfect-collinearity-example-1",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Perfect Collinearity Example 1",
    "text": "Perfect Collinearity Example 1\n\\[\n\\begin{aligned}\n\\hat{wage} & = \\alpha + \\hat\\beta_1 education + \\hat\\beta_2 experience \\\\\n& + \\hat\\beta_3 male  + \\hat\\beta_4 female\n\\end{aligned}\n\\]\n\n\n\\(male + female = 1\\)\nIs there a multicollinearity problem here?"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#perfect-collinearity-example-2",
    "href": "slides/w04-mlr-part2.html#perfect-collinearity-example-2",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Perfect Collinearity Example 2",
    "text": "Perfect Collinearity Example 2\n\\[\n\\begin{aligned}\n\\hat{output} & = \\alpha + \\hat\\beta_1 land + \\hat\\beta_2 seeds + \\hat\\beta_3 fertilizers \\\\ &  + \\hat\\beta_4 others + \\hat\\beta_5 total\n\\end{aligned}\n\\]\n\n\nwhere \\(total = seeds + fertilizers + others\\)\nIs there a multicollinearity problem here?\nCoefficient \\(\\hat\\beta_5\\) is aliased and wont be estimated"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#perfect-collinearity-example-3",
    "href": "slides/w04-mlr-part2.html#perfect-collinearity-example-3",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Perfect Collinearity Example 3",
    "text": "Perfect Collinearity Example 3\nSame model but in log:\n\\[\n\\begin{aligned}\n\\ln (\\hat{output})&  = \\hat\\beta_1 \\ln (land) + \\hat\\beta_2 \\ln (seeds) + \\hat\\beta_3 \\ln (fertilizers) \\\\ & + \\hat\\beta_4 \\ln (others) + \\hat\\beta_5 \\ln (total)\n\\end{aligned}\n\\]\nwhere \\(total = seeds + fertilizers + others\\)\n\n\nIs there a multicollinearity problem here?\nThink!\n\\(log(a) + log(b) = log(a * b)\\)\n\n\n\n\n\n\nDanger\n\n\nNo, there is no collinearity because \\(\\ln (seeds) + \\ln (fertilizers) + \\ln (others) \\ne \\ln (total)\\). This is not a linear combination! \\(\\ln (seeds \\cdot fertilizers \\cdot others) \\ne \\ln (total)\\)"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#perfect-collinearity-example-4",
    "href": "slides/w04-mlr-part2.html#perfect-collinearity-example-4",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Perfect Collinearity Example 4",
    "text": "Perfect Collinearity Example 4\nUsing the production function estimated above, let us convert tenure type into a set of dummy variables:\n\nfarm_dta_coll <- \n  farm_dta %>% \n  mutate(\n    status_owner = ifelse(status == \"owner\", 1, 0),\n    status_share = as.integer(status == \"share\"),\n    status_mixed = as.integer(status == \"mixed\")\n    )\nglimpse(farm_dta_coll)\n\nRows: 1,026\nColumns: 7\n$ output       <int> 7980, 4083, 2650, 4500, 16300, 17424, 3840, 2800, 950, 24…\n$ land         <dbl> 3.000, 2.000, 1.000, 2.000, 3.572, 3.572, 1.420, 1.420, 0…\n$ labor        <int> 2915, 2155, 1075, 2091, 3889, 3519, 810, 855, 460, 109, 2…\n$ status       <fct> owner, owner, owner, owner, share, share, mixed, mixed, m…\n$ status_owner <dbl> 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, …\n$ status_share <int> 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, …\n$ status_mixed <int> 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#perfect-collinearity-example-4-1",
    "href": "slides/w04-mlr-part2.html#perfect-collinearity-example-4-1",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Perfect Collinearity Example 4",
    "text": "Perfect Collinearity Example 4\n\n\n\nCall:\nlm(formula = output ~ status_owner + status_share + status_mixed + \n    land + labor, data = farm_dta_coll)\n\nCoefficients:\n (Intercept)  status_owner  status_share  status_mixed          land  \n     -84.405        90.608       178.457            NA      2145.659  \n       labor  \n       1.248  \n\n\n\nCall:\nlm(formula = output ~ status_mixed + status_owner + status_share + \n    land + labor, data = farm_dta_coll)\n\nCoefficients:\n (Intercept)  status_mixed  status_owner  status_share          land  \n      94.052      -178.457       -87.849            NA      2145.659  \n       labor  \n       1.248"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#perfect-collinearity-example-4-2",
    "href": "slides/w04-mlr-part2.html#perfect-collinearity-example-4-2",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Perfect Collinearity Example 4",
    "text": "Perfect Collinearity Example 4\n\n\n\n\n\n\n\n\n\n\n\n\nLinear prod.fn. (level-level)\nLinear prod.fn. with dummies 1\nLinear prod.fn. with dummies 2\n\n\n\n\n(Intercept)\n6.203 (36.845)\n-84.405 (60.186)\n94.052 (93.612)\n\n\nstatusshare\n87.849 (96.953)\n\n\n\n\nstatusmixed\n-90.608 (63.901)\n\n\n\n\nland\n2145.659 *** (111.521)\n2145.659 *** (111.521)\n2145.659 *** (111.521)\n\n\nlabor\n1.248 *** (0.126)\n1.248 *** (0.126)\n1.248 *** (0.126)\n\n\nstatus_owner\n\n90.608 (63.901)\n-87.849 (96.953)\n\n\nstatus_share\n\n178.457 + (107.992)\n\n\n\nstatus_mixed\n\n\n-178.457 + (107.992)\n\n\nNum.Obs.\n1026\n1026\n1026\n\n\nR2\n0.819\n0.819\n0.819\n\n\nR2 Adj.\n0.819\n0.819\n0.819\n\n\nF\n1158.652"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#multicollinearity-near-perfect-collinearity",
    "href": "slides/w04-mlr-part2.html#multicollinearity-near-perfect-collinearity",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Multicollinearity: Near-perfect collinearity",
    "text": "Multicollinearity: Near-perfect collinearity"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#no-perfect-collinearity-2",
    "href": "slides/w04-mlr-part2.html#no-perfect-collinearity-2",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "No perfect collinearity",
    "text": "No perfect collinearity"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#linear-transformation-of-the-model-1",
    "href": "slides/w04-mlr-part2.html#linear-transformation-of-the-model-1",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Linear transformation of the model",
    "text": "Linear transformation of the model\n\n\n\n\n\n\nLinear prod.fn. (level-level)\nCD prod.fn. (log-log)\n\n\n\n\n(Intercept)\n6.203 (36.845)\n5.703 *** (0.202)\n\n\nSharecropping\n87.849 (96.953)\n0.004 (0.047)\n\n\nMixed\n-90.608 (63.901)\n0.017 (0.031)\n\n\nLand, ha\n2145.659 *** (111.521)\n0.679 *** (0.027)\n\n\nLabor, hours\n1.248 *** (0.126)\n0.344 *** (0.030)\n\n\nNum.Obs.\n1026\n1026\n\n\nR2\n0.819\n0.843\n\n\nR2 Adj.\n0.819\n0.842\n\n\nF\n1158.652\n1369.377"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#origin",
    "href": "slides/w04-mlr-part2.html#origin",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Origin",
    "text": "Origin\n\n\nWhen two variables \\(x_1\\) and \\(x_2\\) are highly correlated.\nAnd both (\\(x_1\\) and \\(x_2\\)) affect \\(y\\) significantly.\n\\(x_1\\) and \\(x_2\\) are collinear\nEstimated Standard Errors are inflated (larger then they could be).\nInference about \\(x_1\\) and \\(x_2\\) (only) is misleading."
  },
  {
    "objectID": "slides/w04-mlr-part2.html#detection",
    "href": "slides/w04-mlr-part2.html#detection",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Detection",
    "text": "Detection\n\n\nVariance Inflation Factor\n\nNot a statistical test, only an informative number.\nThis shows by how much, the SE of the collinear variables are larger than what they could have been if there was no collinearity.\n\nStep-wise regression approach:\n\nInclude collinear variables one by one and together, and\nobserve how the \\(R^2\\), and variables significance changes"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#problem",
    "href": "slides/w04-mlr-part2.html#problem",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Problem",
    "text": "Problem\n\n\nWhen two variables \\(x_1\\) and \\(x_2\\) are highly correlated.\nAnd both (\\(x_1\\) and \\(x_2\\)) affect \\(y\\) significantly.\n\\(x_1\\) and \\(x_2\\) are collinear\nEstimated Standard Errors are inflated (larger then they could be).\nInference about \\(x_1\\) and \\(x_2\\) (only) is misleading."
  },
  {
    "objectID": "slides/w04-mlr-part2.html#solutions",
    "href": "slides/w04-mlr-part2.html#solutions",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Solutions",
    "text": "Solutions\n\n\nFind different independent variables.\nDrop one if it is redundant and repeating.\nKeep both and ignore if \\(x_1\\) and \\(x_2\\) are just control variables.\nKeep is removing may cause the OVB."
  },
  {
    "objectID": "slides/w04-mlr-part2.html#multicollinearity-example-1.-production-function",
    "href": "slides/w04-mlr-part2.html#multicollinearity-example-1.-production-function",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Multicollinearity Example 1. Production Function",
    "text": "Multicollinearity Example 1. Production Function\n\nlibrary(performance)\ncheck_collinearity(fit2_cd)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n       Term  VIF   VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n     status 1.02 [1.00, 1.49]         1.01      0.98     [0.67, 1.00]\n  log(land) 4.36 [3.93, 4.87]         2.09      0.23     [0.21, 0.25]\n log(labor) 4.38 [3.94, 4.89]         2.09      0.23     [0.20, 0.25]\n\n\n\nIf VIF > 10, we may suspect multicollinearity.\n\n\nRemoving land or labor will cause the OVB (omitted variable bias), we must keep both variables."
  },
  {
    "objectID": "slides/w04-mlr-part2.html#multicollinearity-example-2",
    "href": "slides/w04-mlr-part2.html#multicollinearity-example-2",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Multicollinearity Example 2",
    "text": "Multicollinearity Example 2\n\\[\n\\begin{aligned}\n\\hat{output} = \\hat\\beta_1 land + \\hat\\beta_2 seeds + \\hat\\beta_3 fertilizers + \\hat\\beta_4 others\n\\end{aligned}\n\\]\n\n\nwhere \\(seeds\\) and \\(fertilizers\\) highly correlate between each other (r=0.9),\nVIF of \\(seeds\\) and \\(fertilizers\\) is > 12.2\nour key-interest variable is \\(land\\).\nAs \\(fertilizers\\) is a control variable and we may have OVB if we remove it,\nIf we really want to reduce VIF…:\n\nDis-aggregate fertilizers into mineral and organic, for example.\nAggregate fertilizers and seeds"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#multicollinearity-example-3",
    "href": "slides/w04-mlr-part2.html#multicollinearity-example-3",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Multicollinearity Example 3",
    "text": "Multicollinearity Example 3\nSame model but with a quadratic term:\n\\[\\hat{output} = \\hat\\beta_1 land + \\hat\\beta_2 land^2 + \\hat\\beta_3 seeds + \\hat\\beta_4 fertilizers + \\hat\\beta_5 others\\]\n\n\nVIF of \\(land\\) and \\(land^2\\) is > 25\nIs there a multicollinearity problem here?\nThink!\nNot really\n\\(land^2\\) is not a linear combination of \\(land\\) ;\nLinear combination is when \\(land + land\\) not when \\(land \\times land\\) ;"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#multicollinearity-example-4",
    "href": "slides/w04-mlr-part2.html#multicollinearity-example-4",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Multicollinearity Example 4",
    "text": "Multicollinearity Example 4\nStep-wise approach to regression analysis:\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel 1\nModel 2\nModel 3\nModel 4\n\n\n\n\n(Intercept)\n4.997*** (0.098)\n-20.158*** (2.726)\n-10.254*** (1.553)\n-1.278 (11.509)\n\n\nmuniPrecip\n-0.009+ (0.005)\n-0.010*** (0.002)\n-0.010*** (0.002)\n-0.011*** (0.002)\n\n\nyear\n\n0.013*** (0.001)\n\n-0.011 (0.014)\n\n\nlog(muniPop)\n\n\n1.025*** (0.104)\n1.917 (1.138)\n\n\nNum.Obs.\n24\n24\n24\n24\n\n\nR2\n0.148\n0.832\n0.848\n0.852\n\n\nR2 Adj.\n0.110\n0.815\n0.833\n0.830\n\n\n\nNote: ^^ Model 1: log(muniUse) ~ muniPrecipModel 2: log(muniUse) ~ muniPrecip + yearModel 3: log(muniUse) ~ muniPrecip + log(muniPop)Model 4: log(muniUse) ~ muniPrecip + year + log(muniPop)"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#problem-and-detection",
    "href": "slides/w04-mlr-part2.html#problem-and-detection",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Problem and detection",
    "text": "Problem and detection\n\n\nProblem: when \\(Var(u|x_{i1}, x_{i2}, \\cdots , x_{ik}) \\ne \\sigma^2\\)\n\nEstimates are not biased, but;\nStandard errors are inefficient;\nInference is misleading and false.\n\nDetection:\n\nGraphical: residuals vs regressors and fitted values plots;\nStatistical tests: Breusch-Pagan, White test, Goldfeld-Quandt."
  },
  {
    "objectID": "slides/w04-mlr-part2.html#solutions-1",
    "href": "slides/w04-mlr-part2.html#solutions-1",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Solutions",
    "text": "Solutions\n\n\nRe-specify the model/ choose different regressors;\nCorrect SE with alternative estimation procedures\n\nrobust: use heteroskedasticity-consistent (robust) standard errors.\ncluster: use clustered standard errors.\n\nUse a weighted regression estimated with Generalized Linear Model (GLM) estimator instead of OLS;\n\nGenerally discouraged for small samples (n < 5000) as GLM have weaker asymptotic properties as compared to the OLS.\n\nSee also: (Angrist & Pischke, 2009, Ch. 3.4), “Assumption AMLR.5” in (Wooldridge, 2020)"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#heteroscedasticity-example-1",
    "href": "slides/w04-mlr-part2.html#heteroscedasticity-example-1",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Heteroscedasticity example (1)",
    "text": "Heteroscedasticity example (1)"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#heteroscedasticity-example-2",
    "href": "slides/w04-mlr-part2.html#heteroscedasticity-example-2",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Heteroscedasticity example (2)",
    "text": "Heteroscedasticity example (2)"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#heteroscedasticity-3.-production-function",
    "href": "slides/w04-mlr-part2.html#heteroscedasticity-3.-production-function",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Heteroscedasticity (3). Production function",
    "text": "Heteroscedasticity (3). Production function\n\n\nlm(formula = output ~ status + land + labor, data = farm_dta)\n\n\n\ncheck_model(fit1, check = c(\"linearity\", \"homogeneity\"))"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#heteroscedasticity-3.-partial-transformation",
    "href": "slides/w04-mlr-part2.html#heteroscedasticity-3.-partial-transformation",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Heteroscedasticity (3). Partial transformation",
    "text": "Heteroscedasticity (3). Partial transformation\n\n\nlm(formula = output ~ status + log(land) + log(labor), data = farm_dta)\n\n\n\ncheck_model(fit3_level_log, check = c(\"linearity\", \"homogeneity\"))"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#heteroscedasticity-3.-cd-production-function",
    "href": "slides/w04-mlr-part2.html#heteroscedasticity-3.-cd-production-function",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Heteroscedasticity (3). CD production function",
    "text": "Heteroscedasticity (3). CD production function\n\n\nlm(formula = log(output) ~ status + log(land) + log(labor), data = farm_dta)\n\n\n\ncheck_model(fit2_cd, check = c(\"linearity\", \"homogeneity\"))"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#statistical-tests-1-breusch-pagan",
    "href": "slides/w04-mlr-part2.html#statistical-tests-1-breusch-pagan",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Statistical tests (1): Breusch-Pagan",
    "text": "Statistical tests (1): Breusch-Pagan\n\n\\(H_0:\\) variance is constant; \\(H_1\\) non constant variance.\nIf \\(H_0\\) is rejected, we have a Heteroscedasticity.\n\n\n\n\n\nlm(formula = output ~ status + land + labor, data = farm_dta)\n\n\n\nlibrary(lmtest)\nbptest(fit1)\n\n\n    studentized Breusch-Pagan test\n\ndata:  fit1\nBP = 348.57, df = 4, p-value < 2.2e-16\n\n\n\n\n\nlm(formula = log(output) ~ status + log(land) + log(labor), data = farm_dta)\n\n\n\nlibrary(lmtest)\nbptest(fit2_cd)\n\n\n    studentized Breusch-Pagan test\n\ndata:  fit2_cd\nBP = 19.776, df = 4, p-value = 0.0005528"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#statistical-tests-2-white-test",
    "href": "slides/w04-mlr-part2.html#statistical-tests-2-white-test",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Statistical tests (2): White test",
    "text": "Statistical tests (2): White test\nSame idea as Breusch-Pagan test. Assumes that residuals variance is a function of polynomial of regressors:\n\\[\n\\text{residuals} = \\log(\\text{land}) + \\log(\\text{labor}) + \\log(\\text{land}) ^ 2 + \\log(\\text{labor})^2\n\\]\n\n\n\n\nlm(formula = output ~ status + land + labor, data = farm_dta)\n\n\n\nbptest(fit1, \n       ~ land * labor + I(labor^2) + I(labor^2), \n       data = farm_dta)\n\n\n    studentized Breusch-Pagan test\n\ndata:  fit1\nBP = 364.4, df = 4, p-value < 2.2e-16\n\n\n\n\n\nlm(formula = log(output) ~ status + log(land) + log(labor), data = farm_dta)\n\n\n\nbptest(fit1, \n       ~ log(land) * log(labor) + \n         I(log(labor)^2) + I(log(labor)^2), \n       data = farm_dta)\n\n\n    studentized Breusch-Pagan test\n\ndata:  fit1\nBP = 309.24, df = 4, p-value < 2.2e-16"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#heteroscedasticity-2.-production-function",
    "href": "slides/w04-mlr-part2.html#heteroscedasticity-2.-production-function",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Heteroscedasticity (2). Production function",
    "text": "Heteroscedasticity (2). Production function\n\n\nlm(formula = output ~ status + land + labor, data = farm_dta)\n\n\n\ncheck_model(fit1, check = c(\"linearity\", \"homogeneity\"))"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#heteroscedasticity-2.-partial-transformation",
    "href": "slides/w04-mlr-part2.html#heteroscedasticity-2.-partial-transformation",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Heteroscedasticity (2). Partial transformation",
    "text": "Heteroscedasticity (2). Partial transformation\n\n\nlm(formula = output ~ status + log(land) + log(labor), data = farm_dta)\n\n\n\ncheck_model(fit3_level_log, check = c(\"linearity\", \"homogeneity\"))"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#heteroscedasticity-2.-cd-production-function",
    "href": "slides/w04-mlr-part2.html#heteroscedasticity-2.-cd-production-function",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Heteroscedasticity (2). CD production function",
    "text": "Heteroscedasticity (2). CD production function\n\n\nlm(formula = log(output) ~ status + log(land) + log(labor), data = farm_dta)\n\n\n\ncheck_model(fit2_cd, check = c(\"linearity\", \"homogeneity\"))"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#robust-standard-errors-1",
    "href": "slides/w04-mlr-part2.html#robust-standard-errors-1",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Robust standard errors (1)",
    "text": "Robust standard errors (1)\nMethods of correcting the variance-covariance matrix.\n\n\nvcov(fit1)\n\n\n\n\n\n \n  \n      \n    (Intercept) \n    statusshare \n    statusmixed \n    land \n    labor \n  \n \n\n  \n    (Intercept) \n    1357.56 \n    -997.12 \n    -909.30 \n    -383.44 \n    -0.71 \n  \n  \n    statusshare \n    -997.12 \n    9399.81 \n    910.48 \n    -15.30 \n    0.24 \n  \n  \n    statusmixed \n    -909.30 \n    910.48 \n    4083.37 \n    -146.28 \n    0.16 \n  \n  \n    land \n    -383.44 \n    -15.30 \n    -146.28 \n    12437.02 \n    -12.75 \n  \n  \n    labor \n    -0.71 \n    0.24 \n    0.16 \n    -12.75 \n    0.02"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#common-methods-of-robust-standard-errors",
    "href": "slides/w04-mlr-part2.html#common-methods-of-robust-standard-errors",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Common methods of robust standard errors",
    "text": "Common methods of robust standard errors\nRobust estimators for variance covariance matrix:\n\nsandwich::vcovHC - heteroskedasticity consistent\nsandwich::vcovCL - clustered SE\nclubSandwich::vcovCR - clustered heteroskedasticity consistent SE\nsandwich::vcovHAC - heteroskedasticity and autocorrelation consistent\nEstimation methods:\n\nHC3 - optimal one as per (Long & Ervin, 2000)\nHC1 - default in Stata"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#example-of-the-robust-vcov",
    "href": "slides/w04-mlr-part2.html#example-of-the-robust-vcov",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Example of the robust vcov",
    "text": "Example of the robust vcov\n\n\nlm(formula = output ~ status + land + labor, data = farm_dta)\n\n\n\n\nRegular vcov\n\nvcov(fit1)\n\n\n\n\n\n \n  \n      \n    (Intercept) \n    statusshare \n    statusmixed \n    land \n    labor \n  \n \n\n  \n    (Intercept) \n    1357.56 \n    -997.12 \n    -909.30 \n    -383.44 \n    -0.71 \n  \n  \n    statusshare \n    -997.12 \n    9399.81 \n    910.48 \n    -15.30 \n    0.24 \n  \n  \n    statusmixed \n    -909.30 \n    910.48 \n    4083.37 \n    -146.28 \n    0.16 \n  \n  \n    land \n    -383.44 \n    -15.30 \n    -146.28 \n    12437.02 \n    -12.75 \n  \n  \n    labor \n    -0.71 \n    0.24 \n    0.16 \n    -12.75 \n    0.02 \n  \n\n\n\n\n\n\nRobust vcov\n\nlibrary(sandwich)\nvcovHC(fit1, type = \"HC3\")\n\n\n\n\n\n \n  \n      \n    (Intercept) \n    statusshare \n    statusmixed \n    land \n    labor \n  \n \n\n  \n    (Intercept) \n    4235.06 \n    -3084.98 \n    249.07 \n    -6038.87 \n    -5.91 \n  \n  \n    statusshare \n    -3084.98 \n    11437.98 \n    799.53 \n    -2421.05 \n    8.76 \n  \n  \n    statusmixed \n    249.07 \n    799.53 \n    2692.17 \n    1349.86 \n    -4.71 \n  \n  \n    land \n    -6038.87 \n    -2421.05 \n    1349.86 \n    75901.71 \n    -62.24 \n  \n  \n    labor \n    -5.91 \n    8.76 \n    -4.71 \n    -62.24 \n    0.09"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#effect-of-robust-se-on-inference",
    "href": "slides/w04-mlr-part2.html#effect-of-robust-se-on-inference",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Effect of robust SE on inference",
    "text": "Effect of robust SE on inference\n\n\nlm(formula = output ~ status + land + labor, data = farm_dta)\n\n\n\n\n\n\n\n\n\n\nLinear prod.fn. (level-level) regular SE\n\n\n\n\n(Intercept)\n6.203  (36.845)\n\n\nSharecropping\n87.849  (96.953)\n\n\nMixed\n-90.608  (63.901)\n\n\nLand, ha\n2145.659 ***  (111.521)\n\n\nLabor, hours\n1.248 ***  (0.126)\n\n\nNum.Obs.\n1026\n\n\nR2 Adj.\n0.819\n\n\n\n\n\n\n\n\n\n\n\n\nLinear prod.fn. (level-level) robust SE\n\n\n\n\n(Intercept)\n6.203  (68.312)\n\n\nSharecropping\n87.849  (111.881)\n\n\nMixed\n-90.608 +  (53.100)\n\n\nLand, ha\n2145.659 ***  (291.480)\n\n\nLabor, hours\n1.248 ***  (0.312)\n\n\nNum.Obs.\n1026\n\n\nR2 Adj.\n0.819\n\n\nStd.Errors\nHC3"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#effect-of-robust-se-on-inference-1",
    "href": "slides/w04-mlr-part2.html#effect-of-robust-se-on-inference-1",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Effect of robust SE on inference",
    "text": "Effect of robust SE on inference\n\n\nlm(formula = log(output) ~ status + log(land) + log(labor), data = farm_dta)\n\n\n\n\n\n\n\n\n\n\nCD prod.fn. (level-level) regular SE\n\n\n\n\n(Intercept)\n5.703 ***  (0.202)\n\n\nSharecropping\n0.004  (0.047)\n\n\nMixed\n0.017  (0.031)\n\n\nLand, ha\n0.679 ***  (0.027)\n\n\nLabor, hours\n0.344 ***  (0.030)\n\n\nNum.Obs.\n1026\n\n\nR2 Adj.\n0.842\n\n\n\n\n\n\n\n\n\n\n\n\nCD prod.fn. (level-level) robust SE\n\n\n\n\n(Intercept)\n5.703 ***  (0.263)\n\n\nSharecropping\n0.004  (0.048)\n\n\nMixed\n0.017  (0.028)\n\n\nLand, ha\n0.679 ***  (0.036)\n\n\nLabor, hours\n0.344 ***  (0.039)\n\n\nNum.Obs.\n1026\n\n\nR2 Adj.\n0.842\n\n\nStd.Errors\nHC3"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#exploratory-statistics-1",
    "href": "slides/w04-mlr-part2.html#exploratory-statistics-1",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Exploratory statistics (1)",
    "text": "Exploratory statistics (1)\n\ndatasummary_skim(farm_dta, output = \"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnique (#)\nMissing (%)\nMean\nSD\nMin\nMedian\nMax\n\n\n\n\noutput\n482\n0\n1405.2\n1921.8\n42.0\n886.5\n20960.0\n\n\nland\n300\n0\n0.4\n0.5\n0.0\n0.3\n5.3\n\n\nlabor\n544\n0\n388.4\n484.2\n17.0\n252.0\n4774.0"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#exploratory-statistics-2",
    "href": "slides/w04-mlr-part2.html#exploratory-statistics-2",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Exploratory statistics (2)",
    "text": "Exploratory statistics (2)\n\ndatasummary_skim(farm_dta, output = \"markdown\", \"categorical\")\n\n\n\n\nstatus\nN\n%\n\n\n\n\nowner\n736\n71.7\n\n\nshare\n79\n7.7\n\n\nmixed\n211\n20.6"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#exploratory-statistics-3",
    "href": "slides/w04-mlr-part2.html#exploratory-statistics-3",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Exploratory statistics (3)",
    "text": "Exploratory statistics (3)\n\ndatasummary( (land  + labor  + output) ~ status * (mean + sd), farm_dta, output = \"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nowner / mean\nowner / sd\nshare / mean\nshare / sd\nmixed / mean\nmixed / sd\n\n\n\n\nland\n0.44\n0.58\n0.36\n0.56\n0.44\n0.39\n\n\nlabor\n395.42\n522.12\n315.51\n566.17\n391.44\n262.63\n\n\noutput\n1435.68\n2014.60\n1250.65\n2639.25\n1356.59\n1104.10"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#exploratory-statistics-3-1",
    "href": "slides/w04-mlr-part2.html#exploratory-statistics-3-1",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Exploratory statistics (3)",
    "text": "Exploratory statistics (3)\n\nggpairs(farm_dta, aes(colour = status, alpha = 0.2))"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#change-in-the-linearity-assumption",
    "href": "slides/w04-mlr-part2.html#change-in-the-linearity-assumption",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Change in the linearity assumption",
    "text": "Change in the linearity assumption\n\n\nWithout log-log\n\n\n\n\n\n\n\n\n\n\nWith log-log"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#an-example-of-robust-vcov",
    "href": "slides/w04-mlr-part2.html#an-example-of-robust-vcov",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "An example of robust vcov",
    "text": "An example of robust vcov\n\n\nlm(formula = output ~ status + land + labor, data = farm_dta)\n\n\nRegular vcov\n\nvcov(fit1)\n\n\n\n\n\n \n  \n      \n    (Intercept) \n    statusshare \n    statusmixed \n    land \n    labor \n  \n \n\n  \n    (Intercept) \n    1357.56 \n    -997.12 \n    -909.30 \n    -383.44 \n    -0.71 \n  \n  \n    statusshare \n    -997.12 \n    9399.81 \n    910.48 \n    -15.30 \n    0.24 \n  \n  \n    statusmixed \n    -909.30 \n    910.48 \n    4083.37 \n    -146.28 \n    0.16 \n  \n  \n    land \n    -383.44 \n    -15.30 \n    -146.28 \n    12437.02 \n    -12.75 \n  \n  \n    labor \n    -0.71 \n    0.24 \n    0.16 \n    -12.75 \n    0.02 \n  \n\n\n\n\n\nRobust vcov\n\nlibrary(sandwich)\nvcovHC(fit1, type = \"HC3\")\n\n\n\n\n\n \n  \n      \n    (Intercept) \n    statusshare \n    statusmixed \n    land \n    labor \n  \n \n\n  \n    (Intercept) \n    4235.06 \n    -3084.98 \n    249.07 \n    -6038.87 \n    -5.91 \n  \n  \n    statusshare \n    -3084.98 \n    11437.98 \n    799.53 \n    -2421.05 \n    8.76 \n  \n  \n    statusmixed \n    249.07 \n    799.53 \n    2692.17 \n    1349.86 \n    -4.71 \n  \n  \n    land \n    -6038.87 \n    -2421.05 \n    1349.86 \n    75901.71 \n    -62.24 \n  \n  \n    labor \n    -5.91 \n    8.76 \n    -4.71 \n    -62.24 \n    0.09"
  },
  {
    "objectID": "slides/w04-mlr-part2.html#homoscedasticity",
    "href": "slides/w04-mlr-part2.html#homoscedasticity",
    "title": "Multiple Linear Regression: practical aspects",
    "section": "Homoscedasticity",
    "text": "Homoscedasticity\n\\[\nVar(u|x_{i1}, x_{i2}, \\cdots , x_{ik}) = \\sigma^2\n\\]"
  },
  {
    "objectID": "slides/w06-DiD.html#the-basics",
    "href": "slides/w06-DiD.html#the-basics",
    "title": "Difference in Difference (DID)",
    "section": "The basics:",
    "text": "The basics:\n\nThe ceteris Paribus\nSelection Bias\nOVB"
  },
  {
    "objectID": "slides/w06-DiD.html#did",
    "href": "slides/w06-DiD.html#did",
    "title": "Difference in Difference (DID)",
    "section": "DID",
    "text": "DID\n\n\n\nSource: (de Janvry & Sadoulet, 2021, Chapter 4)"
  },
  {
    "objectID": "slides/w06-DiD.html#assumptions",
    "href": "slides/w06-DiD.html#assumptions",
    "title": "Difference in Difference (DID)",
    "section": "Assumptions",
    "text": "Assumptions\n\nParallel trends\nLinear Additive Effect\nAll other assumption of the linear regression"
  },
  {
    "objectID": "slides/w06-DiD.html#learn-more-about-did",
    "href": "slides/w06-DiD.html#learn-more-about-did",
    "title": "Difference in Difference (DID)",
    "section": "Learn more about DID:",
    "text": "Learn more about DID:\nWatch: Introduction to Differences-in-Differences https://youtu.be/eiffOVbYvNc\nRead:\n\n(Angrist & Pischke, 2014, Chapter 5)\n(Angrist & Pischke, 2009, also Chapter 5)\n(de Janvry & Sadoulet, 2021, Chapter 4)\n(Card & Krueger, 1994)"
  },
  {
    "objectID": "slides/w06-DiD.html#research-question-and-the-setting",
    "href": "slides/w06-DiD.html#research-question-and-the-setting",
    "title": "Difference in Difference (DID)",
    "section": "Research question and the setting",
    "text": "Research question and the setting\n\n\n\n\n\nTwo states: New Jersey (NJ) and Pennsylvanian (PA)\nTwo time periods: Before and after\nTreatment: Minimal Wage in one state (New Jersey) only"
  },
  {
    "objectID": "slides/w06-DiD.html#geography",
    "href": "slides/w06-DiD.html#geography",
    "title": "Difference in Difference (DID)",
    "section": "Geography",
    "text": "Geography"
  },
  {
    "objectID": "slides/w06-DiD.html#empirical-problem",
    "href": "slides/w06-DiD.html#empirical-problem",
    "title": "Difference in Difference (DID)",
    "section": "Empirical problem",
    "text": "Empirical problem\n\\[\nY_{jt} = \\beta_0 + \\beta_1 \\text{NJ}_j  + \\beta_2 \\text{Time}_t  + \\beta_3 (\\text{NJ}_j \\times \\text{Time}_t) + \\beta_4 X_{jt} + \\epsilon_{jt}\n\\]\nWhere:\n\nOutcome: \\(Y_{jt}\\) - Employment level at the key fast food chains, Full Time Equivalent persons.\nTreatment: Minimal wage policy in NJ \\(\\text{NJ}_j\\) indicator variable where 1 = NJ, and 0 = PA\nTime: $ _t$ indicator variable where 0 = before, and 1 = after\n\n\nWhat are we expecting to find?\n\n\n\nMP223-Applied Econometrics (SoSe 2023). Author: Eduard Bukin (2023). | GitHub | StudIP | ILIAS.\n\n\n\nAngrist, J. D., & Pischke, J.-S. (2009). Mostly harmless econometrics. Princeton University Press. http://doi.org/10.1515/9781400829828\n\n\nAngrist, J. D., & Pischke, J.-S. (2014). Mastering’metrics: The path from cause to effect. Princeton University Press.\n\n\nCard, D., & Krueger, A. B. (1994). Minimum wages and employment: A case study of the fast-food industry in new jersey and pennsylvania. The American Economic Review, 84(4), 772–793. Retrieved from http://www.jstor.org/stable/2118030\n\n\nde Janvry, A., & Sadoulet, E. (2021). Development Economics : Theory and Practice. Taylor & Francis Group.\n\n\nOtrachshenko, V., Popova, O., & Tavares, J. (2016). Psychological costs of currency transition: Evidence from the euro adoption. European Journal of Political Economy, 45, 89–100. http://doi.org/10.1016/j.ejpoleco.2016.10.002"
  },
  {
    "objectID": "slides/w06-interactions-DiD.html#the-basics",
    "href": "slides/w06-interactions-DiD.html#the-basics",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "The basics:",
    "text": "The basics:\n\nThe ceteris Paribus\nSelection Bias\nOVB\nQuasi-experimental design: DID\n\nWhy do we need it?"
  },
  {
    "objectID": "slides/w06-interactions-DiD.html#interaction-term",
    "href": "slides/w06-interactions-DiD.html#interaction-term",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Interaction term",
    "text": "Interaction term\n\\[\ny = \\beta_0 + \\beta_1 x_1  + \\beta_2 x_2 + \\beta_3 (x_1 \\cdot x_2) + \\beta_4 x_4 + \\epsilon\n\\]\n\nInterpretation is only possible through marginal effects:\n\nPartial derivative of the regression equation with respect to a regressor of interest.\n\n\\(\\frac{\\partial y}{\\partial x_1} = \\beta_1 + \\beta_3 x_2\\) or \\(\\frac{\\partial y}{\\partial x_2} = \\beta_2 + \\beta_3 x_1\\)\n\n\nEffect (slope) of \\(x_1\\) depends on \\(x_2\\) and vice versa."
  },
  {
    "objectID": "slides/w06-interactions-DiD.html#recall-the-hedonic-land-prices-model",
    "href": "slides/w06-interactions-DiD.html#recall-the-hedonic-land-prices-model",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Recall the hedonic land prices model:",
    "text": "Recall the hedonic land prices model:\n\\[\n\\text{acrePrice} = \\beta_0 +  \\beta_1 \\text{crpPct} + \\beta_2 \\text{acres} + \\beta_3 \\text{improvements} \\\\\n+ \\beta_4 \\text{year} + \\beta_5 \\text{region}  + e\n\\]\n\nacrePrice - sale price in dollars per acre;\ncrpPct - the percentage of all farm acres enrolled in CRP;\nacres - size of the farm in acres;\nimprovements - share of infrastructure’s value in the land price;\nregion - region in the state Minnesota;\nyear - year of the land sales translation;"
  },
  {
    "objectID": "slides/w06-interactions-DiD.html#loading-data",
    "href": "slides/w06-interactions-DiD.html#loading-data",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Loading data",
    "text": "Loading data\n\n\n\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(modelsummary)\nlibrary(ggeffects)\nlibrary(marginaleffects)\nlibrary(GGally)\noptions(modelsummary_get = \"broom\")\n## 1. Load the data\ndta <- read_excel(\"land-prices.xlsx\") %>%\n  mutate(improvements  = as.numeric(improvements),\n         development = as.factor(as.integer(improvements > 25)),\n         productivity  = as.numeric(productivity),\n         tillable  = as.numeric(tillable),\n         year = as.factor(year)) %>% \n  select(acrePrice, crpPct, acres, improvements, development, region, year)\n\nglimpse(dta)"
  },
  {
    "objectID": "slides/w06-interactions-DiD.html#descriptive-statistics",
    "href": "slides/w06-interactions-DiD.html#descriptive-statistics",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics\n\ndatasummary(\n    acrePrice + crpPct + acres + improvements + as.integer(development) ~ \n      (mean + sd + min + median + max) * \n      Arguments(na.rm = TRUE),\n    data = dta, \n    output = \"html\"\n  )\n\n\n\n\n\n \n  \n      \n    mean \n    sd \n    min \n    median \n    max \n  \n \n\n  \n    acrePrice \n    2787.33 \n    1914.04 \n    108.00 \n    2442.00 \n    15000.00 \n  \n  \n    crpPct \n    4.16 \n    17.17 \n    0.00 \n    0.00 \n    100.00 \n  \n  \n    acres \n    112.69 \n    128.46 \n    1.00 \n    80.00 \n    6970.00 \n  \n  \n    improvements \n    4.49 \n    12.94 \n    0.00 \n    0.00 \n    100.00 \n  \n  \n    as.integer(development) \n    1.08 \n    0.26 \n    1.00 \n    1.00 \n    2.00"
  },
  {
    "objectID": "slides/w06-interactions-DiD.html#descriptive-plots",
    "href": "slides/w06-interactions-DiD.html#descriptive-plots",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Descriptive Plots",
    "text": "Descriptive Plots\n\ndta %>%\n  select(acrePrice, crpPct, acres, improvements, development) %>%\n  ggpairs(aes(colour = development, alpha = 0.2))"
  },
  {
    "objectID": "slides/w06-interactions-DiD.html#basic-regressions",
    "href": "slides/w06-interactions-DiD.html#basic-regressions",
    "title": "Difference in Difference (DID)",
    "section": "Basic regressions",
    "text": "Basic regressions\n\nfit1 <- \n  lm(acrePrice  ~ acres + development + crpPct + region + factor(year),\n     data = dta)\n\n\ncust_summ <- function(x, coef_omit = \"reg|year\", ...) {\n  x <- set_names(x, str_c(\"Model \", seq_along(x)))\n  all_eq <- x %>% imap_chr(. , ~ {str_c(.y, \": \", as.character(.x$call)[[2]])}) %>% \n    str_c(collapse = \"\\n\")\n  \n  modelsummary(\n    x,\n    estimate = \"{estimate}{stars} ({std.error})\",\n    statistic = NULL,\n    output = \"html\",\n    gof_omit =  c(\"AIC|BIC|Log|F|RMS\"), \n    coef_omit = coef_omit,\n    notes = all_eq,\n    ...\n  )\n}"
  },
  {
    "objectID": "slides/w06-interactions-DiD.html#interprete-the-baseline-results",
    "href": "slides/w06-interactions-DiD.html#interprete-the-baseline-results",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Interprete the baseline results",
    "text": "Interprete the baseline results\n\ncust_summ(list(fit1))\n\n\n\n \n  \n      \n    Model 1 \n  \n \n\n  \n    (Intercept) \n    2019.877*** (65.416) \n  \n  \n    acres \n    −0.857*** (0.086) \n  \n  \n    development1 \n    1555.287*** (41.467) \n  \n  \n    crpPct \n    −8.892*** (0.645) \n  \n  \n    Num.Obs. \n    18650 \n  \n  \n    R2 \n    0.413 \n  \n  \n    R2 Adj. \n    0.412 \n  \n\n\n Model 1: acrePrice ~ acres + development + crpPct + region + factor(year)"
  },
  {
    "objectID": "slides/w06-interactions-DiD.html#fitted-values-vs-predictors",
    "href": "slides/w06-interactions-DiD.html#fitted-values-vs-predictors",
    "title": "Difference in Difference (DID)",
    "section": "Fitted values vs predictors",
    "text": "Fitted values vs predictors\n\n\n\nlibrary(ggeffects)\nggpredict(fit1, terms = \"acres\") %>%\n  plot() + \n  ylab(\"Predicted price per acre (acrePrice), USD\")+ \n  xlab(\"Parcel size (acres), acres\")\n\n\n\n\n\n\n\n\n\n\nggpredict(fit1, terms = \"development\") %>%\n  plot() + \n  ylab(\"Predicted price per acre (acrePrice), USD\")+ \n  xlab(\"Development, 0 = no 1 = yes\")"
  },
  {
    "objectID": "slides/w06-interactions-DiD.html#fitted-values-vs-predictors-2",
    "href": "slides/w06-interactions-DiD.html#fitted-values-vs-predictors-2",
    "title": "Difference in Difference (DID)",
    "section": "Fitted values vs predictors (2)",
    "text": "Fitted values vs predictors (2)\n\nggpredict(fit1, terms = c(\"acres\", \"development\")) %>%\n  plot() + \n  ylab(\"Predicted price per acre (acrePrice), USD\")+ \n  xlab(\"Parcel size (acres), acres\")"
  },
  {
    "objectID": "slides/w06-interactions-DiD.html#adding-an-interaction-term",
    "href": "slides/w06-interactions-DiD.html#adding-an-interaction-term",
    "title": "Difference in Difference (DID)",
    "section": "Adding an interaction term",
    "text": "Adding an interaction term\n\nfit2 <- \n  lm(acrePrice  ~ acres * development + crpPct + region + factor(year),\n     data = dta)\ncust_summ(list(fit1, fit2))\n\n\n\n\n\n \n  \n      \n    Model 1 \n     Model 2 \n  \n \n\n  \n    (Intercept) \n    2019.877*** (65.416) \n    1997.055*** (64.413) \n  \n  \n    acres \n    −0.857*** (0.086) \n    −0.652*** (0.085) \n  \n  \n    development1 \n    1555.287*** (41.467) \n    2978.491*** (71.424) \n  \n  \n    crpPct \n    −8.892*** (0.645) \n    −9.073*** (0.635) \n  \n  \n    acres × development1 \n     \n    −18.053*** (0.743) \n  \n  \n    Num.Obs. \n    18650 \n    18650 \n  \n  \n    R2 \n    0.413 \n    0.431 \n  \n  \n    R2 Adj. \n    0.412 \n    0.430 \n  \n\n\n Model 1: acrePrice ~ acres + development + crpPct + region + factor(year)\nModel 2: acrePrice ~ acres * development + crpPct + region + factor(year)"
  },
  {
    "objectID": "slides/w06-interactions-DiD.html#fitted-values-vs-predictors-with-interaction-term",
    "href": "slides/w06-interactions-DiD.html#fitted-values-vs-predictors-with-interaction-term",
    "title": "Difference in Difference (DID)",
    "section": "Fitted values vs predictors with interaction term",
    "text": "Fitted values vs predictors with interaction term\n\n\n\nggpredict(fit2, terms = c(\"development\")) %>%\n  plot() + \n  ylab(\"Predicted price per acre (acrePrice), USD\") + \n  xlab(\"Development, 0 = no 1 = yes\")\n\n\n\n\n\n\n\n\n\n\nggpredict(fit2, terms = c(\"development\", \"acres [0, 10, 50, 100, 200, 250]\")) %>%\n  plot() + \n  ylab(\"Predicted price per acre (acrePrice), USD\") + \n  xlab(\"Development, 0 = no 1 = yes\")"
  },
  {
    "objectID": "slides/w06-interactions-DiD.html#did",
    "href": "slides/w06-interactions-DiD.html#did",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "DID",
    "text": "DID\n\n\n\nSource: (de Janvry & Sadoulet, 2021, Chapter 4)"
  },
  {
    "objectID": "slides/w06-interactions-DiD.html#assumptions",
    "href": "slides/w06-interactions-DiD.html#assumptions",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Assumptions",
    "text": "Assumptions\n\nParallel trends\nLinear Additive Effect\nAll other assumption of the linear regression"
  },
  {
    "objectID": "slides/w06-interactions-DiD.html#learn-more-about-did",
    "href": "slides/w06-interactions-DiD.html#learn-more-about-did",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Learn more about DID:",
    "text": "Learn more about DID:\nWatch: Introduction to Differences-in-Differences https://youtu.be/eiffOVbYvNc\nRead:\n\n(Angrist & Pischke, 2014, Chapter 5)\n(Angrist & Pischke, 2009, also Chapter 5)\n(de Janvry & Sadoulet, 2021, Chapter 4)\n(Card & Krueger, 1994)"
  },
  {
    "objectID": "slides/w06-interactions-DiD.html#research-question-and-the-setting",
    "href": "slides/w06-interactions-DiD.html#research-question-and-the-setting",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Research question and the setting",
    "text": "Research question and the setting\n\nCausal effect of the minimum wage policy on employmnet is not negative.\n\n\n\nTwo states: New Jersey (NJ) and Pennsylvanian (PA)\nTwo time periods: Before and after\nTreatment: Minimal Wage in one state (New Jersey) only"
  },
  {
    "objectID": "slides/w06-interactions-DiD.html#geography",
    "href": "slides/w06-interactions-DiD.html#geography",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Geography",
    "text": "Geography"
  },
  {
    "objectID": "slides/w06-interactions-DiD.html#empirical-problem",
    "href": "slides/w06-interactions-DiD.html#empirical-problem",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Empirical problem",
    "text": "Empirical problem\n\\[\nY_{jt} = \\beta_0 + \\beta_1 \\text{NJ}_j  + \\beta_2 \\text{Time}_t  + \\beta_3 (\\text{NJ}_j \\times \\text{Time}_t) + \\beta_4 X_{jt} + \\epsilon_{jt}\n\\]\nWhere:\n\nOutcome: \\(Y_{jt}\\) - Employment level at the key fast food chains, Full Time Equivalent persons.\nTreatment: Minimal wage policy in NJ \\(\\text{NJ}_j\\) indicator variable where 1 = NJ, and 0 = PA.\nTime: $ _t$ indicator variable where 0 = before, and 1 = after.\n\n\n\nWhat are we expecting to find?\nWhat are the expected values of the coefficients?\n\nIf the minimal wage policy reduces employmnet.\nIf the minimal wage policy increases employmnet."
  },
  {
    "objectID": "slides/w06-interactions-DiD.html#practical-exercise-in-r",
    "href": "slides/w06-interactions-DiD.html#practical-exercise-in-r",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Practical exercise in R",
    "text": "Practical exercise in R\nFollow pre-recorded video and do it at home."
  },
  {
    "objectID": "slides/w06-interactions-DiD.html#references-1",
    "href": "slides/w06-interactions-DiD.html#references-1",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "References",
    "text": "References\n\n\n\nMP223-Applied Econometrics (SoSe 2023). Author: Eduard Bukin (2023). | GitHub | StudIP | ILIAS.\n\n\n\nAngrist, J. D., & Pischke, J.-S. (2009). Mostly harmless econometrics. Princeton University Press. http://doi.org/10.1515/9781400829828\n\n\nAngrist, J. D., & Pischke, J.-S. (2014). Mastering’metrics: The path from cause to effect. Princeton University Press.\n\n\nCard, D., & Krueger, A. B. (1994). Minimum wages and employment: A case study of the fast-food industry in new jersey and pennsylvania. The American Economic Review, 84(4), 772–793. Retrieved from http://www.jstor.org/stable/2118030\n\n\nde Janvry, A., & Sadoulet, E. (2021). Development Economics : Theory and Practice. Taylor & Francis Group.\n\n\nOtrachshenko, V., Popova, O., & Tavares, J. (2016). Psychological costs of currency transition: Evidence from the euro adoption. European Journal of Political Economy, 45, 89–100. http://doi.org/10.1016/j.ejpoleco.2016.10.002"
  },
  {
    "objectID": "slides/w06-interactions-DiD.html#reporting-results-with-marginal-effects",
    "href": "slides/w06-interactions-DiD.html#reporting-results-with-marginal-effects",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Reporting results with marginal effects",
    "text": "Reporting results with marginal effects\n\nReport coefficients \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\) as is.\nReport marginal effects at mean values of the interaction terms:\n\n\\(\\text{ME}(x_1) = \\frac{\\partial y}{\\partial x_1} = \\beta_1 + \\beta_3 \\bar x_2\\)\n\\(\\text{ME}(x_2) = \\frac{\\partial y}{\\partial x_2} = \\beta_2 + \\beta_3 \\bar x_1\\)\n\nTo compute standard errors for the marginal eff, we use:\n\nDelta method or Bootstrapping.\n\n\nLearn more: (Angrist & Pischke, 2009, also Chapter 3)"
  },
  {
    "objectID": "slides/w06-interactions-DiD.html#example-1.-basic-regressions",
    "href": "slides/w06-interactions-DiD.html#example-1.-basic-regressions",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Example 1. Basic regressions",
    "text": "Example 1. Basic regressions\n\nfit1 <- \n  lm(acrePrice  ~ acres + development + crpPct + region + factor(year),\n     data = dta)\n\nsumm <-\n  function(x, coef_omit = \"reg|year\", output = \"html\", notes = NULL, ...) {\n    modelsummary(\n      x,\n      estimate = \"{estimate}{stars} ({std.error})\",\n      statistic = NULL,\n      output = output,\n      gof_omit =  c(\"AIC|BIC|Log|F|RMS\"),\n      coef_omit = coef_omit,\n      notes = notes,\n      ...\n    )\n  }\n\ncust_summ <- function(x, coef_omit = \"reg|year\", output = \"html\", ...) {\n  x <- set_names(x, str_c(\"Model \", seq_along(x)))\n  all_eq <- x %>% imap_chr(. , ~ {str_c(.y, \": \", as.character(.x$call)[[2]])}) %>% \n    str_c(collapse = \"\\n\")\n  summ(x, coef_omit = coef_omit, output = output, notes = all_eq, ...)\n}"
  },
  {
    "objectID": "slides/w06-interactions-DiD.html#fitted-vs-area-and-development",
    "href": "slides/w06-interactions-DiD.html#fitted-vs-area-and-development",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Fitted vs area and development",
    "text": "Fitted vs area and development\n\n\n\n\nlibrary(ggeffects)\nggpredict(fit1, terms = \"acres\") %>%\n  plot() + \n  ylab(\"Predicted price per acre (acrePrice), USD\")+ \n  xlab(\"Parcel size (acres), acres\")\n\n\n\n\n\n\n\n\n\n\n\n\nggpredict(fit1, terms = \"development\") %>%\n  plot() + \n  ylab(\"Predicted price per acre (acrePrice), USD\")+ \n  xlab(\"Development, 0 = no 1 = yes\")"
  },
  {
    "objectID": "slides/w06-interactions-DiD.html#fitted-vs-area-and-development-2",
    "href": "slides/w06-interactions-DiD.html#fitted-vs-area-and-development-2",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Fitted vs area and development (2)",
    "text": "Fitted vs area and development (2)\n\nggpredict(fit1, terms = c(\"acres\", \"development\")) %>%\n  plot() + \n  ylab(\"Predicted price per acre (acrePrice), USD\") + \n  xlab(\"Parcel size (acres), acres\")\n\n\n\n\n\n\n\n\n\n\n\n\nWithout an interaction term, dummy variables simply cause shifts in the outcome variable."
  },
  {
    "objectID": "slides/w06-interactions-DiD.html#example-2.-interaction-term-with-a-binary-variable",
    "href": "slides/w06-interactions-DiD.html#example-2.-interaction-term-with-a-binary-variable",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Example 2. Interaction term with a binary variable",
    "text": "Example 2. Interaction term with a binary variable\n\nfit2 <- \n  lm(acrePrice  ~ acres * development + crpPct + region + factor(year),\n     data = dta)\ncust_summ(list(fit1, fit2))\n\n\n\n\n\n \n  \n      \n    Model 1 \n     Model 2 \n  \n \n\n  \n    (Intercept) \n    2019.877*** (65.416) \n    1997.055*** (64.413) \n  \n  \n    acres \n    −0.857*** (0.086) \n    −0.652*** (0.085) \n  \n  \n    development1 \n    1555.287*** (41.467) \n    2978.491*** (71.424) \n  \n  \n    crpPct \n    −8.892*** (0.645) \n    −9.073*** (0.635) \n  \n  \n    acres × development1 \n     \n    −18.053*** (0.743) \n  \n  \n    Num.Obs. \n    18650 \n    18650 \n  \n  \n    R2 \n    0.413 \n    0.431 \n  \n  \n    R2 Adj. \n    0.412 \n    0.430 \n  \n\n\n Model 1: acrePrice ~ acres + development + crpPct + region + factor(year)\nModel 2: acrePrice ~ acres * development + crpPct + region + factor(year)"
  },
  {
    "objectID": "slides/w06-interactions-DiD.html#fitted-vs-development-with-an-interaction-term",
    "href": "slides/w06-interactions-DiD.html#fitted-vs-development-with-an-interaction-term",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Fitted vs development with an interaction term",
    "text": "Fitted vs development with an interaction term\n\n\n\n\nggpredict(fit2, terms = c(\"development\")) %>%\n  plot() + \n  ylab(\"Predicted price per acre (acrePrice), USD\") + \n  xlab(\"Development, 0 = no 1 = yes\")\n\n\n\n\n\n\n\n\n\n\n\n\nggpredict(fit2, terms = c(\"development\", \"acres [0, 10, 50, 100, 200, 250, 500]\")) %>%\n  plot() + \n  ylab(\"Predicted price per acre (acrePrice), USD\") + \n  xlab(\"Development, 0 = no 1 = yes\")"
  },
  {
    "objectID": "slides/w06-interactions-DiD.html#fitted-vs-area-with-an-interaction-term",
    "href": "slides/w06-interactions-DiD.html#fitted-vs-area-with-an-interaction-term",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Fitted vs area with an interaction term",
    "text": "Fitted vs area with an interaction term\n\n\n\n\nggpredict(fit2, terms = c(\"acres [0, 10, 50, 100, 200, 250, 500, 1000]\")) %>%\n  plot() + \n  ylab(\"Predicted price per acre (acrePrice), USD\") + \n  xlab(\"Parcel size (acres), acres\")\n\n\n\n\n\n\n\n\n\n\n\n\nggpredict(fit2, terms = c(\"acres [0, 10, 50, 100, 200, 250, 500, 1000]\", \"development\")) %>%\n  plot() + \n  ylab(\"Predicted price per acre (acrePrice), USD\") + \n  xlab(\"Parcel size (acres), acres\")"
  },
  {
    "objectID": "slides/w06-interactions-DiD.html#marginal-effect-acres-and-development-at-means",
    "href": "slides/w06-interactions-DiD.html#marginal-effect-acres-and-development-at-means",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Marginal Effect acres and development at means",
    "text": "Marginal Effect acres and development at means\n\nlibrary(marginaleffects)\nmodelsummary(fit2)\nmodelsummary(marginaleffects(fit2))\n\n\n\n\n\n\n\nCoef. as is.\nM.E. at means\n\n\n\n\nacres\n-0.652*** (0.085)\n-2.017*** (0.097)\n\n\ncrpPct\n-9.073*** (0.635)\n-9.073*** (0.635)\n\n\ndevelopment1\n2978.491*** (71.424)\n944.727*** (47.947)\n\n\nacres * development1\n-18.053*** (0.743)\n\n\n\nNum.Obs.\n18650\n18650\n\n\nR2\n0.431\n0.431\n\n\nR2 Adj.\n0.430\n0.430"
  },
  {
    "objectID": "slides/w06-interactions-DiD.html#marginal-effect-acres-and-development-visually",
    "href": "slides/w06-interactions-DiD.html#marginal-effect-acres-and-development-visually",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Marginal Effect acres and development (visually)",
    "text": "Marginal Effect acres and development (visually)\n\n\n\n\nplot_slopes(fit2, variables = \"acres\", condition = \"development\") + \n  ylab(\"Marginal effect of acres (slope)\") + xlab(\"Development, 0 = no 1 = yes\")\n\n\n\n\n\n\n\n\n\n\n\n\nplot_slopes(fit2, variables = \"development\", condition = \"acres\") + \n  ylab(\"Marginal effect of development dummy (slope)\") + xlab(\"Parcel size, acres\") + \n  scale_y_continuous(n.breaks = 10)"
  },
  {
    "objectID": "slides/w06-interactions-DiD.html#example-1.-interaction-with-a-continious-variable",
    "href": "slides/w06-interactions-DiD.html#example-1.-interaction-with-a-continious-variable",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Example 1. Interaction with a continious variable",
    "text": "Example 1. Interaction with a continious variable\n\nfit3 <- \n  lm(acrePrice  ~ acres * improvements + crpPct + region + factor(year),\n     data = dta)\ncust_summ(list(fit1, fit2, fit3), output = \"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel 1\nModel 2\nModel 3\n\n\n\n\n(Intercept)\n2019.877*** (65.416)\n1997.055*** (64.413)\n1923.131*** (64.085)\n\n\nacres\n-0.857*** (0.086)\n-0.652*** (0.085)\n-0.276** (0.089)\n\n\ndevelopment1\n1555.287*** (41.467)\n2978.491*** (71.424)\n\n\n\ncrpPct\n-8.892*** (0.645)\n-9.073*** (0.635)\n-8.980*** (0.631)\n\n\nacres × development1\n\n-18.053*** (0.743)\n\n\n\nimprovements\n\n\n56.695*** (1.307)\n\n\nacres × improvements\n\n\n-0.277*** (0.014)\n\n\nNum.Obs.\n18650\n18650\n18650\n\n\nR2\n0.413\n0.431\n0.438\n\n\nR2 Adj.\n0.412\n0.430\n0.438\n\n\n\nNote: ^^ Model 1: acrePrice ~ acres + development + crpPct + region + factor(year) Model 2: acrePrice ~ acres * development + crpPct + region + factor(year) Model 3: acrePrice ~ acres * improvements + crpPct + region + factor(year)"
  },
  {
    "objectID": "slides/w06-interactions-DiD.html#fitted-vs-area-improvements",
    "href": "slides/w06-interactions-DiD.html#fitted-vs-area-improvements",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Fitted vs area * improvements",
    "text": "Fitted vs area * improvements\n\n\n\n\nggpredict(fit3, terms = c(\"acres [0, 10, 50, 100, 200, 250, 500, 1000]\", \"improvements [0, 10, 50, 100]\")) %>%\n  plot() + \n  ylab(\"Predicted price per acre (acrePrice), USD\") + \n  xlab(\"Parcel size (acres), acres\")\n\n\n\n\n\n\n\n\n\n\n\n\nggpredict(fit3, terms = c(\"improvements\", \"acres [1, 50, 100, 200, 500]\")) %>%\n  plot() + \n  ylab(\"Predicted price per acre (acrePrice), USD\") + \n  xlab(\"Improvements, % of price due to infrastructure\")"
  },
  {
    "objectID": "slides/w06-interactions-DiD.html#marginal-effect-acres-improvements-visually",
    "href": "slides/w06-interactions-DiD.html#marginal-effect-acres-improvements-visually",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Marginal Effect acres * improvements (visually)",
    "text": "Marginal Effect acres * improvements (visually)\n\n\n\n\nplot_slopes(fit3, variables = \"acres\", condition = \"improvements\") + \n  ylab(\"Marginal effect of acres (slope)\")  + \n  xlab(\"Improvements, % of price due to infrastructure\")  + \n  scale_y_continuous(n.breaks = 10) \n\n\n\n\n\n\n\n\n\n\n\n\nplot_slopes(fit3, variables = \"improvements\", condition = \"acres\") + \n  ylab(\"Marginal effect of improvements (slope)\") + xlab(\"Parcel size, acres\") + \n  scale_y_continuous(n.breaks = 10)"
  },
  {
    "objectID": "slides/w06-interactions-DiD.html#reporting-all-regressions-results",
    "href": "slides/w06-interactions-DiD.html#reporting-all-regressions-results",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Reporting all regressions results",
    "text": "Reporting all regressions results\n\n\n\n\n \n  \n      \n    Model 1 (no interaction) \n     Model 2 (Area*Development) \n     Model 3 (Area*Improvements) \n  \n \n\n  \n    Area, acres (*) \n    −0.857* (0.350) \n    −2.017*** (0.097) \n    −1.520*** (0.089) \n  \n  \n    crpPct \n    −8.892*** (1.998) \n    −9.073*** (0.635) \n    −8.980*** (0.631) \n  \n  \n    Development status, dummy (*) \n     \n    944.727*** (47.947) \n     \n  \n  \n    Share infrastructure in land price, % (*) \n     \n     \n    25.490*** (0.984) \n  \n  \n    Num.Obs. \n    18650 \n    18650 \n    18650 \n  \n  \n    R2 \n    0.413 \n    0.431 \n    0.438 \n  \n  \n    R2 Adj. \n    0.412 \n    0.430 \n    0.438 \n  \n\n\n (*) marginal effects of the coefficeints are reported at means of the corresponding interaction terms. Robist standard errors clustered at region are reported in brackets. For marginal effects, standard errors are estimated using delta method."
  },
  {
    "objectID": "slides/w06-interactions-DiD.html#takeaway-and-homework",
    "href": "slides/w06-interactions-DiD.html#takeaway-and-homework",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Takeaway and homework",
    "text": "Takeaway and homework\n\nMarginal effects.\nDifference between fitted values and marginal effects.\nHomework:\n\nReproduce code from the slides.\nFollow pre-recorded materials with extra calculations."
  },
  {
    "objectID": "slides/w06b-DiD.html#the-basics",
    "href": "slides/w06b-DiD.html#the-basics",
    "title": "Difference in Difference",
    "section": "The basics:",
    "text": "The basics:\n\nThe ceteris Paribus\nSelection Bias\nOVB\nQuasi-experimental design: DID\n\nWhy do we need it?"
  },
  {
    "objectID": "slides/w06b-DiD.html#did",
    "href": "slides/w06b-DiD.html#did",
    "title": "Difference in Difference",
    "section": "DID",
    "text": "DID\n\n\n\nSource: (de Janvry & Sadoulet, 2021, Chapter 4)"
  },
  {
    "objectID": "slides/w06b-DiD.html#assumptions",
    "href": "slides/w06b-DiD.html#assumptions",
    "title": "Difference in Difference",
    "section": "Assumptions",
    "text": "Assumptions\n\nParallel trends\nLinear Additive Effect\nAll GM assumption of the linear regression"
  },
  {
    "objectID": "slides/w06b-DiD.html#learn-more-about-did",
    "href": "slides/w06b-DiD.html#learn-more-about-did",
    "title": "Difference in Difference",
    "section": "Learn more about DID:",
    "text": "Learn more about DID:\nWatch: Introduction to Differences-in-Differences https://youtu.be/eiffOVbYvNc\nRead:\n\n(Angrist & Pischke, 2014, Chapter 5)\n(Angrist & Pischke, 2009, also Chapter 5)\n(de Janvry & Sadoulet, 2021, Chapter 4)\n(Card & Krueger, 1994)"
  },
  {
    "objectID": "slides/w06b-DiD.html#research-question-and-the-setting",
    "href": "slides/w06b-DiD.html#research-question-and-the-setting",
    "title": "Difference in Difference",
    "section": "Research question and the setting",
    "text": "Research question and the setting\n\nCausal effect of the minimum wage policy on employmnet is not negative.\n\n\n\nTwo states: New Jersey (NJ) and Pennsylvanian (PA)\nTwo time periods: Before and after\nTreatment: Minimal Wage in one state (New Jersey) only"
  },
  {
    "objectID": "slides/w06b-DiD.html#geography",
    "href": "slides/w06b-DiD.html#geography",
    "title": "Difference in Difference",
    "section": "Geography",
    "text": "Geography"
  },
  {
    "objectID": "slides/w06b-DiD.html#empirical-problem",
    "href": "slides/w06b-DiD.html#empirical-problem",
    "title": "Difference in Difference",
    "section": "Empirical problem",
    "text": "Empirical problem\n\\[\nY_{jt} = \\beta_0 + \\beta_1 \\text{NJ}_j  + \\beta_2 \\text{Time}_t  + \\beta_3 (\\text{NJ}_j \\times \\text{Time}_t) + \\beta_4 X_{jt} + \\epsilon_{jt}\n\\]\nWhere:\n\nOutcome: \\(Y_{jt}\\) - Employment level at the key fast food chains, Full Time Equivalent persons.\nTreatment: Minimal wage policy in NJ \\(\\text{NJ}_j\\) indicator variable where 1 = NJ, and 0 = PA.\nTime: \\(\\text{Time}_t\\) indicator variable where 0 = before, and 1 = after.\n\n\n\nWhat are we expecting to find?\nWhat are the expected values of the coefficients?\n\nIf the minimal wage policy reduces employmnet.\nIf the minimal wage policy increases employmnet."
  },
  {
    "objectID": "slides/w06b-DiD.html#practical-exercise-in-r",
    "href": "slides/w06b-DiD.html#practical-exercise-in-r",
    "title": "Difference in Difference",
    "section": "Practical exercise in R",
    "text": "Practical exercise in R\nLet us estimate \\(\\beta_3\\) now using R and corresponding data."
  },
  {
    "objectID": "slides/w06b-DiD.html#references-1",
    "href": "slides/w06b-DiD.html#references-1",
    "title": "Difference in Difference",
    "section": "References",
    "text": "References\n\n\n\nMP223-Applied Econometrics (SoSe 2023). Author: Eduard Bukin (2023). | GitHub | StudIP | ILIAS.\n\n\n\nAngrist, J. D., & Pischke, J.-S. (2009). Mostly harmless econometrics. Princeton University Press. http://doi.org/10.1515/9781400829828\n\n\nAngrist, J. D., & Pischke, J.-S. (2014). Mastering’metrics: The path from cause to effect. Princeton University Press.\n\n\nCard, D., & Krueger, A. B. (1994). Minimum wages and employment: A case study of the fast-food industry in new jersey and pennsylvania. The American Economic Review, 84(4), 772–793. Retrieved from http://www.jstor.org/stable/2118030\n\n\nde Janvry, A., & Sadoulet, E. (2021). Development Economics : Theory and Practice. Taylor & Francis Group.\n\n\nOtrachshenko, V., Popova, O., & Tavares, J. (2016). Psychological costs of currency transition: Evidence from the euro adoption. European Journal of Political Economy, 45, 89–100. http://doi.org/10.1016/j.ejpoleco.2016.10.002"
  },
  {
    "objectID": "slides/w06a-interactions.html#the-basics",
    "href": "slides/w06a-interactions.html#the-basics",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "The basics:",
    "text": "The basics:\n\nThe ceteris Paribus\nSelection Bias\nOVB\nQuasi-experimental design: DID\n\nWhy do we need it?"
  },
  {
    "objectID": "slides/w06a-interactions.html#interaction-term",
    "href": "slides/w06a-interactions.html#interaction-term",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Interaction term",
    "text": "Interaction term\n\\[\ny = \\beta_0 + \\beta_1 x_1  + \\beta_2 x_2 + \\beta_3 (x_1 \\cdot x_2) + \\beta_4 x_4 + \\epsilon\n\\]\n\nInterpretation is only possible through marginal effects:\n\n\nMarginal effect is a partial derivative of the regression equation with respect to a regressor of interest evaluated at certain value of the interaction term.\n\n\n\\(\\frac{\\partial y}{\\partial x_1} = \\beta_1 + \\beta_3 x_2\\)\n\\(\\frac{\\partial y}{\\partial x_2} = \\beta_2 + \\beta_3 x_1\\)\n\n\nMarginal Effect (the slope) of \\(x_1\\) depends on \\(x_2\\) and vice versa."
  },
  {
    "objectID": "slides/w06a-interactions.html#reporting-results-with-marginal-effects",
    "href": "slides/w06a-interactions.html#reporting-results-with-marginal-effects",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Reporting results with marginal effects",
    "text": "Reporting results with marginal effects\n\nReport coefficients \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\) as is.\nReport marginal effects at mean values of the interaction term(s):\n\n\\(\\text{ME}(\\bar x_1) = \\beta_1 + \\beta_3 \\bar x_2\\)\n\\(\\text{ME}(\\bar x_2) = \\beta_2 + \\beta_3 \\bar x_1\\)\n\nTo compute standard errors for the marginal effects, we use delta method.\n\nLearn more: (Angrist & Pischke, 2009, also Chapter 3)"
  },
  {
    "objectID": "slides/w06a-interactions.html#recall-the-hedonic-land-prices-model",
    "href": "slides/w06a-interactions.html#recall-the-hedonic-land-prices-model",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Recall the hedonic land prices model:",
    "text": "Recall the hedonic land prices model:\n\\[\n\\text{acrePrice} = \\beta_0 +  \\beta_1 \\text{crpPct} + \\beta_2 \\text{acres} + \\beta_3 \\text{improvements} \\\\\n+ \\beta_4 \\text{year} + \\beta_5 \\text{region}  + e\n\\]\n\nacrePrice - sale price in dollars per acre;\ncrpPct - the percentage of all farm acres enrolled in CRP;\nacres - size of the farm in acres;\nimprovements - share of infrastructure’s value in the land price;\ndevelopment - dummy variable aliased with improvements. Is 1 when improvements > 25% and is 0 otherwise.\nregion - region in the state Minnesota;\nyear - year of the land sales translation;"
  },
  {
    "objectID": "slides/w06a-interactions.html#loading-data",
    "href": "slides/w06a-interactions.html#loading-data",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Loading data",
    "text": "Loading data\n\n\nWarning: package 'modelsummary' was built under R version 4.2.3\n\n\nWarning: package 'marginaleffects' was built under R version 4.2.3\n\n\nWarning: package 'GGally' was built under R version 4.2.3\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\nWarning in mask$eval_all_mutate(quo): NAs introduced by coercion\n\n\nWarning in mask$eval_all_mutate(quo): NAs introduced by coercion\n\nWarning in mask$eval_all_mutate(quo): NAs introduced by coercion\n\n\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(modelsummary)\nlibrary(ggeffects)\nlibrary(marginaleffects)\nlibrary(GGally)\noptions(modelsummary_get = \"broom\")\n## 1. Load the data\ndta <- read_excel(\"land-prices.xlsx\") %>%\n  mutate(improvements  = as.numeric(improvements),\n         development = as.factor(as.integer(improvements > 25)),\n         productivity  = as.numeric(productivity),\n         tillable  = as.numeric(tillable),\n         year = as.factor(year)) %>% \n  select(acrePrice, crpPct, acres, improvements, development, region, year)\n\nglimpse(dta)"
  },
  {
    "objectID": "slides/w06a-interactions.html#descriptive-statistics",
    "href": "slides/w06a-interactions.html#descriptive-statistics",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics\n\n\nShow the code\ndatasummary(\n    (acrePrice + crpPct + acres + improvements) * (development)  ~ \n      N + ( mean + sd + min + median + max) * \n      Arguments(na.rm = TRUE),\n    data = dta%>% \n       mutate(development = development %>% forcats::fct_explicit_na()) #, \n    # output = \"html\"\n  )\n\n\nWarning in .Primitive(\"min\")(c(NA_real_, NA_real_, NA_real_, NA_real_,\nNA_real_, : no non-missing arguments to min; returning Inf\n\n\nWarning in .Primitive(\"max\")(c(NA_real_, NA_real_, NA_real_, NA_real_,\nNA_real_, : no non-missing arguments to max; returning -Inf\n\n\n\n\n \n  \n      \n    development \n    N \n    mean \n    sd \n    min \n    median \n    max \n  \n \n\n  \n    acrePrice \n    0 \n    17239 \n    2683.35 \n    1773.92 \n    108.00 \n    2392.00 \n    15000.00 \n  \n  \n     \n    1 \n    1411 \n    4018.31 \n    2868.57 \n    230.00 \n    3153.00 \n    14960.00 \n  \n  \n     \n    (Missing) \n    50 \n    3900.24 \n    2353.14 \n    331.00 \n    3960.50 \n    15000.00 \n  \n  \n    crpPct \n    0 \n    17239 \n    4.33 \n    17.56 \n    0.00 \n    0.00 \n    100.00 \n  \n  \n     \n    1 \n    1411 \n    1.89 \n    10.44 \n    0.00 \n    0.00 \n    86.00 \n  \n  \n     \n    (Missing) \n    50 \n    9.44 \n    26.25 \n    0.00 \n    0.00 \n    100.00 \n  \n  \n    acres \n    0 \n    17239 \n    115.45 \n    132.40 \n    1.00 \n    80.00 \n    6970.00 \n  \n  \n     \n    1 \n    1411 \n    78.46 \n    52.14 \n    5.00 \n    62.00 \n    380.00 \n  \n  \n     \n    (Missing) \n    50 \n    124.20 \n    125.42 \n    7.00 \n    82.50 \n    605.00 \n  \n  \n    improvements \n    0 \n    17239 \n    1.15 \n    4.07 \n    0.00 \n    0.00 \n    25.00 \n  \n  \n     \n    1 \n    1411 \n    45.37 \n    14.18 \n    26.00 \n    42.00 \n    100.00 \n  \n  \n     \n    (Missing) \n    0"
  },
  {
    "objectID": "slides/w06a-interactions.html#descriptive-plots",
    "href": "slides/w06a-interactions.html#descriptive-plots",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Descriptive Plots",
    "text": "Descriptive Plots\n\n\nShow the code\ndta %>%\n  # filter(!is.na(development)) %>% %>% \n  # mutate(development = development %>% forcats::fct_explicit_na()) %>% \n  select(acrePrice, crpPct, acres, improvements, development) %>%\n  ggpairs(aes(colour = development, alpha = 0.2))\n\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 50 rows containing missing values\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 50 rows containing missing values\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 50 rows containing missing values\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 50 rows containing missing values\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 50 rows containing missing values\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 50 rows containing missing values\n\n\nWarning: Removed 50 rows containing missing values (`geom_point()`).\nRemoved 50 rows containing missing values (`geom_point()`).\nRemoved 50 rows containing missing values (`geom_point()`).\n\n\nWarning: Removed 50 rows containing non-finite values (`stat_density()`).\n\n\nWarning: Removed 50 rows containing non-finite values (`stat_boxplot()`).\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 50 rows containing non-finite values (`stat_bin()`)."
  },
  {
    "objectID": "slides/w06a-interactions.html#example-1.-basic-regressions",
    "href": "slides/w06a-interactions.html#example-1.-basic-regressions",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Example 1. Basic regressions",
    "text": "Example 1. Basic regressions\n\n\nShow the code\nfit1 <- \n  lm(acrePrice  ~ acres + development + crpPct + region + factor(year),\n     data =\n       dta %>% \n       mutate(\n         development1 = development,\n         development2 = development,\n         development = development %>% forcats::fct_explicit_na()) %>% \n       mutate(\n         development1_reverse = ifelse(is.na(development1), 1, 0),\n         development1 = ifelse(is.na(development1), 0, as.character(development1)) %>%\n           as.numeric()\n       )\n     \n     )\nfit1\n\n\n\nCall:\nlm(formula = acrePrice ~ acres + development + crpPct + region + \n    factor(year), data = dta %>% mutate(development1 = development, \n    development2 = development, development = development %>% \n        forcats::fct_explicit_na()) %>% mutate(development1_reverse = ifelse(is.na(development1), \n    1, 0), development1 = ifelse(is.na(development1), 0, as.character(development1)) %>% \n    as.numeric()))\n\nCoefficients:\n         (Intercept)                 acres          development1  \n           2021.4868               -0.8614             1555.0949  \ndevelopment(Missing)                crpPct       regionNorthwest  \n            183.2505               -8.8900            -2079.4434  \n regionSouth Central      regionSouth East      regionSouth West  \n            404.8314              499.2447             -208.3592  \n  regionWest Central      factor(year)2003      factor(year)2004  \n           -993.6831              317.3415              675.3764  \n    factor(year)2005      factor(year)2006      factor(year)2007  \n           1092.3309             1146.1577             1320.2950  \n    factor(year)2008      factor(year)2009      factor(year)2010  \n           1886.5410             1927.9044             2209.0035  \n    factor(year)2011  \n           2271.7464  \n\n\nShow the code\n   dta %>% \n       mutate(\n         development1 = development,\n         development2 = development,\n         development = development %>% forcats::fct_explicit_na()) %>% \n       mutate(\n         development1_reverse = ifelse(is.na(development1), 1, 0),\n         development1 = ifelse(is.na(development1), 0, as.character(development1)) %>%\n           as.numeric()\n       ) %>% \n     filter(is.na(development2))\n\n\n# A tibble: 50 × 10\n   acrePrice crpPct acres improvements development region     year  development1\n       <dbl>  <dbl> <dbl>        <dbl> <fct>       <chr>      <fct>        <dbl>\n 1       717      0   103           NA (Missing)   Northwest  2010             0\n 2      3626      0   273           NA (Missing)   Central    2011             0\n 3      4635      0    80           NA (Missing)   South Cen… 2011             0\n 4      4883      0   200           NA (Missing)   South Cen… 2010             0\n 5      4640      0    85           NA (Missing)   South Cen… 2010             0\n 6      4513      0    40           NA (Missing)   South Cen… 2011             0\n 7      4724      0    80           NA (Missing)   South Cen… 2011             0\n 8      6298     57    79           NA (Missing)   South Cen… 2011             0\n 9      8196      0    65           NA (Missing)   South Cen… 2011             0\n10      4995      0    39           NA (Missing)   South Cen… 2011             0\n# ℹ 40 more rows\n# ℹ 2 more variables: development2 <fct>, development1_reverse <dbl>\n\n\nShow the code\nsumm <-\n  function(x, coef_omit = \"reg|year\", output = \"html\", notes = NULL, ...) {\n    modelsummary(\n      x,\n      estimate = \"{estimate}{stars} ({std.error})\",\n      statistic = NULL,\n      output = output,\n      gof_omit =  c(\"AIC|BIC|Log|F|RMS\"),\n      coef_omit = coef_omit,\n      notes = notes,\n      ...\n    )\n  }\n\ncust_summ <- function(x, coef_omit = \"reg|year\", output = \"markdown\", ...) {\n  x <- set_names(x, str_c(\"Model \", seq_along(x)))\n  all_eq <- x %>% imap_chr(. , ~ {str_c(.y, \": \", as.character(.x$call)[[2]])}) %>% \n    str_c(collapse = \"</br>\")\n  summ(x, coef_omit = coef_omit, output = output, notes = all_eq, ...)\n}"
  },
  {
    "objectID": "slides/w06a-interactions.html#interpret-the-baseline-results",
    "href": "slides/w06a-interactions.html#interpret-the-baseline-results",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Interpret the baseline results",
    "text": "Interpret the baseline results\n\n\nShow the code\ncust_summ(list(fit1))\n\n\n\n\n\n\nModel 1\n\n\n\n\n(Intercept)\n2021.487*** (65.458)\n\n\nacres\n-0.861*** (0.086)\n\n\ndevelopment1\n1555.095*** (41.498)\n\n\ndevelopment(Missing)\n183.250 (210.799)\n\n\ncrpPct\n-8.890*** (0.643)\n\n\n:———————\n———————:\n\n\nNum.Obs.\n18700\n\n\nR2\n0.413\n\n\nR2 Adj.\n0.413\n\n\n\nNote: ^^ Model 1: acrePrice ~ acres + development + crpPct + region + factor(year)"
  },
  {
    "objectID": "slides/w06a-interactions.html#fitted-vs-area-and-development",
    "href": "slides/w06a-interactions.html#fitted-vs-area-and-development",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Fitted vs area and development",
    "text": "Fitted vs area and development\n\n\n\n\n\nShow the code\nlibrary(ggeffects)\nggpredict(fit1, terms = \"acres\") %>%\n  plot() + \n  ylab(\"Predicted price per acre (acrePrice), USD\")+ \n  xlab(\"Parcel size (acres), acres\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nggpredict(fit1, terms = \"development\") %>%\n  plot() + \n  ylab(\"Predicted price per acre (acrePrice), USD\")+ \n  xlab(\"Development, 0 = no 1 = yes\")"
  },
  {
    "objectID": "slides/w06a-interactions.html#fitted-vs-area-and-development-2",
    "href": "slides/w06a-interactions.html#fitted-vs-area-and-development-2",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Fitted vs area and development (2)",
    "text": "Fitted vs area and development (2)\n\n\n\n\n\nShow the code\nggpredict(fit1, terms = c(\"acres\", \"development\")) %>%\n  plot() + \n  ylab(\"Predicted price per acre (acrePrice), USD\") + \n  xlab(\"Parcel size (acres), acres\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nWithout an interaction term, dummy variables simply cause shifts in the outcome variable."
  },
  {
    "objectID": "slides/w06a-interactions.html#example-2.-interaction-term-with-a-binary-variable",
    "href": "slides/w06a-interactions.html#example-2.-interaction-term-with-a-binary-variable",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Example 2. Interaction term with a binary variable",
    "text": "Example 2. Interaction term with a binary variable\n\n\nShow the code\nfit2 <- \n  lm(acrePrice  ~ acres * development + crpPct + region + factor(year),\n     data = dta)\ncust_summ(list(fit1, fit2))\n\n\n\n\n\n\nModel 1\nModel 2\n\n\n\n\n(Intercept)\n2021.487*** (65.458)\n1997.055*** (64.413)\n\n\nacres\n-0.861*** (0.086)\n-0.652*** (0.085)\n\n\ndevelopment1\n1555.095*** (41.498)\n2978.491*** (71.424)\n\n\ndevelopment(Missing)\n183.250 (210.799)\n\n\n\ncrpPct\n-8.890*** (0.643)\n-9.073*** (0.635)\n\n\nacres × development1\n\n-18.053*** (0.743)\n\n\n:———————\n———————:\n———————:\n\n\nNum.Obs.\n18700\n18650\n\n\nR2\n0.413\n0.431\n\n\nR2 Adj.\n0.413\n0.430\n\n\n\nNote: ^^ Model 1: acrePrice ~ acres + development + crpPct + region + factor(year)Model 2: acrePrice ~ acres * development + crpPct + region + factor(year)"
  },
  {
    "objectID": "slides/w06a-interactions.html#fitted-vs-development-with-an-interaction-term",
    "href": "slides/w06a-interactions.html#fitted-vs-development-with-an-interaction-term",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Fitted vs development with an interaction term",
    "text": "Fitted vs development with an interaction term\n\n\n\n\n\nShow the code\nggpredict(fit2, terms = c(\"development\")) %>%\n  plot() + \n  ylab(\"Predicted price per acre (acrePrice), USD\") + \n  xlab(\"Development, 0 = no 1 = yes\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nggpredict(fit2, terms = c(\"development\", \"acres [0, 10, 50, 100, 200, 250, 500]\")) %>%\n  plot() + \n  ylab(\"Predicted price per acre (acrePrice), USD\") + \n  xlab(\"Development, 0 = no 1 = yes\")"
  },
  {
    "objectID": "slides/w06a-interactions.html#fitted-vs-area-with-an-interaction-term",
    "href": "slides/w06a-interactions.html#fitted-vs-area-with-an-interaction-term",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Fitted vs area with an interaction term",
    "text": "Fitted vs area with an interaction term\n\n\n\n\n\nShow the code\nggpredict(fit2, terms = c(\"acres [0, 10, 50, 100, 200, 250, 500, 1000]\")) %>%\n  plot() + \n  ylab(\"Predicted price per acre (acrePrice), USD\") + \n  xlab(\"Parcel size (acres), acres\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nggpredict(fit2, terms = c(\"acres [0, 10, 50, 100, 200, 250, 500, 1000]\", \"development\")) %>%\n  plot() + \n  ylab(\"Predicted price per acre (acrePrice), USD\") + \n  xlab(\"Parcel size (acres), acres\")"
  },
  {
    "objectID": "slides/w06a-interactions.html#marginal-effect-acres-and-development-at-means",
    "href": "slides/w06a-interactions.html#marginal-effect-acres-and-development-at-means",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Marginal Effect acres and development at means",
    "text": "Marginal Effect acres and development at means\n\n\nShow the code\nlibrary(marginaleffects)\nmodelsummary(fit2)\nmodelsummary(marginaleffects(fit2))\n\n\n\n\nShow the code\nsumm(\n  list(`Coef. as is` = fit2,\n       `M.E. at means` = marginaleffects(fit2)),\n  output = \"html\",\n  coef_map = c(\n    \"acres\" = \"acres\",\n    \"crpPct\" = \"crpPct\",\n    \"development\" = \"development1\",\n    \"development1\" = \"development1\",\n    \"acres:development1\" = \"acres * development1\"\n  )\n)\n\n\n\n\n \n  \n      \n    Coef. as is \n    M.E. at means \n  \n \n\n  \n    acres \n    −0.652*** (0.085) \n    −2.017*** (0.097) \n  \n  \n    crpPct \n    −9.073*** (0.635) \n    −9.073*** (0.635) \n  \n  \n    development1 \n    2978.491*** (71.424) \n    944.727*** (47.947) \n  \n  \n    acres * development1 \n    −18.053*** (0.743) \n     \n  \n  \n    Num.Obs. \n    18650 \n    18650 \n  \n  \n    R2 \n    0.431 \n    0.431 \n  \n  \n    R2 Adj. \n    0.430 \n    0.430"
  },
  {
    "objectID": "slides/w06a-interactions.html#marginal-effect-acres-and-development-visually",
    "href": "slides/w06a-interactions.html#marginal-effect-acres-and-development-visually",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Marginal Effect acres and development (visually)",
    "text": "Marginal Effect acres and development (visually)\n\n\n\n\n\nShow the code\nplot_slopes(fit2, variables = \"acres\", condition = \"development\") + \n  ylab(\"Marginal effect of acres (slope)\") + xlab(\"Development, 0 = no 1 = yes\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nplot_slopes(fit2, variables = \"development\", condition = \"acres\") + \n  ylab(\"Marginal effect of development dummy (slope)\") + xlab(\"Parcel size, acres\") + \n  scale_y_continuous(n.breaks = 10)"
  },
  {
    "objectID": "slides/w06a-interactions.html#example-1.-interaction-with-a-continuous-variable",
    "href": "slides/w06a-interactions.html#example-1.-interaction-with-a-continuous-variable",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Example 1. Interaction with a continuous variable",
    "text": "Example 1. Interaction with a continuous variable\n\nfit3 <- \n  lm(acrePrice  ~ acres * improvements + crpPct + region + factor(year),\n     data = dta)\ncust_summ(list(fit1, fit2, fit3), output = \"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel 1\nModel 2\nModel 3\n\n\n\n\n(Intercept)\n2019.877*** (65.416)\n1997.055*** (64.413)\n1923.131*** (64.085)\n\n\nacres\n-0.857*** (0.086)\n-0.652*** (0.085)\n-0.276** (0.089)\n\n\ndevelopment1\n1555.287*** (41.467)\n2978.491*** (71.424)\n\n\n\ncrpPct\n-8.892*** (0.645)\n-9.073*** (0.635)\n-8.980*** (0.631)\n\n\nacres × development1\n\n-18.053*** (0.743)\n\n\n\nimprovements\n\n\n56.695*** (1.307)\n\n\nacres × improvements\n\n\n-0.277*** (0.014)\n\n\nNum.Obs.\n18650\n18650\n18650\n\n\nR2\n0.413\n0.431\n0.438\n\n\nR2 Adj.\n0.412\n0.430\n0.438\n\n\n\nNote: ^^ Model 1: acrePrice ~ acres + development + crpPct + region + factor(year) Model 2: acrePrice ~ acres * development + crpPct + region + factor(year) Model 3: acrePrice ~ acres * improvements + crpPct + region + factor(year)"
  },
  {
    "objectID": "slides/w06a-interactions.html#fitted-vs-area-improvements",
    "href": "slides/w06a-interactions.html#fitted-vs-area-improvements",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Fitted vs area * improvements",
    "text": "Fitted vs area * improvements\n\n\n\n\n\nShow the code\nggpredict(fit3, terms = c(\"acres [0, 10, 50, 100, 200, 250, 500, 1000]\", \"improvements [0, 10, 50, 100]\")) %>%\n  plot() + \n  ylab(\"Predicted price per acre (acrePrice), USD\") + \n  xlab(\"Parcel size (acres), acres\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nggpredict(fit3, terms = c(\"improvements\", \"acres [1, 50, 100, 200, 500]\")) %>%\n  plot() + \n  ylab(\"Predicted price per acre (acrePrice), USD\") + \n  xlab(\"Improvements, % of price due to infrastructure\")"
  },
  {
    "objectID": "slides/w06a-interactions.html#marginal-effect-acres-improvements-visually",
    "href": "slides/w06a-interactions.html#marginal-effect-acres-improvements-visually",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Marginal Effect acres * improvements (visually)",
    "text": "Marginal Effect acres * improvements (visually)\n\n\n\n\n\nShow the code\nplot_slopes(fit3, variables = \"acres\", condition = \"improvements\") + \n  ylab(\"Marginal effect of acres (slope)\")  + \n  xlab(\"Improvements, % of price due to infrastructure\")  + \n  scale_y_continuous(n.breaks = 10) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nplot_slopes(fit3, variables = \"improvements\", condition = \"acres\") + \n  ylab(\"Marginal effect of improvements (slope)\") + xlab(\"Parcel size, acres\") + \n  scale_y_continuous(n.breaks = 10)"
  },
  {
    "objectID": "slides/w06a-interactions.html#reporting-all-regressions-results",
    "href": "slides/w06a-interactions.html#reporting-all-regressions-results",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Reporting all regressions results",
    "text": "Reporting all regressions results\n\n\nShow the code\nmodelsummary(\n  list(\n    `Model 1 (no interaction)` = fit1,\n    `Model 2 (Area*Development)` = marginaleffects(fit2, vcov = ~ region),\n    `Model 3 (Area*Improvements)` = slopes(fit3, vcov = ~ region)\n    ),\n  vcov = list(~ region, NULL, NULL),\n  estimate = \"{estimate}{stars} ({std.error})\",\n  statistic = NULL,\n  gof_omit =  c(\"AIC|BIC|Log|F|RMS|Std\"),\n  output = \"html\",\n  coef_map = c(\n    \"acres\" = \"Area, acres (*)\",\n    \"crpPct\" = \"crpPct\",\n    \"development\" = \"Development status, dummy (*)\",\n    \"improvements\" = \"Share infrastructure in land price, % (*)\"\n  ), \n  notes = \"(*) marginal effects of the coefficients are reported at means of the corresponding interaction terms. Robust standard errors clustered at region are reported in brackets. For marginal effects, standard errors are estimated using delta method.\"\n)\n\n\n\n\n \n  \n      \n    Model 1 (no interaction) \n     Model 2 (Area*Development) \n     Model 3 (Area*Improvements) \n  \n \n\n  \n    Area, acres (*) \n    −0.861* (0.350) \n    −2.017*** (0.097) \n    −1.520*** (0.089) \n  \n  \n    crpPct \n    −8.890*** (1.991) \n    −9.073*** (0.635) \n    −8.980*** (0.631) \n  \n  \n    Development status, dummy (*) \n     \n    944.727*** (47.947) \n     \n  \n  \n    Share infrastructure in land price, % (*) \n     \n     \n    25.490*** (0.984) \n  \n  \n    Num.Obs. \n    18700 \n    18650 \n    18650 \n  \n  \n    R2 \n    0.413 \n    0.431 \n    0.438 \n  \n  \n    R2 Adj. \n    0.413 \n    0.430 \n    0.438 \n  \n\n\n (*) marginal effects of the coefficients are reported at means of the corresponding interaction terms. Robust standard errors clustered at region are reported in brackets. For marginal effects, standard errors are estimated using delta method."
  },
  {
    "objectID": "slides/w06a-interactions.html#takeaway-and-homework-1",
    "href": "slides/w06a-interactions.html#takeaway-and-homework-1",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Takeaway and homework",
    "text": "Takeaway and homework\n\nMarginal effects.\nDifference between fitted values and marginal effects.\nHomework:\n\nReproduce code from the slides.\nFollow pre-recorded materials with extra calculations."
  },
  {
    "objectID": "slides/w06a-interactions.html#references-1",
    "href": "slides/w06a-interactions.html#references-1",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "References",
    "text": "References\n\n\n\nMP223-Applied Econometrics (SoSe 2023). Author: Eduard Bukin (2023). | GitHub | StudIP | ILIAS.\n\n\n\nAngrist, J. D., & Pischke, J.-S. (2009). Mostly harmless econometrics. Princeton University Press. http://doi.org/10.1515/9781400829828"
  },
  {
    "objectID": "slides/w06a-interactions.html#example-3.-interaction-with-a-continuous-variable",
    "href": "slides/w06a-interactions.html#example-3.-interaction-with-a-continuous-variable",
    "title": "Interaction terms in MLR and Difference in Difference (DID)",
    "section": "Example 3. Interaction with a continuous variable",
    "text": "Example 3. Interaction with a continuous variable\n\n\nShow the code\nfit3 <- \n  lm(acrePrice  ~ acres * improvements + crpPct + region + factor(year),\n     data = dta)\ncust_summ(list(fit1, fit2, fit3), output = \"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\nModel 1\nModel 2\nModel 3\n\n\n\n\n(Intercept)\n2021.487*** (65.458)\n1997.055*** (64.413)\n1923.131*** (64.085)\n\n\nacres\n-0.861*** (0.086)\n-0.652*** (0.085)\n-0.276** (0.089)\n\n\ndevelopment1\n1555.095*** (41.498)\n2978.491*** (71.424)\n\n\n\ndevelopment(Missing)\n183.250 (210.799)\n\n\n\n\ncrpPct\n-8.890*** (0.643)\n-9.073*** (0.635)\n-8.980*** (0.631)\n\n\nacres × development1\n\n-18.053*** (0.743)\n\n\n\nimprovements\n\n\n56.695*** (1.307)\n\n\nacres × improvements\n\n\n-0.277*** (0.014)\n\n\n:———————\n———————:\n———————:\n———————:\n\n\nNum.Obs.\n18700\n18650\n18650\n\n\nR2\n0.413\n0.431\n0.438\n\n\nR2 Adj.\n0.413\n0.430\n0.438\n\n\n\nNote: ^^ Model 1: acrePrice ~ acres + development + crpPct + region + factor(year)Model 2: acrePrice ~ acres * development + crpPct + region + factor(year)Model 3: acrePrice ~ acres * improvements + crpPct + region + factor(year)"
  },
  {
    "objectID": "slides/w07-OVB.html#long-model",
    "href": "slides/w07-OVB.html#long-model",
    "title": "Omitted Variable Bias",
    "section": "Long Model",
    "text": "Long Model\n\nA regression model that we wish to have.\n\n\n\\[Y_i = \\alpha ^ l + \\beta ^ l P_i + \\gamma A_i + e^l_i\\]\nwhere:\n\n\n\\(Y_i\\) is the outcome variable;\n\\(P_i\\) is the key variable of interest;\n\\(A_i\\) is the omitted variable;\n\\(\\alpha ^ l\\) , \\(\\beta ^ l\\) are true regression coefficients;\n\\(\\gamma\\) is the effect of omitted variable in long;\n\\(e^l_i\\) true error terms."
  },
  {
    "objectID": "slides/w07-OVB.html#short-model",
    "href": "slides/w07-OVB.html#short-model",
    "title": "Omitted Variable Bias",
    "section": "Short model",
    "text": "Short model\n\nIs a model that we actually have, which omits one important variable (\\(A_i\\)) from the long model.\n\n\n\\[Y_i = \\alpha ^ s + \\beta^s P_i + e^s_i\\]\nwhere:\n\n\n\\(Y_i\\) is the outcome variable;\n\\(P_i\\) is the key variable of interest;\n\\(\\alpha ^ s\\) , \\(\\beta ^ s\\) are the estimates of regression coefficients in the short model;\n\\(e^s_i\\) error terms."
  },
  {
    "objectID": "slides/w07-OVB.html#omitted-variable-bias-1",
    "href": "slides/w07-OVB.html#omitted-variable-bias-1",
    "title": "Omitted Variable Bias",
    "section": "Omitted Variable Bias (1)",
    "text": "Omitted Variable Bias (1)\nOmitting variable \\(A_i\\) in the short model causes bias of \\(\\beta^s\\).\n\n\\[\n\\beta^s = \\beta^l + \\text{OVB}\n\\]\n\n\nWe can measure Omitted Variable Bias (\\(\\text{OVB}\\)) as:\n\\[\n\\text{OVB} =  \\beta^s - \\beta^l\n\\]"
  },
  {
    "objectID": "slides/w07-OVB.html#omitted-variable-bias-happens-when",
    "href": "slides/w07-OVB.html#omitted-variable-bias-happens-when",
    "title": "Omitted Variable Bias",
    "section": "Omitted Variable Bias happens when:",
    "text": "Omitted Variable Bias happens when:\n\n\n\\(P_i\\) and \\(A_i\\) relates to each other:\n\n\\(E[A_i|P_i] \\neq 0\\) ;\n\n\\(A_i\\) and \\(Y_i\\) relates to each other:\n\n\\(E[Y_i| A_i] \\neq 0\\) in the long regression or \\(\\gamma \\neq 0\\);"
  },
  {
    "objectID": "slides/w07-OVB.html#auxiliary-regression",
    "href": "slides/w07-OVB.html#auxiliary-regression",
    "title": "Omitted Variable Bias",
    "section": "Auxiliary regression",
    "text": "Auxiliary regression\n\nIs a regression of omitted variable (\\(A_i\\)) on treatment \\(P_i\\) and other regressors in short (if any).\n\n\n\\[\nA_i = \\pi_0 + \\pi_1 P_i + u_i\n\\]\n\n\n\nAuxiliary regression helps us to calculate the \\(\\text{OVB}\\)."
  },
  {
    "objectID": "slides/w07-OVB.html#omitted-variable-bias-2",
    "href": "slides/w07-OVB.html#omitted-variable-bias-2",
    "title": "Omitted Variable Bias",
    "section": "Omitted Variable Bias (2)",
    "text": "Omitted Variable Bias (2)\nWith:\n\nLong: \\(Y_i = \\alpha ^ l + \\beta ^ l P_i + \\gamma A_i + e^l_i\\);\nShort: \\(Y_i = \\alpha ^ s + \\beta^s P_i + e^s_i\\);\nAuxiliary: \\(A_i = \\pi_0 + \\pi_1 P_i + u_i\\);\n\n\nWe can measure Omitted Variable Bias as:\n\\[\\text{OVB} = \\beta^s - \\beta^l\\]\n\n\n\\[\\text{OVB} = \\pi_1 \\times \\gamma\\]"
  },
  {
    "objectID": "slides/w07-OVB.html#math-behind-the-ovb",
    "href": "slides/w07-OVB.html#math-behind-the-ovb",
    "title": "Omitted Variable Bias",
    "section": "Math behind the OVB",
    "text": "Math behind the OVB\n\nLong: \\(Y_i = \\alpha ^ l + \\beta ^ l P_i + \\gamma A_i + e^l_i\\);\nShort: \\(Y_i = \\alpha ^ s + \\beta^s P_i + e^s_i\\);\nAuxiliary: \\(A_i = \\pi_0 + \\pi_1 P_i + u_i\\);\n\n\n\nLet us substitute \\(A_i\\) in the long with Auxiliary regression:\n\n\n\n\\[\n\\Rightarrow Y_i = \\alpha ^ l + \\beta ^ l P_i + \\gamma \\{\\pi_0 + \\pi_1 P_i + u \\} + e^l_i\n\\]\n\n\n\\[\n\\Rightarrow Y_i = \\underbrace{\\alpha ^ l +  \\gamma \\pi_0}_{\\alpha ^ s}\n+ \\underbrace{(\\beta ^ l  + \\gamma \\pi_1)}_{\\beta^s} P_i\n+ \\underbrace{e^l_i + \\gamma u_i}_{e^s_i}\n\\]\n\nWe obtain our short regression, where every estimate is biased."
  },
  {
    "objectID": "slides/w07-OVB.html#how-to-resolve-the-ovb",
    "href": "slides/w07-OVB.html#how-to-resolve-the-ovb",
    "title": "Omitted Variable Bias",
    "section": "How to resolve the OVB?",
    "text": "How to resolve the OVB?\n\n\nNo solution!\n\nProxies;\nResearch design (Panel Regression/DiD, RDD);\n\nAcknowledge presence of the OVB;\nDiscuss the bias;"
  },
  {
    "objectID": "slides/w07-OVB.html#mincer-equation",
    "href": "slides/w07-OVB.html#mincer-equation",
    "title": "Omitted Variable Bias",
    "section": "Mincer equation",
    "text": "Mincer equation\nIn 1970, Jacob Mincer in his work Schooling, Experience, and Earnings (Mincer, 1974) attempted to quantify the premium of schooling on wage. He used the following regression equation:\n\\[\n\\log \\text{wage}_i = \\beta_0 + \\beta_1 \\text{educ}_i + \\beta_2 \\text{exper}_i+ \\epsilon_i\n\\]\n\nProve that omitting experience causes OBV!"
  },
  {
    "objectID": "slides/w07-OVB.html#step-1.-write-long-short-and-auxiliary-regressions",
    "href": "slides/w07-OVB.html#step-1.-write-long-short-and-auxiliary-regressions",
    "title": "Omitted Variable Bias",
    "section": "Step 1. Write long, short and auxiliary regressions:",
    "text": "Step 1. Write long, short and auxiliary regressions:\n\nLong: \\(\\log \\text{wage}_i = \\beta_0 + \\beta_1 \\text{educ}_i + \\beta_2 \\text{exper}_i+ \\epsilon_i\\)\n\n\nShort: \\(\\log \\text{wage}_i = \\beta_0^s + \\beta_1^s \\text{educ}_i + \\epsilon_i^s\\)\n\n\nAuxiliary: \\(\\text{exper}_i = \\rho_0 + \\rho_1 \\text{educ}_i + u_i\\)"
  },
  {
    "objectID": "slides/w07-OVB.html#step-2.-hypothesize-about-crucial-effects",
    "href": "slides/w07-OVB.html#step-2.-hypothesize-about-crucial-effects",
    "title": "Omitted Variable Bias",
    "section": "Step 2. Hypothesize about crucial effects:",
    "text": "Step 2. Hypothesize about crucial effects:\n\nUse literature and other empirical research to reinforce your claims.\n\n\nEffect of experience on wage (\\(\\beta_2\\))\n\n\n\\[\\beta_2 > 0\\]\n\nMore years of experience, higher wage\n\n\n\n\nEffect of education on experience (\\(\\rho_1\\))\n\n\n\n\\[\\rho_1 < 0\\]\n\nMore time person spend in education, less time is left to work and gain experience."
  },
  {
    "objectID": "slides/w07-OVB.html#step-3.-write-down-an-obv-formula",
    "href": "slides/w07-OVB.html#step-3.-write-down-an-obv-formula",
    "title": "Omitted Variable Bias",
    "section": "Step 3. Write down an OBV formula",
    "text": "Step 3. Write down an OBV formula\n\\[\\text{OVB} = \\beta_2 \\times \\rho_1\\]\n\nGiven our previous hypotheses:\n\n\\(\\beta_2 > 0 = +\\)\n\\(\\rho_1 < 0 = -\\)\n\n\n\n\\[\\text{OVB} = (+) \\times (-) < 0\\]\n\n\n\nOmitting experience in short regression might cause a downward bias on the estimated effect of education. As a result, we may:\n\nunderestimate the effect of education.\nfind the effect of education insignificant or negative."
  },
  {
    "objectID": "slides/w07-OVB.html#wage-and-education",
    "href": "slides/w07-OVB.html#wage-and-education",
    "title": "Omitted Variable Bias",
    "section": "Wage and Education",
    "text": "Wage and Education\nSupposed that we have estimates equation:\n\\(\\log \\text{wage}_i = \\beta_0 + \\beta_1 \\text{educ}_i + \\beta_2 \\text{exper}_i + \\beta_3 \\text{exper}^2_i+ \\epsilon_i\\)\n\n\nCould there be any other OVB in the wage-education relationship?"
  },
  {
    "objectID": "slides/w07-OVB.html#checking-obv-empirically",
    "href": "slides/w07-OVB.html#checking-obv-empirically",
    "title": "Omitted Variable Bias",
    "section": "Checking OBV empirically",
    "text": "Checking OBV empirically\n\n\n[1] -0.01794277\n\n\n[1] -0.01794277\n\n\n\n\n[1] -0.01794277\n\n\n[1] -0.01794277"
  },
  {
    "objectID": "slides/w07-OVB.html#takeaways",
    "href": "slides/w07-OVB.html#takeaways",
    "title": "Omitted Variable Bias",
    "section": "Takeaways",
    "text": "Takeaways\n\nOVB Formula (Short, Long and Auxiliary regressions)\nBe ready to demonstrate how to use the OVB formula for making an educated guess about the direction of the bias during the exam."
  },
  {
    "objectID": "slides/w07-OVB.html#diy-example-1.",
    "href": "slides/w07-OVB.html#diy-example-1.",
    "title": "Omitted Variable Bias",
    "section": "DIY Example 1.",
    "text": "DIY Example 1.\nYou want to estimate the causal effect of union membership on employees’ wages. And you estimate the following regression equation:\n\\[\n\\log \\text{wage}_i = \\beta_0 + \\beta_1 \\text{union} + \\beta_2 \\text{experience} + \\beta_3 \\text{experience}^2 \\\\\n+ \\beta_4 \\text{married} + \\beta_5 \\text{female} + \\beta_6 \\text{hours per week} + \\epsilon_i\n\\]\nYour colleagues suggest that you should include an individual’s education in the list of control variables as omitting such regressor biases the estimate.\n\nUsing OVB formula prove that omitting education causes/does not causes the OVB.\nCalculate the extent of the OVB."
  },
  {
    "objectID": "slides/w07-OVB.html#omitted-variable-bias-2-the-key",
    "href": "slides/w07-OVB.html#omitted-variable-bias-2-the-key",
    "title": "Omitted Variable Bias",
    "section": "Omitted Variable Bias (2) the key",
    "text": "Omitted Variable Bias (2) the key\nWith:\n\nLong: \\(Y_i = \\alpha ^ l + \\beta ^ l P_i + \\gamma A_i + e^l_i\\);\nShort: \\(Y_i = \\alpha ^ s + \\beta^s P_i + e^s_i\\);\nAuxiliary: \\(A_i = \\pi_0 + \\pi_1 P_i + u_i\\);\n\n\nWe can measure Omitted Variable Bias as:\n\\[\\text{OVB} = \\beta^s - \\beta^l\\]\n\n\n\\[\\text{OVB} = \\pi_1 \\times \\gamma\\]"
  },
  {
    "objectID": "slides/w07-OVB.html#why-ovb-formula-is-important-1",
    "href": "slides/w07-OVB.html#why-ovb-formula-is-important-1",
    "title": "Omitted Variable Bias",
    "section": "Why OVB formula is important (1)",
    "text": "Why OVB formula is important (1)\n\nPresence of OVB in regression renders all our estimates biased/useless.\n\n\n\nOmitted Variable - means that we cannot have it in the regression, we can’t use data.\nHaving knowledge of mathematics behind OVB, we can make an educated guess about consequences of the variable omission: the BIAS (Angrist & Pischke, 2014)"
  },
  {
    "objectID": "slides/w07-OVB.html#why-ovb-2",
    "href": "slides/w07-OVB.html#why-ovb-2",
    "title": "Omitted Variable Bias",
    "section": "Why OVB (2)",
    "text": "Why OVB (2)\n\n\nwrite down Short, Long and Auxiliary regressions\nJustify potential signs of \\(\\pi_1\\) and \\(\\gamma\\);\nConclude how the OV biases our regression based on the formula: \\(\\text{OVB} = \\pi_1 \\times \\gamma\\).\nOBV can bias estimates:\n\nupwards (\\(\\text{OVB} > 0\\)): increasing the effect of \\(P_i\\)\ndownwards (\\(\\text{OVB} < 0\\)): decreeing the effect of \\(P_i\\)\nrendering the effect of \\(P_i\\) insignificant"
  },
  {
    "objectID": "slides/w07-OVB.html#how-to-tcheck-the-ovb-2",
    "href": "slides/w07-OVB.html#how-to-tcheck-the-ovb-2",
    "title": "Omitted Variable Bias",
    "section": "How to tcheck the OVB (2)",
    "text": "How to tcheck the OVB (2)\n\n\nWrite down Short, Long and Auxiliary regressions\nJustify potential signs of \\(\\pi_1\\) and \\(\\gamma\\);\nConclude how the OV biases our regression based on the formula: \\(\\text{OVB} = \\pi_1 \\times \\gamma\\).\nOBV can bias estimates:\n\nupwards (\\(\\text{OVB} > 0\\)): increasing the effect of \\(P_i\\)\ndownwards (\\(\\text{OVB} < 0\\)): decreeing the effect of \\(P_i\\)\nrendering the effect of \\(P_i\\) insignificant"
  },
  {
    "objectID": "slides/w07-OVB.html#checking-obv-based-on-the-data",
    "href": "slides/w07-OVB.html#checking-obv-based-on-the-data",
    "title": "Omitted Variable Bias",
    "section": "Checking OBV based on the data",
    "text": "Checking OBV based on the data\n\n\n\n\nlibrary(tidyverse)\ndta <- read_csv(\"wage.csv\")\n\n\nshort <- lm(log(wage) ~ educ, data = dta)\nshort\n\n\nCall:\nlm(formula = log(wage) ~ educ, data = dta)\n\nCoefficients:\n(Intercept)         educ  \n    5.97306      0.05984  \n\n\n\nlong <- lm(log(wage) ~ educ + exper, data = dta)\nlong\n\n\nCall:\nlm(formula = log(wage) ~ educ + exper, data = dta)\n\nCoefficients:\n(Intercept)         educ        exper  \n    5.50271      0.07778      0.01978  \n\n\n\naux <- lm(exper ~ educ, data = dta)\naux\n\n\nCall:\nlm(formula = exper ~ educ, data = dta)\n\nCoefficients:\n(Intercept)         educ  \n    23.7831      -0.9073"
  },
  {
    "objectID": "slides/w07-OVB.html#estimating-the-bias",
    "href": "slides/w07-OVB.html#estimating-the-bias",
    "title": "Omitted Variable Bias",
    "section": "Estimating the bias",
    "text": "Estimating the bias\n\ncoef(aux)[[\"educ\"]] * coef(long)[[\"exper\"]]\n\n[1] -0.01794277\n\n\n\nChecking the difference between long and short.\n\ncoef(short)[[\"educ\"]] - coef(long)[[\"educ\"]]\n\n[1] -0.01794277"
  },
  {
    "objectID": "slides/w07-OVB.html#how-to-check-the-ovb-2",
    "href": "slides/w07-OVB.html#how-to-check-the-ovb-2",
    "title": "Omitted Variable Bias",
    "section": "How to check the OVB (2)",
    "text": "How to check the OVB (2)\n\n\nWrite down Short, Long and Auxiliary regressions\nJustify potential signs of \\(\\pi_1\\) and \\(\\gamma\\);\nConclude how the OV biases our regression based on the formula: \\(\\text{OVB} = \\pi_1 \\times \\gamma\\).\nOBV can bias estimates:\n\nupwards (\\(\\text{OVB} > 0\\)): increasing the effect of \\(P_i\\)\ndownwards (\\(\\text{OVB} < 0\\)): decreeing the effect of \\(P_i\\)\nrendering the effect of \\(P_i\\) insignificant"
  },
  {
    "objectID": "slides/w07-OVB.html#the-problem",
    "href": "slides/w07-OVB.html#the-problem",
    "title": "Omitted Variable Bias",
    "section": "The problem",
    "text": "The problem\nSupposed that we have estimated the following regression:\n\\[\n\\log \\text{wage}_i = \\beta_0^s + \\beta_1^s \\text{educ}_i + \\beta_2^s \\text{exper}_i + \\beta_3^s \\text{exper}^2_i + \\epsilon_i\n\\]\n\nWhat the other variables that are omitted and that may cause the bias to our estimates?\n\nNote that OV should correlate with \\(\\log \\text{wage}_i\\) and \\(\\text{educ}_i\\);\n\n\n\nThese are other human capital related variables: age, ability, motivation."
  },
  {
    "objectID": "slides/w07-OVB.html#show-that-omitting-ability-causes-the-ovb",
    "href": "slides/w07-OVB.html#show-that-omitting-ability-causes-the-ovb",
    "title": "Omitted Variable Bias",
    "section": "Show that omitting ability causes the OVB",
    "text": "Show that omitting ability causes the OVB\n\n\nWrite short, long and auxiliary regression\n\nshort:\nlong:\nauxiliary:\n\nWrite the OVB formula:\n\n\\(\\text{OVB} = \\cdot\\)\n\nMake Hypothesis about the effect of included on omitted and omitted on dependent:\n\nArgument your statements.\n\nConclude about the bias:\n\n\\(\\text{OVB} = \\cdot\\)"
  },
  {
    "objectID": "slides/w07-OVB.html#solution",
    "href": "slides/w07-OVB.html#solution",
    "title": "Omitted Variable Bias",
    "section": "Solution",
    "text": "Solution\n\n\nWrite short, long and auxiliary regression\n\nshort: \\(\\log \\text{wage}_i = \\beta_0^s + \\beta_1^s \\text{educ}_i + \\beta_2^s \\text{exper}_i + \\beta_3^s \\text{exper}^2_i + \\epsilon_i\\)\nlong: \\(\\log \\text{wage}_i = \\beta_0 + \\beta_1 \\text{educ}_i + \\beta_2 \\text{exper}_i + \\beta_3 \\text{exper}^2_i + \\gamma \\text{ability}_i + \\epsilon_i\\)\nauxiliary: \\(\\text{ability}_i = \\rho_0 + \\rho_1 \\text{educ}_i + \\rho_2 \\text{exper}_i + \\rho_3 \\text{exper}^2_i + u_i\\)\n\nWrite the OVB formula:\n\n\\(\\text{OVB} = \\beta_1^s - \\beta_1 = \\rho_1 \\times \\gamma\\)\n\nMake Hypothesis about \\(\\gamma\\) and \\(\\rho_1\\).\n\n\\(\\rho_1 > 0\\) as more years of education are usually associated with higher abilities;\n\\(\\gamma\\) Higher abilities are usually rewarded with higher salary.\n\nConclude about the bias;\n\n\\(\\text{OVB} = (+) \\times (+) > 0\\)"
  },
  {
    "objectID": "slides/w07-OVB.html#check-this-conclusion-empirically",
    "href": "slides/w07-OVB.html#check-this-conclusion-empirically",
    "title": "Omitted Variable Bias",
    "section": "Check this conclusion empirically",
    "text": "Check this conclusion empirically\n\nEstimate the short model\nEstimate the long model where instead of abilities the IQ level is used as a proxy.\nCalculate the extent of the OVB."
  },
  {
    "objectID": "slides/w07-OVB.html#conclude-about-the-bias",
    "href": "slides/w07-OVB.html#conclude-about-the-bias",
    "title": "Omitted Variable Bias",
    "section": "4. Conclude about the bias;",
    "text": "4. Conclude about the bias;\n\n\nWe might have an upwards bias of the estimates in our regression.\nSpecifically the effect of education is overestimated.\nIn the long model we might observe a lower effect of education on wage."
  },
  {
    "objectID": "slides/w07-OVB.html#check-this-conclusion-empirically-in-r",
    "href": "slides/w07-OVB.html#check-this-conclusion-empirically-in-r",
    "title": "Omitted Variable Bias",
    "section": "Check this conclusion empirically in R",
    "text": "Check this conclusion empirically in R\n\nEstimate the short model\nEstimate the long model where instead of abilities the IQ level is used as a proxy.\nCalculate the extent of the OVB"
  },
  {
    "objectID": "slides/w07-OVB.html#watch-several-videos-about",
    "href": "slides/w07-OVB.html#watch-several-videos-about",
    "title": "Omitted Variable Bias",
    "section": "Watch several videos about",
    "text": "Watch several videos about\nDale, S. B., & Krueger, A. B. (2002). Estimating the payoff to attending a more selective college: An application of selection on observables and unobservables. The Quarterly Journal of Economics, 117(4), 1491-1527.\n\nVideo 1. Selection Bias: Will You Make More Going to a Private University? From minute 6:30 to the end. https://youtu.be/6YrIDhaUQOE\nVideo 2. From 47:22 2017 AEA Cross-Section Econometrics. Part 2\n\nNote that other videos and handouts are available here 2017 AEA Cross-Section Econometrics"
  },
  {
    "objectID": "slides/w07-OVB.html#homework-1.",
    "href": "slides/w07-OVB.html#homework-1.",
    "title": "Omitted Variable Bias",
    "section": "Homework 1.",
    "text": "Homework 1.\nYou want to estimate the causal effect of union membership on employees’ wages. And you estimate the following regression equation:\n\\[\n\\log \\text{wage}_i = \\beta_0 + \\beta_1 \\text{union} + \\beta_2 \\text{experience} + \\beta_3 \\text{experience}^2 \\\\\n+ \\beta_4 \\text{married} + \\beta_5 \\text{female} + \\beta_6 \\text{hours per week} + \\epsilon_i\n\\]\nYour colleagues suggest that you should include an individual’s education in the list of control variables as omitting such regressor biases the estimate.\n\nUsing OVB formula prove that omitting education causes/does not causes the OVB.\nCalculate the extent of the OVB"
  },
  {
    "objectID": "slides/w07-OVB.html#do-the-ovb-analysis-on-your-own",
    "href": "slides/w07-OVB.html#do-the-ovb-analysis-on-your-own",
    "title": "Omitted Variable Bias",
    "section": "Do the OVB analysis on your own",
    "text": "Do the OVB analysis on your own\nYou want to estimate the causal effect of union membership on employees’ wages. And you estimate the following regression equation:\n\\[\n\\log \\text{wage}_i = \\beta_0 + \\beta_1 \\text{union} + \\beta_2 \\text{experience} + \\beta_3 \\text{experience}^2 \\\\\n+ \\beta_4 \\text{married} + \\beta_5 \\text{sex} + \\beta_6 \\text{hours per week} + \\epsilon_i\n\\]\nYour colleagues suggest that you should include an individual’s education in the list of control variables as omitting such regressor biases the estimate.\n\nUsing OVB formula prove that omitting education causes/does not causes the OVB.\nCalculate the extent of the OVB"
  },
  {
    "objectID": "slides/w08-panel-regression.html#cross-sectional-data",
    "href": "slides/w08-panel-regression.html#cross-sectional-data",
    "title": "Panel Regression Analysis",
    "section": "Cross-sectional data",
    "text": "Cross-sectional data\n\n\n\n\n\n\nID\nY\nX1\nX2\n\n\n\n\n\\(1\\)\n\\(y_{1}\\)\n\\(x^{1}_{1}\\)\n\\(x^{2}_{1}\\)\n\n\n\\(2\\)\n\\(y_{2}\\)\n\\(x^{1}_{2}\\)\n\\(x^{2}_{2}\\)\n\n\n\\(3\\)\n\\(y_{3}\\)\n\\(x^{1}_{3}\\)\n\\(x^{2}_{3}\\)\n\n\n\\(4\\)\n\\(y_{4}\\)\n\\(x^{1}_{4}\\)\n\\(x^{2}_{4}\\)\n\n\n\\(5\\)\n\\(y_{5}\\)\n\\(x^{1}_{5}\\)\n\\(x^{2}_{5}\\)\n\n\n\\(6\\)\n\\(y_{6}\\)\n\\(x^{1}_{6}\\)\n\\(x^{2}_{6}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(N\\)\n\\(y_{N}\\)\n\\(x^{1}_{N}\\)\n\\(x^{2}_{N}\\)\n\n\n\n\n\n\nData that we usually collect in a single data collection.\n\nEach individual is represented by one observation.\n\nCould be repeatedly collected multiple times (repeated cross-section),\n\nbut, in every repetition, there are different individuals!"
  },
  {
    "objectID": "slides/w08-panel-regression.html#panel-data",
    "href": "slides/w08-panel-regression.html#panel-data",
    "title": "Panel Regression Analysis",
    "section": "Panel data",
    "text": "Panel data\n\n\n\ntable with data, where\neach individual (cohort, e.i. region, country)\nis represented by multiple observations at different time periods.\n\n\n\n\n\n\nID\nTime\nY\nX1\nX2\n\n\n\n\n\\(1\\)\n\\(1\\)\n\\(y_{11}\\)\n\\(x^{1}_{11}\\)\n\\(x^{2}_{11}\\)\n\n\n\\(1\\)\n\\(2\\)\n\\(y_{12}\\)\n\\(x^{1}_{12}\\)\n\\(x^{2}_{12}\\)\n\n\n\\(1\\)\n\\(3\\)\n\\(y_{13}\\)\n\\(x^{1}_{13}\\)\n\\(x^{2}_{13}\\)\n\n\n\\(2\\)\n\\(2\\)\n\\(y_{22}\\)\n\\(x^{1}_{22}\\)\n\\(x^{2}_{22}\\)\n\n\n\\(2\\)\n\\(3\\)\n\\(y_{23}\\)\n\\(x^{1}_{23}\\)\n\\(x^{2}_{23}\\)\n\n\n\\(3\\)\n\\(1\\)\n\\(y_{31}\\)\n\\(x^{1}_{31}\\)\n\\(x^{2}_{31}\\)\n\n\n\\(3\\)\n\\(2\\)\n\\(y_{32}\\)\n\\(x^{1}_{32}\\)\n\\(x^{2}_{32}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(N\\)\n\\(1\\)\n\\(y_{N1}\\)\n\\(x^{1}_{N1}\\)\n\\(x^{1}_{N1}\\)\n\n\n\\(N\\)\n\\(2\\)\n\\(y_{N2}\\)\n\\(x^{1}_{N2}\\)\n\\(x^{2}_{N2}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(N\\)\n\\(T\\)\n\\(y_{NT}\\)\n\\(x^{1}_{NT}\\)\n\\(x^{2}_{NT}\\)"
  },
  {
    "objectID": "slides/w08-panel-regression.html#panel-data-1",
    "href": "slides/w08-panel-regression.html#panel-data-1",
    "title": "Panel Regression Analysis",
    "section": "Panel data",
    "text": "Panel data\n\n\n\nID\nTime\nY\nX1\nX2\n\n\n\n\n\\(1\\)\n\\(1\\)\n\\(y_{11}\\)\n\\(x^{1}_{11}\\)\n\\(x^{2}_{11}\\)\n\n\n\\(1\\)\n\\(2\\)\n\\(y_{12}\\)\n\\(x^{1}_{12}\\)\n\\(x^{2}_{12}\\)\n\n\n\\(1\\)\n\\(3\\)\n\\(y_{13}\\)\n\\(x^{1}_{13}\\)\n\\(x^{2}_{13}\\)\n\n\n\\(2\\)\n\\(2\\)\n\\(y_{22}\\)\n\\(x^{1}_{22}\\)\n\\(x^{2}_{22}\\)\n\n\n\\(2\\)\n\\(3\\)\n\\(y_{23}\\)\n\\(x^{1}_{23}\\)\n\\(x^{2}_{23}\\)\n\n\n\\(3\\)\n\\(1\\)\n\\(y_{31}\\)\n\\(x^{1}_{31}\\)\n\\(x^{2}_{31}\\)\n\n\n\\(3\\)\n\\(2\\)\n\\(y_{32}\\)\n\\(x^{1}_{32}\\)\n\\(x^{2}_{32}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(N\\)\n\\(1\\)\n\\(y_{N1}\\)\n\\(x^{1}_{N1}\\)\n\\(x^{1}_{N1}\\)\n\n\n\\(N\\)\n\\(2\\)\n\\(y_{N2}\\)\n\\(x^{1}_{N2}\\)\n\\(x^{2}_{N2}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(N\\)\n\\(T\\)\n\\(y_{NT}\\)\n\\(x^{1}_{NT}\\)\n\\(x^{2}_{NT}\\)"
  },
  {
    "objectID": "slides/w08-panel-regression.html#panel-data-balanced-and-unbalanced",
    "href": "slides/w08-panel-regression.html#panel-data-balanced-and-unbalanced",
    "title": "Panel Regression Analysis",
    "section": "Panel data: Balanced and Unbalanced",
    "text": "Panel data: Balanced and Unbalanced\n\n\nBalanced\n\n\n\n\n\\(\\text{ID}\\)\n\\(\\text{Time}\\)\n\\(Y\\)\n\\(X\\)\n\n\n\n\n1\n1\n\\(Y_{11}\\)\n\\(X_{11}\\)\n\n\n1\n2\n\\(Y_{12}\\)\n\\(X_{12}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n1\n\\(T\\)\n\\(Y_{1T}\\)\n\\(X_{1T}\\)\n\n\n2\n1\n\\(Y_{21}\\)\n\\(X_{21}\\)\n\n\n2\n2\n\\(Y_{22}\\)\n\\(X_{22}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n2\n\\(T\\)\n\\(Y_{2T}\\)\n\\(X_{2T}\\)\n\n\n3\n1\n\\(Y_{31}\\)\n\\(X_{31}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(N\\)\n\\(T\\)\n\\(Y_{NT}\\)\n\\(X_{NT}\\)\n\n\n\n\nEach individual is represented in all time periods.\n\n\n\nUn balanced\n\n\n\n\n\\(\\text{ID}\\)\n\\(\\text{Time}\\)\n\\(Y\\)\n\\(X\\)\n\n\n\n\n1\n1\n\\(Y_{11}\\)\n\\(X_{11}\\)\n\n\n1\n2\n\\(Y_{12}\\)\n\\(X_{12}\\)\n\n\n2\n2\n\\(Y_{22}\\)\n\\(X_{22}\\)\n\n\n2\n3\n\\(Y_{23}\\)\n\\(X_{23}\\)\n\n\n3\n3\n\\(Y_{33}\\)\n\\(X_{33}\\)\n\n\n4\n1\n\\(Y_{41}\\)\n\\(X_{41}\\)\n\n\n5\n2\n\\(Y_{52}\\)\n\\(X_{52}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(N\\)\n\\(T\\)\n\\(Y_{NT}\\)\n\\(X_{NT}\\)\n\n\n\n\nEach individual only appears in some time periods (not all)."
  },
  {
    "objectID": "slides/w08-panel-regression.html#let-us-investigate",
    "href": "slides/w08-panel-regression.html#let-us-investigate",
    "title": "Panel Regression Analysis",
    "section": "Let us investigate…",
    "text": "Let us investigate…\nThe relationship between bill length and depth in penguins…"
  },
  {
    "objectID": "slides/w08-panel-regression.html#the-data",
    "href": "slides/w08-panel-regression.html#the-data",
    "title": "Panel Regression Analysis",
    "section": "The data",
    "text": "The data\n\nlibrary(tidyverse)\nlibrary(modelsummary)\npenguins <- read_csv(\"penguins\")\npenguins %>% glimpse()\n\n\n\nRows: 344\nColumns: 8\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    <dbl> 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm <int> 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       <int> 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               <fct> male, female, female, NA, female, male, female, male…\n$ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…"
  },
  {
    "objectID": "slides/w08-panel-regression.html#the-relationship",
    "href": "slides/w08-panel-regression.html#the-relationship",
    "title": "Panel Regression Analysis",
    "section": "The relationship",
    "text": "The relationship\n\ngg_bill <- \n  penguins %>% ggplot() + \n  aes(x = bill_length_mm, y = bill_depth_mm) +\n  xlab(\"Bill length, mm\") + ylab(\"Bill depth, mm\") +\n  geom_point()\ngg_bill"
  },
  {
    "objectID": "slides/w08-panel-regression.html#simpsons-paradox-with-penguins",
    "href": "slides/w08-panel-regression.html#simpsons-paradox-with-penguins",
    "title": "Panel Regression Analysis",
    "section": "Simpson’s paradox (with penguins)",
    "text": "Simpson’s paradox (with penguins)"
  },
  {
    "objectID": "slides/w08-panel-regression.html#the-trend",
    "href": "slides/w08-panel-regression.html#the-trend",
    "title": "Panel Regression Analysis",
    "section": "The trend",
    "text": "The trend\n\ngg_bill + \n  geom_smooth(method = \"lm\", formula = y ~ x, colour = \"black\")"
  },
  {
    "objectID": "slides/w08-panel-regression.html#is-this-the-true-trend",
    "href": "slides/w08-panel-regression.html#is-this-the-true-trend",
    "title": "Panel Regression Analysis",
    "section": "Is this the true trend?",
    "text": "Is this the true trend?\n\ngg_bill + \n  geom_smooth(method = \"lm\", formula = y ~ x, colour = \"black\") + \n  aes(colour = species)"
  },
  {
    "objectID": "slides/w08-panel-regression.html#the-true-trends",
    "href": "slides/w08-panel-regression.html#the-true-trends",
    "title": "Panel Regression Analysis",
    "section": "The true trends",
    "text": "The true trends\n\ngg_bill + \n  geom_smooth(method = \"lm\", formula = y ~ x, colour = \"black\") + \n  aes(colour = species) + \n  geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE)"
  },
  {
    "objectID": "slides/w08-panel-regression.html#regression-results",
    "href": "slides/w08-panel-regression.html#regression-results",
    "title": "Panel Regression Analysis",
    "section": "Regression results",
    "text": "Regression results\n\nfit1 <- lm(bill_depth_mm ~ bill_length_mm, data = penguins)\nfit2 <- lm(bill_depth_mm ~ bill_length_mm + species, data = penguins)\nfit3 <- lm(bill_depth_mm ~ -1 + bill_length_mm + species, data = penguins)\nmodelsummary(\n  list(fit1, fit2, fit3),\n  estimate = \"{estimate}{stars} ({std.error})\", \n  statistic = NULL, \n  gof_map = c(\"nobs\", \"adj.r.squared\")\n)\n\n\n\n\n\n \n  \n      \n     (1) \n      (2) \n      (3) \n  \n \n\n  \n    (Intercept) \n    20.885*** (0.844) \n    10.592*** (0.683) \n     \n  \n  \n    bill_length_mm \n    −0.085*** (0.019) \n    0.200*** (0.017) \n    0.200*** (0.017) \n  \n  \n    speciesChinstrap \n     \n    −1.933*** (0.224) \n    8.659*** (0.862) \n  \n  \n    speciesGentoo \n     \n    −5.106*** (0.191) \n    5.486*** (0.835) \n  \n  \n    speciesAdelie \n     \n     \n    10.592*** (0.683) \n  \n  \n    Num.Obs. \n    342 \n    342 \n    342 \n  \n  \n    R2 Adj. \n    0.052 \n    0.767 \n    0.997"
  },
  {
    "objectID": "slides/w08-panel-regression.html#simpsons-paradox-conclusion",
    "href": "slides/w08-panel-regression.html#simpsons-paradox-conclusion",
    "title": "Panel Regression Analysis",
    "section": "Simpson’s paradox conclusion",
    "text": "Simpson’s paradox conclusion\n\nTrends or relationships are observed in the whole population, but they reverse or disappear, when all groups are treated as one.\n\n\nCauses:\n\nUnobserved heterogeneity/differences between groups.\nUnderlining processes that are different between parts of the population.\n\n\n\nResolutions to the paradox:\n\nControl variables in the MRL.\nPanel data."
  },
  {
    "objectID": "slides/w08-panel-regression.html#problem-setting",
    "href": "slides/w08-panel-regression.html#problem-setting",
    "title": "Panel Regression Analysis",
    "section": "Problem setting",
    "text": "Problem setting\nDoes the collective bargaining (union membership) has any effect on wages?\n\nSee: (Card, 1996; Freeman, 1984)\n\n\n\\[\nlog(\\text{Wage}_{it}) = \\beta_0 + \\beta_1 \\cdot \\text{Union}_{it} + \\beta_2 \\cdot {X_{it}} + \\beta_3 \\cdot \\text{Ability}_{i} + \\epsilon_{it}\n\\]\nwhere \\(i\\) is the individual and \\(t\\) is the time dimension;"
  },
  {
    "objectID": "slides/w08-panel-regression.html#is-there-an-endogeneity-problem",
    "href": "slides/w08-panel-regression.html#is-there-an-endogeneity-problem",
    "title": "Panel Regression Analysis",
    "section": "Is there an endogeneity problem?",
    "text": "Is there an endogeneity problem?\n\\[\nlog(\\text{Wage}_{it}) = \\beta_0 + \\beta_1 \\cdot \\text{Union}_{it} + \\beta_2 \\cdot {X_{it}} + \\beta_3 \\cdot \\text{Ability}_{i} + \\epsilon_{it}\n\\]\n\nIs there a source of endogeneity / selection bias here?\n\nAny ideas?\n\n\\(\\text{Ability}_{i}\\) not observable and not measurable;\n\ntime invariant;\ncorrelates with \\(X\\) and \\(Y\\);\n\nOmitting ability causes bias"
  },
  {
    "objectID": "slides/w08-panel-regression.html#one-solution",
    "href": "slides/w08-panel-regression.html#one-solution",
    "title": "Panel Regression Analysis",
    "section": "One solution:",
    "text": "One solution:\n\nWage, union membership, skills, experience - all change over time!\nAbility are time-invariant and unique to each individual;\n\nwe can inctorduce a dummy variables for each individual,\nto approximate ability!\n\nBut this is only possible with panel data\n\nWhen each individual is observed multiple times!\n\nAlso called Fixed Effect - Panel Regression model of (Difference in Difference)"
  },
  {
    "objectID": "slides/w08-panel-regression.html#other-solutions",
    "href": "slides/w08-panel-regression.html#other-solutions",
    "title": "Panel Regression Analysis",
    "section": "Other solutions:",
    "text": "Other solutions:\n\nAny ideas?\nIntroduce control variables that are proxy of ability.\nEmploy specific research design:\n\nRCT\nRDD"
  },
  {
    "objectID": "slides/w08-panel-regression.html#the-data-1",
    "href": "slides/w08-panel-regression.html#the-data-1",
    "title": "Panel Regression Analysis",
    "section": "The data",
    "text": "The data\n\n\n\n\n \n  \n    id \n    time \n    exp \n    wks \n    bluecol \n    ind \n    south \n    smsa \n    married \n    union \n    ed \n    black \n    lwage \n    female \n    wage \n  \n \n\n  \n    5 \n    1 \n    10 \n    50 \n    yes \n    0 \n    no \n    no \n    yes \n    yes \n    16 \n    no \n    6.43775 \n    1 \n    624.9990 \n  \n  \n    5 \n    2 \n    11 \n    46 \n    yes \n    0 \n    no \n    no \n    yes \n    yes \n    16 \n    no \n    6.62007 \n    1 \n    749.9976 \n  \n  \n    5 \n    3 \n    12 \n    40 \n    yes \n    0 \n    no \n    no \n    yes \n    yes \n    16 \n    no \n    6.63332 \n    1 \n    760.0012 \n  \n  \n    5 \n    4 \n    13 \n    50 \n    no \n    0 \n    no \n    no \n    yes \n    no \n    16 \n    no \n    6.98286 \n    1 \n    1077.9970 \n  \n  \n    5 \n    5 \n    14 \n    47 \n    yes \n    0 \n    no \n    yes \n    yes \n    no \n    16 \n    no \n    7.04752 \n    1 \n    1150.0032 \n  \n  \n    5 \n    6 \n    15 \n    47 \n    no \n    0 \n    no \n    no \n    yes \n    no \n    16 \n    no \n    7.31322 \n    1 \n    1499.9994 \n  \n  \n    5 \n    7 \n    16 \n    49 \n    no \n    0 \n    no \n    no \n    yes \n    no \n    16 \n    no \n    7.29574 \n    1 \n    1474.0073 \n  \n  \n    168 \n    1 \n    3 \n    40 \n    no \n    0 \n    no \n    yes \n    yes \n    no \n    17 \n    no \n    6.23245 \n    1 \n    509.0010 \n  \n  \n    168 \n    2 \n    4 \n    42 \n    no \n    0 \n    no \n    yes \n    yes \n    no \n    17 \n    no \n    6.57925 \n    1 \n    719.9991 \n  \n  \n    168 \n    3 \n    5 \n    44 \n    no \n    0 \n    no \n    yes \n    yes \n    yes \n    17 \n    no \n    6.65286 \n    1 \n    774.9977 \n  \n  \n    168 \n    4 \n    6 \n    48 \n    no \n    0 \n    no \n    yes \n    yes \n    yes \n    17 \n    no \n    6.74524 \n    1 \n    850.0031 \n  \n  \n    168 \n    5 \n    7 \n    48 \n    no \n    0 \n    no \n    yes \n    yes \n    no \n    17 \n    no \n    7.49554 \n    1 \n    1799.9965 \n  \n  \n    168 \n    6 \n    8 \n    48 \n    no \n    0 \n    no \n    yes \n    yes \n    no \n    17 \n    no \n    8.16052 \n    1 \n    3500.0061 \n  \n  \n    168 \n    7 \n    9 \n    50 \n    no \n    0 \n    no \n    yes \n    yes \n    no \n    17 \n    no \n    8.30820 \n    1 \n    4057.0038"
  },
  {
    "objectID": "slides/w08-panel-regression.html#pooled-regression",
    "href": "slides/w08-panel-regression.html#pooled-regression",
    "title": "Panel Regression Analysis",
    "section": "Pooled Regression",
    "text": "Pooled Regression\n\\[\nlog(\\text{Wage}_{it}) = \\beta_0 + \\beta_1 \\cdot \\text{Union}_{it} + \\beta_2 \\cdot {X_{it}} + \\epsilon_{it}\n\\]\n\n\nRegression model on all observations in the panel data set without any individual effects.\n\n\nunion_fit_0 <- lm(log(wage) ~ union + ed + exp + wks, data = wage_dta)\nunion_fit_0\n\n\n\n\nCall:\nlm(formula = log(wage) ~ union + ed + exp + wks, data = wage_dta)\n\nCoefficients:\n(Intercept)     unionyes           ed          exp          wks  \n   4.904952     0.133921     0.082861     0.013193     0.008467"
  },
  {
    "objectID": "slides/w08-panel-regression.html#least-squares-dummy-variable-regression",
    "href": "slides/w08-panel-regression.html#least-squares-dummy-variable-regression",
    "title": "Panel Regression Analysis",
    "section": "Least-squares dummy variable regression",
    "text": "Least-squares dummy variable regression\n\\[\nlog(\\text{Wage}_{it}) = \\beta_0 + \\beta_1 \\cdot \\text{Union}_{it} + \\beta_2 \\cdot {X_{it}} + \\beta_3 \\cdot \\color{Red}{\\delta_{i}} + \\epsilon_{it}\n\\]\n\nPooled regression plus dummy variable for each individual.\nThis is not a Fixed Effect Panel Regression model, but a form of it.\n\n\nunion_fit_1 <- lm(log(wage) ~ union + ed + exp + wks + factor(id), data = wage_dta)\ncoef(union_fit_1)\n\n\n\n  (Intercept)      unionyes            ed           exp           wks \n 4.5080309947  0.0310063771  0.0926571158  0.0968921019  0.0011035227 \n  factor(id)2   factor(id)3   factor(id)4   factor(id)5   factor(id)6 \n-2.2655221864 -0.0679307502 -2.3947793449 -0.4109418992 -1.3874674503 \n  factor(id)7   factor(id)8   factor(id)9  factor(id)10  factor(id)11 \n-1.1362785960 -1.4477135171  0.2723291673 -0.1075227973 -1.3842484712 \n factor(id)12  factor(id)13  factor(id)14  factor(id)15  factor(id)16 \n-1.2616563373 -1.3172925638 -0.8889020189 -0.8432575405 -1.4745899376 \n factor(id)17  factor(id)18  factor(id)19  factor(id)20  factor(id)21 \n-0.8441587959 -1.2742692661 -3.2082101697 -1.7193081621 -1.7458225333 \n factor(id)22  factor(id)23  factor(id)24  factor(id)25  factor(id)26 \n-1.2996715867  0.1362158912 -0.7882521921 -0.4824654849 -0.6052144027 \n factor(id)27  factor(id)28  factor(id)29  factor(id)30  factor(id)31 \n-0.1639394268 -1.7505153234 -1.8458437659 -1.1971050753 -1.2688614236 \n factor(id)32  factor(id)33  factor(id)34  factor(id)35  factor(id)36 \n-1.8010348079 -0.4993946191 -3.9776169750 -0.1144740507 -0.2558972243 \n factor(id)37  factor(id)38  factor(id)39  factor(id)40  factor(id)41 \n-0.2249786188 -0.6736263981 -0.0025109045 -1.7826220350 -0.9121282370 \n factor(id)42  factor(id)43  factor(id)44  factor(id)45  factor(id)46 \n-2.2237936635  0.2413936161  0.1624657380 -0.8444194003 -0.1974056530 \n factor(id)47  factor(id)48  factor(id)49  factor(id)50  factor(id)51 \n-1.6159192305 -1.7035416114 -1.8647062443 -2.4251811715 -1.9049737011 \n factor(id)52  factor(id)53  factor(id)54  factor(id)55  factor(id)56 \n-1.4889956316 -0.3542162186 -0.3625391918 -1.7719093066 -1.8379124593 \n factor(id)57  factor(id)58  factor(id)59  factor(id)60  factor(id)61 \n-1.5742321625 -2.8782348123 -1.6128342792 -1.5560016659 -1.8558689251 \n factor(id)62  factor(id)63  factor(id)64  factor(id)65  factor(id)66 \n-0.0537704902 -0.1401364357 -2.7097225771 -1.7119738693 -0.9377652717 \n factor(id)67  factor(id)68  factor(id)69  factor(id)70  factor(id)71 \n-2.4506638129 -2.6116256913 -2.0043879179 -0.3437942805  0.0062279026 \n factor(id)72  factor(id)73  factor(id)74  factor(id)75  factor(id)76 \n-0.7770392581 -0.8324925145 -0.6702293878 -1.6657642886 -1.0769968570 \n factor(id)77  factor(id)78  factor(id)79  factor(id)80  factor(id)81 \n-0.7243244748 -3.5242010580 -0.8338248371  0.0349689517 -0.1776675032 \n factor(id)82  factor(id)83  factor(id)84  factor(id)85  factor(id)86 \n-1.2737000464 -0.7906868358 -3.4418176841 -0.5622796402  0.3499130686 \n factor(id)87  factor(id)88  factor(id)89  factor(id)90  factor(id)91 \n-0.5045389733  0.0380401475 -0.6233464898  0.2826280518  0.2620449272 \n factor(id)92  factor(id)93  factor(id)94  factor(id)95  factor(id)96 \n 0.0289977222 -2.3216403929 -0.5280709628 -2.6695298324 -1.8101171540 \n factor(id)97  factor(id)98  factor(id)99 factor(id)100 factor(id)101 \n-2.3424909985 -0.9272796237 -1.1399774230 -0.3052923053 -2.0064100169 \nfactor(id)102 factor(id)103 factor(id)104 factor(id)105 factor(id)106 \n 0.3157008827 -0.9088377852 -1.1835804053 -0.2772538384  0.0581496528 \nfactor(id)107 factor(id)108 factor(id)109 factor(id)110 factor(id)111 \n-1.7912635929 -0.8796058507 -1.3243770872 -2.8463802275 -0.0460009045 \nfactor(id)112 factor(id)113 factor(id)114 factor(id)115 factor(id)116 \n-0.7305311839 -1.7335454885 -2.8425752763 -0.3387170812 -0.6042258392 \nfactor(id)117 factor(id)118 factor(id)119 factor(id)120 factor(id)121 \n-1.0761445880 -1.3451630729 -0.1660476107 -0.6073506039 -0.2294792380 \nfactor(id)122 factor(id)123 factor(id)124 factor(id)125 factor(id)126 \n-0.5827050570 -1.5103279045 -1.8115963996 -0.8185827116  0.2118703253 \nfactor(id)127 factor(id)128 factor(id)129 factor(id)130 factor(id)131 \n-1.7356276361 -2.2764190892 -2.3502707273 -1.8838901593 -1.7774889180 \nfactor(id)132 factor(id)133 factor(id)134 factor(id)135 factor(id)136 \n-1.1039651819 -1.1578763217 -0.2356162218 -0.4350180955 -0.7918763863 \nfactor(id)137 factor(id)138 factor(id)139 factor(id)140 factor(id)141 \n-1.1654461065 -1.2049708577  0.1297104422 -0.0892275833 -0.8983086258 \nfactor(id)142 factor(id)143 factor(id)144 factor(id)145 factor(id)146 \n-2.8736569247 -2.5763786160 -0.2206851710 -1.0460117682 -2.5755773652 \nfactor(id)147 factor(id)148 factor(id)149 factor(id)150 factor(id)151 \n-0.2364974230 -0.1573802939 -0.1374446850 -0.8807032960 -1.6957509318 \nfactor(id)152 factor(id)153 factor(id)154 factor(id)155 factor(id)156 \n-0.3889136714 -0.8036953530 -0.6832796416  0.3284782420 -2.3604419579 \nfactor(id)157 factor(id)158 factor(id)159 factor(id)160 factor(id)161 \n-0.0855476488 -1.9158229929 -2.2570550641 -1.9574349185 -0.3252021885 \nfactor(id)162 factor(id)163 factor(id)164 factor(id)165 factor(id)166 \n-0.5140633822 -0.6273090766 -0.4458525807 -3.1347772728 -0.2183026348 \nfactor(id)167 factor(id)168 factor(id)169 factor(id)170 factor(id)171 \n-1.2239655304  0.4438625670 -1.5297736706 -2.0895503730 -0.9346219685 \nfactor(id)172 factor(id)173 factor(id)174 factor(id)175 factor(id)176 \n-0.0871821856 -1.6966359127  0.1864583479  0.0095012507  0.3129948360 \nfactor(id)177 factor(id)178 factor(id)179 factor(id)180 factor(id)181 \n-1.4989523797  0.3993597409 -2.2172222793 -0.2925061489 -0.3059554133 \nfactor(id)182 factor(id)183 factor(id)184 factor(id)185 factor(id)186 \n-0.8910018832 -1.0454580196 -1.5991940535 -1.4123302001 -2.1886562071 \nfactor(id)187 factor(id)188 factor(id)189 factor(id)190 factor(id)191 \n-1.0673557123 -0.0366403620 -1.6422195644  0.2895208379 -1.4791178455 \nfactor(id)192 factor(id)193 factor(id)194 factor(id)195 factor(id)196 \n 0.0743443142 -2.1729539630 -0.3371512760 -0.6968243523  0.1549563657 \nfactor(id)197 factor(id)198 factor(id)199 factor(id)200 factor(id)201 \n-1.4919082370 -1.8806299735 -0.1043902382 -0.2540378231 -1.4008995546 \nfactor(id)202 factor(id)203 factor(id)204 factor(id)205 factor(id)206 \n-0.4715792510 -1.4354460647 -2.0841559662 -1.5908389011  0.0974942925 \nfactor(id)207 factor(id)208 factor(id)209 factor(id)210 factor(id)211 \n-0.3734463538 -1.5833722358 -1.9386342238 -2.1330802475 -1.4607569689 \nfactor(id)212 factor(id)213 factor(id)214 factor(id)215 factor(id)216 \n 0.0692387738 -1.4362958615 -1.8477752431 -3.5018407719 -0.9702974562 \nfactor(id)217 factor(id)218 factor(id)219 factor(id)220 factor(id)221 \n-0.1864633404 -0.5583055945 -0.7131139439 -2.4184070473 -1.9991061550 \nfactor(id)222 factor(id)223 factor(id)224 factor(id)225 factor(id)226 \n 0.0252310278 -0.9071314759 -1.5476122285 -0.4589191769 -2.9925640720 \nfactor(id)227 factor(id)228 factor(id)229 factor(id)230 factor(id)231 \n-2.2095473327  0.3349890128 -1.1599356203  0.3085005957 -0.9650086661 \nfactor(id)232 factor(id)233 factor(id)234 factor(id)235 factor(id)236 \n-2.1620488993 -1.8096153228 -1.5562606230 -1.1494961437  0.0996192770 \nfactor(id)237 factor(id)238 factor(id)239 factor(id)240 factor(id)241 \n-0.5395924150 -2.1478127095 -1.3393016192 -2.0580183534 -0.5006546958 \nfactor(id)242 factor(id)243 factor(id)244 factor(id)245 factor(id)246 \n-0.5467203416  0.3174743400 -1.3575828629 -2.4005959097 -2.8705268871 \nfactor(id)247 factor(id)248 factor(id)249 factor(id)250 factor(id)251 \n-0.6680143531 -1.0334924832 -2.4259470807 -1.2501498259 -0.5113721708 \nfactor(id)252 factor(id)253 factor(id)254 factor(id)255 factor(id)256 \n-1.4304018364 -0.0325545732 -2.6562757026  0.0656002646 -0.1480848210 \nfactor(id)257 factor(id)258 factor(id)259 factor(id)260 factor(id)261 \n-2.1835747504 -2.1795404242 -0.6462803350 -0.0226215308 -0.7093864827 \nfactor(id)262 factor(id)263 factor(id)264 factor(id)265 factor(id)266 \n-0.4419561296 -0.5194252649 -0.0738509630 -0.5043753168 -1.3893601795 \nfactor(id)267 factor(id)268 factor(id)269 factor(id)270 factor(id)271 \n-0.5388875817 -0.7334291072 -0.8729609312 -0.7036995922 -0.3483834285 \nfactor(id)272 factor(id)273 factor(id)274 factor(id)275 factor(id)276 \n-0.1503155312 -1.6624965066 -0.8988125791  0.0182397780  0.3830322685 \nfactor(id)277 factor(id)278 factor(id)279 factor(id)280 factor(id)281 \n-2.0641062977  0.2842052782 -0.5571589206 -2.7175197487  0.2712451990 \nfactor(id)282 factor(id)283 factor(id)284 factor(id)285 factor(id)286 \n-1.2350275452 -2.9457801191 -2.1976344024  0.2953395631 -0.2979508412 \nfactor(id)287 factor(id)288 factor(id)289 factor(id)290 factor(id)291 \n-0.5060374166 -0.1389329664 -1.5606159775 -0.0496261975 -2.5588631607 \nfactor(id)292 factor(id)293 factor(id)294 factor(id)295 factor(id)296 \n-1.7353127339 -0.4838661547 -1.0950139771 -1.3625604521 -2.0039274853 \nfactor(id)297 factor(id)298 factor(id)299 factor(id)300 factor(id)301 \n-0.4900311502 -0.6255232917 -3.2764384180 -2.7874789491 -0.2320672807 \nfactor(id)302 factor(id)303 factor(id)304 factor(id)305 factor(id)306 \n-0.9204708005  0.3208647271 -0.4017890824 -0.9748110949 -1.1893175018 \nfactor(id)307 factor(id)308 factor(id)309 factor(id)310 factor(id)311 \n 0.8071047654 -1.7498638622 -1.2960178469 -0.6605396033  0.2580087741 \nfactor(id)312 factor(id)313 factor(id)314 factor(id)315 factor(id)316 \n-0.7528944270 -0.5506410012 -0.3941112826 -0.9229402595 -1.8274179626 \nfactor(id)317 factor(id)318 factor(id)319 factor(id)320 factor(id)321 \n-0.8247447772 -0.5802491953 -1.5586914742 -0.9323836374 -0.2039400846 \nfactor(id)322 factor(id)323 factor(id)324 factor(id)325 factor(id)326 \n-0.2921333885 -0.6743960485  0.2622449597 -3.1733394515 -1.2746050004 \nfactor(id)327 factor(id)328 factor(id)329 factor(id)330 factor(id)331 \n-0.7198079782 -1.9716336831 -3.2765849294 -2.8887402424 -2.5642055811 \nfactor(id)332 factor(id)333 factor(id)334 factor(id)335 factor(id)336 \n-3.5439335180  0.1442706620 -2.5202972059 -1.9535704434 -2.2741984666 \nfactor(id)337 factor(id)338 factor(id)339 factor(id)340 factor(id)341 \n 0.6128078989 -0.5285042151 -0.8065665718 -1.9086103203  0.2025478485 \nfactor(id)342 factor(id)343 factor(id)344 factor(id)345 factor(id)346 \n-1.8569597798 -1.4207532392 -0.8177345172 -1.1359718040 -1.5565812102 \nfactor(id)347 factor(id)348 factor(id)349 factor(id)350 factor(id)351 \n-1.1854475295 -1.2825974474  0.5210211695 -2.6392655774 -1.3267929343 \nfactor(id)352 factor(id)353 factor(id)354 factor(id)355 factor(id)356 \n 0.1483095360 -1.6234121024 -1.2639442268 -0.3914705698  0.3968436943 \nfactor(id)357 factor(id)358 factor(id)359 factor(id)360 factor(id)361 \n-0.8625592693 -1.2512615948 -3.1090549004 -1.6305910533 -0.6092150258 \nfactor(id)362 factor(id)363 factor(id)364 factor(id)365 factor(id)366 \n-0.8371178797 -0.9898744078 -1.3642822793 -2.6162066308 -0.5612725365 \nfactor(id)367 factor(id)368 factor(id)369 factor(id)370 factor(id)371 \n-1.2495822315 -0.2187092395  0.4178998056  0.0078933499 -1.9408407708 \nfactor(id)372 factor(id)373 factor(id)374 factor(id)375 factor(id)376 \n-0.5027177298 -1.0854903868 -0.8835680799 -1.9056647603 -1.9023803800 \nfactor(id)377 factor(id)378 factor(id)379 factor(id)380 factor(id)381 \n-0.9639363237  0.3406054553 -0.9413693239 -2.2397839447 -1.7982803154 \nfactor(id)382 factor(id)383 factor(id)384 factor(id)385 factor(id)386 \n-1.4710100144 -1.8264254416 -2.4723075037 -0.0768100019 -0.5160529107 \nfactor(id)387 factor(id)388 factor(id)389 factor(id)390 factor(id)391 \n-0.2578492866 -2.3248529615 -1.0053657636 -1.7411722647 -2.5827446340 \nfactor(id)392 factor(id)393 factor(id)394 factor(id)395 factor(id)396 \n-1.4626183438 -0.7086413720  0.2463182313 -2.2837492689 -0.5884685114 \nfactor(id)397 factor(id)398 factor(id)399 factor(id)400 factor(id)401 \n 0.2356664347 -0.7086465595 -1.7699758466 -0.2998927907  0.3625535025 \nfactor(id)402 factor(id)403 factor(id)404 factor(id)405 factor(id)406 \n-0.7479742774 -0.8356707689 -0.6119443914  0.4318878206  0.2769301765 \nfactor(id)407 factor(id)408 factor(id)409 factor(id)410 factor(id)411 \n-1.6938634436 -0.3449254188 -3.0796976040 -1.9186588812 -0.3448521794 \nfactor(id)412 factor(id)413 factor(id)414 factor(id)415 factor(id)416 \n 0.5344690854 -0.2754098403 -2.9000592046 -1.5516997952 -2.1976862146 \nfactor(id)417 factor(id)418 factor(id)419 factor(id)420 factor(id)421 \n-0.3349106320 -2.0011123980 -0.2497252685 -0.3094673540 -1.3378014218 \nfactor(id)422 factor(id)423 factor(id)424 factor(id)425 factor(id)426 \n-0.4763015280 -0.5240316478  0.2449675285  0.3554911994  0.2639519275 \nfactor(id)427 factor(id)428 factor(id)429 factor(id)430 factor(id)431 \n-1.7757641083 -2.9988699763 -0.5174251966 -0.3311521857 -0.6388796236 \nfactor(id)432 factor(id)433 factor(id)434 factor(id)435 factor(id)436 \n-1.5023081692 -1.8757056922  0.2045655478 -0.1300569248 -0.2414767262 \nfactor(id)437 factor(id)438 factor(id)439 factor(id)440 factor(id)441 \n-1.0147851794 -1.7232776561 -0.2994656507 -0.8008434818 -2.2896856636 \nfactor(id)442 factor(id)443 factor(id)444 factor(id)445 factor(id)446 \n-3.5228083702 -1.5956581751  0.7607259370 -0.9304985177 -0.4575093394 \nfactor(id)447 factor(id)448 factor(id)449 factor(id)450 factor(id)451 \n-1.7447682740 -2.3280084075 -0.1979985646  0.1968641662 -0.2394807998 \nfactor(id)452 factor(id)453 factor(id)454 factor(id)455 factor(id)456 \n-0.6871992018 -2.2832695958  0.0553588667 -1.6052273642 -1.3604415774 \nfactor(id)457 factor(id)458 factor(id)459 factor(id)460 factor(id)461 \n-2.9330839431 -0.7950325315 -1.3101227633  0.1845563521 -2.1887667590 \nfactor(id)462 factor(id)463 factor(id)464 factor(id)465 factor(id)466 \n-2.6588026903 -0.7844760292 -3.6891418929 -0.2048295480 -2.1924740196 \nfactor(id)467 factor(id)468 factor(id)469 factor(id)470 factor(id)471 \n-1.7733473344 -3.0882373096 -4.6017229797 -0.0156170664 -2.2125823815 \nfactor(id)472 factor(id)473 factor(id)474 factor(id)475 factor(id)476 \n-0.3975606405 -0.0023810456 -0.5669107940 -0.5452321990 -2.1619484195 \nfactor(id)477 factor(id)478 factor(id)479 factor(id)480 factor(id)481 \n-0.2849685250 -0.0642931007 -0.7619497004 -1.2719168687 -1.2089275559 \nfactor(id)482 factor(id)483 factor(id)484 factor(id)485 factor(id)486 \n-0.2446361845 -0.2852928955  0.4205269799 -1.0918704373 -1.7871075911 \nfactor(id)487 factor(id)488 factor(id)489 factor(id)490 factor(id)491 \n-0.2186232164 -1.6473050452 -2.2352049887 -1.6895014356  0.0093056252 \nfactor(id)492 factor(id)493 factor(id)494 factor(id)495 factor(id)496 \n-1.8240991047 -0.5181324012 -3.7789715200 -0.2995020877 -1.5719156294 \nfactor(id)497 factor(id)498 factor(id)499 factor(id)500 factor(id)501 \n-0.4227856875 -1.3891231669 -2.5486749069 -0.1770255946 -2.0239835831 \nfactor(id)502 factor(id)503 factor(id)504 factor(id)505 factor(id)506 \n-0.8841523636  0.4548914009  0.0415337729 -1.2715375785  0.1577186541 \nfactor(id)507 factor(id)508 factor(id)509 factor(id)510 factor(id)511 \n-1.6492387018 -2.6857072685 -1.0222254888  0.2682863195 -0.7274089220 \nfactor(id)512 factor(id)513 factor(id)514 factor(id)515 factor(id)516 \n 0.4763623194 -2.6688387156 -0.8605254000 -0.5576396661 -1.6446136441 \nfactor(id)517 factor(id)518 factor(id)519 factor(id)520 factor(id)521 \n-0.7938604091 -1.0129183582 -0.2512720094 -1.9850804956 -1.0799849364 \nfactor(id)522 factor(id)523 factor(id)524 factor(id)525 factor(id)526 \n 0.4697657617  0.3596403551 -0.3399189001 -0.5549848962 -0.0701031604 \nfactor(id)527 factor(id)528 factor(id)529 factor(id)530 factor(id)531 \n-0.1188496468 -0.3371025673 -2.0618098622 -0.0152701464  0.5611969023 \nfactor(id)532 factor(id)533 factor(id)534 factor(id)535 factor(id)536 \n-0.3043214598 -2.7035534598 -0.4556794141 -0.9822287470 -0.1899453332 \nfactor(id)537 factor(id)538 factor(id)539 factor(id)540 factor(id)541 \n-0.3095642727 -0.4033201563  0.2336208827 -1.3288471720 -0.2192632278 \nfactor(id)542 factor(id)543 factor(id)544 factor(id)545 factor(id)546 \n-0.9016210223 -0.0808041489  0.3911389336 -2.6144746127 -0.1760991315 \nfactor(id)547 factor(id)548 factor(id)549 factor(id)550 factor(id)551 \n 0.1351162874 -0.2655470565 -0.5352420192 -0.0269963448 -0.3822695480 \nfactor(id)552 factor(id)553 factor(id)554 factor(id)555 factor(id)556 \n-0.4154265877 -0.2116433988 -1.3752666395 -1.5874830748  0.4404262290 \nfactor(id)557 factor(id)558 factor(id)559 factor(id)560 factor(id)561 \n-0.9226222629 -1.0725980066 -3.2024279311 -2.5480062591  0.0979656839 \nfactor(id)562 factor(id)563 factor(id)564 factor(id)565 factor(id)566 \n 0.2441815397 -2.2284900220  0.1573326500 -1.1264255882  0.2474138501 \nfactor(id)567 factor(id)568 factor(id)569 factor(id)570 factor(id)571 \n-3.7249592849 -0.1598818078 -0.9379939705 -0.6580067426  0.0098865558 \nfactor(id)572 factor(id)573 factor(id)574 factor(id)575 factor(id)576 \n 0.0698585935 -0.1342958054  0.2927163728 -0.5525955041  0.2742423461 \nfactor(id)577 factor(id)578 factor(id)579 factor(id)580 factor(id)581 \n-0.7248170828  0.0001653778 -0.3764030891  0.1028761911 -0.2488290888 \nfactor(id)582 factor(id)583 factor(id)584 factor(id)585 factor(id)586 \n 0.3608357712  0.1716823235  0.0733130210 -0.0212577150 -1.2216850413 \nfactor(id)587 factor(id)588 factor(id)589 factor(id)590 factor(id)591 \n-0.2479521116 -0.2529055249 -2.0992273563  0.3733929902 -2.2589147119 \nfactor(id)592 factor(id)593 factor(id)594 factor(id)595 \n 0.0842891323 -1.0349524512  0.0308381380            NA"
  },
  {
    "objectID": "slides/w08-panel-regression.html#data-structure-in-the-lsdv",
    "href": "slides/w08-panel-regression.html#data-structure-in-the-lsdv",
    "title": "Panel Regression Analysis",
    "section": "Data structure in the LSDV",
    "text": "Data structure in the LSDV\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nTime\nY\nX1\nX2\n\\(\\delta_1\\)\n\\(\\delta_2\\)\n\\(\\delta_N\\)\n\n\n\n\n\\(1\\)\n\\(1\\)\n\\(y_{11}\\)\n\\(x^{1}_{11}\\)\n\\(x^{2}_{11}\\)\n1\n0\n0\n\n\n\\(1\\)\n\\(2\\)\n\\(y_{12}\\)\n\\(x^{1}_{12}\\)\n\\(x^{2}_{12}\\)\n1\n0\n0\n\n\n\\(1\\)\n\\(3\\)\n\\(y_{13}\\)\n\\(x^{1}_{13}\\)\n\\(x^{2}_{13}\\)\n1\n0\n0\n\n\n\\(2\\)\n\\(2\\)\n\\(y_{22}\\)\n\\(x^{1}_{22}\\)\n\\(x^{2}_{22}\\)\n0\n1\n0\n\n\n\\(2\\)\n\\(3\\)\n\\(y_{23}\\)\n\\(x^{1}_{23}\\)\n\\(x^{2}_{23}\\)\n0\n1\n0\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(N\\)\n\\(1\\)\n\\(y_{N1}\\)\n\\(x^{1}_{N1}\\)\n\\(x^{1}_{N1}\\)\n0\n0\n1\n\n\n\\(N\\)\n\\(2\\)\n\\(y_{N2}\\)\n\\(x^{1}_{N2}\\)\n\\(x^{2}_{N2}\\)\n0\n0\n1"
  },
  {
    "objectID": "slides/w08-panel-regression.html#results",
    "href": "slides/w08-panel-regression.html#results",
    "title": "Panel Regression Analysis",
    "section": "Results",
    "text": "Results\n\nmodelsummary(\n  list(\n    `Pooled` = union_fit_0, \n    `Least-squares dummy variable` = union_fit_1),\n  estimate = \"{estimate}{stars} ({std.error})\", \n  statistic = NULL,\n  coef_map = c(\"(Intercept)\", \"unionyes\", \"ed\", \"exp\", \"wks\"),\n  gof_map = c(\"nobs\", \"adj.r.squared\" , \"df\"),\n  notes = \"In the Least-squares dummy variable model we omitted all individual-related variables\"\n)\n\n\n\n\n\n \n  \n      \n    Pooled \n    Least-squares dummy variable \n  \n \n\n  \n    (Intercept) \n    4.905*** (0.070) \n    4.508*** (0.290) \n  \n  \n    unionyes \n    0.134*** (0.013) \n    0.031* (0.015) \n  \n  \n    ed \n    0.083*** (0.002) \n    0.093*** (0.027) \n  \n  \n    exp \n    0.013*** (0.001) \n    0.097*** (0.001) \n  \n  \n    wks \n    0.008*** (0.001) \n    0.001+ (0.001) \n  \n  \n    Num.Obs. \n    4165 \n    4165 \n  \n  \n    R2 Adj. \n    0.269 \n    0.890 \n  \n  \n    DF \n    4 \n    597 \n  \n\n\n In the Least-squares dummy variable model we omitted all individual-related variables"
  },
  {
    "objectID": "slides/w08-panel-regression.html#can-we-run-a-least-squares-dummy-variable-with-the-cross-sectional-data",
    "href": "slides/w08-panel-regression.html#can-we-run-a-least-squares-dummy-variable-with-the-cross-sectional-data",
    "title": "Panel Regression Analysis",
    "section": "Can we run a Least-squares dummy variable with the cross-sectional data?",
    "text": "Can we run a Least-squares dummy variable with the cross-sectional data?\n\nAny ideas?\nWhy?….\nNO…\nBecause the number of independent variables have to be less then or equal to the number of observations."
  },
  {
    "objectID": "slides/w08-panel-regression.html#cross-sectional-example",
    "href": "slides/w08-panel-regression.html#cross-sectional-example",
    "title": "Panel Regression Analysis",
    "section": "Cross-sectional example",
    "text": "Cross-sectional example\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nY\nX1\nX2\n\\(\\delta_1\\)\n\\(\\delta_2\\)\n\\(\\delta_3\\)\n\\(\\delta_N\\)\n\n\n\n\n\\(1\\)\n\\(y_{1}\\)\n\\(x^{1}_{1}\\)\n\\(x^{2}_{1}\\)\n1\n0\n0\n0\n\n\n\\(2\\)\n\\(y_{2}\\)\n\\(x^{1}_{2}\\)\n\\(x^{2}_{2}\\)\n0\n1\n0\n0\n\n\n\\(3\\)\n\\(y_{3}\\)\n\\(x^{1}_{3}\\)\n\\(x^{2}_{3}\\)\n0\n0\n1\n0\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(N\\)\n\\(y_{N}\\)\n\\(x^{1}_{N}\\)\n\\(x^{1}_{N}\\)\n0\n0\n0\n1"
  },
  {
    "objectID": "slides/w08-panel-regression.html#with-the-panel-data-lsdv-model-works-but",
    "href": "slides/w08-panel-regression.html#with-the-panel-data-lsdv-model-works-but",
    "title": "Panel Regression Analysis",
    "section": "With the panel data LSDV model works, but…",
    "text": "With the panel data LSDV model works, but…\nit is inefficient! Any ideas why?…\n\nNumber of dummy variables is equal to the number of individuals + control variables.\n\nIf we have 5,000 individuals, we have 5,000+ regression coefficients.\nWhat if we have 100,000 individuals?\n\nHaving too many regressors remains unbiased, but complicates inference:\n\nnumber of degrees of freedom increases;\nadjusted \\(R^2\\) may shrink to zero;"
  },
  {
    "objectID": "slides/w08-panel-regression.html#readings",
    "href": "slides/w08-panel-regression.html#readings",
    "title": "Panel Regression Analysis",
    "section": "Readings",
    "text": "Readings\n\nKey readings:\n\nMundlak (1961)\nAngrist & Pischke (2009) Ch. 5\nJ. M. Wooldridge (2010);\nM. J. Wooldridge (2020);\nSöderbom, Teal, & Eberhardt (2014), Ch. 9-11\n\nOther readings:\n\nCroissant & Millo (2018)"
  },
  {
    "objectID": "slides/w08-panel-regression.html#terminology",
    "href": "slides/w08-panel-regression.html#terminology",
    "title": "Panel Regression Analysis",
    "section": "Terminology",
    "text": "Terminology\nPanel data has \\(i\\) individuals (groups) and \\(t\\) time periods for each individual;\n\nPooled OLS (regression without any panel structure);\nFixed Effect:\n\nLeast-squares dummy variable (Pooled OLS + individual dummies);\n\nPanel Regression Fixed Effect:\n\nWithin-transformation most commonly used\nFirst-difference, Between transformation (look it up in (Croissant & Millo, 2018))\n\nPanel Regression Random Effect"
  },
  {
    "objectID": "slides/w08-panel-regression.html#pooled-ols",
    "href": "slides/w08-panel-regression.html#pooled-ols",
    "title": "Panel Regression Analysis",
    "section": "Pooled OLS",
    "text": "Pooled OLS\n\\[\ny_{it} = \\beta_0 + \\beta_1 \\cdot x_{it}  +  \\epsilon_{it}\n\\]\n\nEstimates are biased because of the OVB.\nWe assume the OVB to be time-invariant (Mundlak, 1961)."
  },
  {
    "objectID": "slides/w08-panel-regression.html#least-squares-dummy-variable-model",
    "href": "slides/w08-panel-regression.html#least-squares-dummy-variable-model",
    "title": "Panel Regression Analysis",
    "section": "Least-squares dummy variable model",
    "text": "Least-squares dummy variable model\n\\[\ny_{it} = \\beta_0 + \\beta_1 \\cdot x_{it}  + \\gamma_i \\cdot \\color{Red}{\\delta_{i}} + \\epsilon_{it}\n\\]\n\nIntroduces a vector of dummy variables \\(\\color{Red}{\\delta}\\) and estimated coefficients for each dummy variable \\(\\gamma_i\\).\nEstimates and unbiased (consistent) but inefficient."
  },
  {
    "objectID": "slides/w08-panel-regression.html#panel-regression-fixed-effect-model",
    "href": "slides/w08-panel-regression.html#panel-regression-fixed-effect-model",
    "title": "Panel Regression Analysis",
    "section": "Panel Regression Fixed Effect Model",
    "text": "Panel Regression Fixed Effect Model\n\n\nWe apply a within-transformation or a first difference transformation:\n\nsubtract group means from each observation\nrun regression on the differences\n\n\n\\[\n\\begin{aligned}\nlog(\\text{Wage}_{it} - \\overline{\\text{Wage}_{i}}) & =  \\beta_0 \\\\\n& + \\beta_1 \\cdot (\\text{Union}_{it} -  \\overline{\\text{Union}_{i}}) \\\\\n& + \\beta_2 \\cdot (X_{it} - \\overline{\\text{X}_{i}}) \\\\\n& +  \\beta_3 \\cdot (\\text{Ability}_{i} - \\overline{\\text{Ability}_{i}}) \\\\\n& +  (\\epsilon_{it} - \\overline{\\epsilon_{i}})\n\\end{aligned}\n\\]\n\nNote:\n\ntime-invariant effects disappear:\n\n\\(\\text{Ability}_{i} - \\overline{\\text{Ability}_{i}} = 0\\).\nOther examples?\nGender, race, individual characteristics …\n\nFE (Within) estimates are identical to the LSDV, but SE are more efficient;"
  },
  {
    "objectID": "slides/w08-panel-regression.html#within-transformation---the-key-method",
    "href": "slides/w08-panel-regression.html#within-transformation---the-key-method",
    "title": "Panel Regression Analysis",
    "section": "Within transformation - the key method",
    "text": "Within transformation - the key method\n\n\\[\n\\begin{aligned}\nlog(\\text{Wage}_{it} - \\overline{\\text{Wage}_{i}}) & =  \\beta_0 \\\\\n& + \\beta_1 \\cdot (\\text{Union}_{it} -  \\overline{\\text{Union}_{i}}) \\\\\n& + \\beta_2 \\cdot (X_{it} - \\overline{\\text{X}_{i}}) \\\\\n& +  \\beta_3 \\cdot (\\text{Ability}_{i} - \\overline{\\text{Ability}_{i}}) \\\\\n& +  (\\epsilon_{it} - \\overline{\\epsilon_{i}})\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/w08-panel-regression.html#first-difference-transformation",
    "href": "slides/w08-panel-regression.html#first-difference-transformation",
    "title": "Panel Regression Analysis",
    "section": "First Difference transformation",
    "text": "First Difference transformation\n\n\\[\n\\begin{aligned}\nlog(\\text{Wage}_{it} - {\\text{Wage}_{i,t-1}}) & = \\beta_0 \\\\\n& + \\beta_1 \\cdot (\\text{Union}_{it} -  {\\text{Union}_{it-1}}) \\\\\n& + \\beta_2 \\cdot (X_{it} - {{X}_{i,t-1}}) \\\\\n& + \\beta_3 \\cdot (\\text{Ability}_{i} - {\\text{Ability}_{i,t-1}}) \\\\\n& + (\\epsilon_{it} - {\\epsilon_{i,t-1}})\n\\end{aligned}\n\\]\nNote:\n\nSimilar to the within transformation, but sacrifices at least one time dimension.\nRelaxes the autocorrelation assumption.\nMay be not possible with unbalanced data."
  },
  {
    "objectID": "slides/w08-panel-regression.html#panel-regression-fe-model-assumptions-1",
    "href": "slides/w08-panel-regression.html#panel-regression-fe-model-assumptions-1",
    "title": "Panel Regression Analysis",
    "section": "Panel Regression FE model assumptions (1)",
    "text": "Panel Regression FE model assumptions (1)\n\nNOT ZERO CORRELATION between effects and regressors: \\(Cov(\\delta_{i},{X}_{it}) \\neq 0\\)\nStrict exogeneity: \\(E[\\epsilon_{is}| {X}_{it}, \\delta_{i}] = 0\\)\n\n\\(Cov(\\epsilon_{is}, {X}_{jt}) = 0\\) and \\(Cov(\\epsilon_{it}, {X}_{it}) = 0\\) , where \\(j\\neq i\\) and \\(s\\neq t\\) ;\nResiduals (\\(\\epsilon\\)) do not correlate with all explanatory variable (\\(X\\)) in all time periods (\\(t\\)) and for all individuals (\\(i\\)).\n\nNo autocorrelation/serial correlation: \\(Cov(\\epsilon_{it}, {X}_{i,t-1}) = 0\\);\nNo cross-sectional dependence: \\(Cov(\\epsilon_{it}, {X}_{j,t}) = 0\\) (when individual observations react similarly to the common shocks or correlate in space);"
  },
  {
    "objectID": "slides/w08-panel-regression.html#panel-regression-fe-model-not-less-important-assumptions-2",
    "href": "slides/w08-panel-regression.html#panel-regression-fe-model-not-less-important-assumptions-2",
    "title": "Panel Regression Analysis",
    "section": "Panel Regression FE model not less important assumptions (2)",
    "text": "Panel Regression FE model not less important assumptions (2)\n\nAll Gauss-Markov assumptions\n\nLinearity\nRandom sampling\nNo endogeneity\nNo collinearity\n\nHomoscedasticity of error terms: \\(Var(\\delta_{i}|{X}_{it}) = \\sigma^2_{\\delta}\\)\nNormality of the residuals"
  },
  {
    "objectID": "slides/w08-panel-regression.html#fixed-effect-literature",
    "href": "slides/w08-panel-regression.html#fixed-effect-literature",
    "title": "Panel Regression Analysis",
    "section": "Fixed effect: literature",
    "text": "Fixed effect: literature\nSeminal papers: Mundlak (1961)\nClimate and agriculture: Mendelsohn, Nordhaus, & Shaw (1994), Blanc & Schlenker (2017), Bozzola, Massetti, Mendelsohn, & Capitanio (2017)\nChoice of irrigation: Kurukulasuriya, Kala, & Mendelsohn (2011), Chatzopoulos & Lippert (2015)\nCrop choice: Kurukulasuriya, Mendelsohn, Kurukulasuriya, & Mendelsohn (2008), Seo & Mendelsohn (2008b),\nLivestock choice Seo, Mendelsohn, Seo, & Mendelsohn (2008), Seo & Mendelsohn (2008a)\nCross-sectional dependence: Conley (1999)"
  },
  {
    "objectID": "slides/w08-panel-regression.html#random-effect-model",
    "href": "slides/w08-panel-regression.html#random-effect-model",
    "title": "Panel Regression Analysis",
    "section": "Random Effect Model",
    "text": "Random Effect Model\n\nIntroduce a random component of the error term \\(\\color{Red}{v}\\)\n\n\n\\[\nlog(\\text{Wage}_{it}) = \\beta_0 + \\beta_1 \\cdot \\text{Union}_{it}  + \\beta_2 \\cdot X_{it} + \\beta_3 \\cdot \\color{Red}{v_{i}} + \\epsilon_{it}\n\\]\n\nDifference from the fixed effect model:\n\nAssumes NO CORRELATION (ZERO CORRELATION) between effects and regressors: \\(Cov(v_{i},{X}_{it}) = 0\\).\nIgnoring RE causes no bias to the estimates;"
  },
  {
    "objectID": "slides/w08-panel-regression.html#limitations-of-fixed-and-random-effect-models",
    "href": "slides/w08-panel-regression.html#limitations-of-fixed-and-random-effect-models",
    "title": "Panel Regression Analysis",
    "section": "Limitations of Fixed and Random effect models",
    "text": "Limitations of Fixed and Random effect models\n\nNOT the ultimate solution to Endogeneity.\nThere might still be some OVB even with the fixed effects.\n\nInstrumental Variables are possible within the panel regression context too.\n\nMeasurement error may cause endogeneity;"
  },
  {
    "objectID": "slides/w08-panel-regression.html#one-of-the-solutions",
    "href": "slides/w08-panel-regression.html#one-of-the-solutions",
    "title": "Panel Regression Analysis",
    "section": "One of the solutions:",
    "text": "One of the solutions:\n\nAbility are time-invariant and unique to each individual;\n\nIf we have multiple observation per each individual (panel data),\nwe can introduce dummy variables for each individual, to approximate ability.\n\nThis is also called Fixed Effect - regression model\n\nor a within transformation model\nor Difference in Difference"
  },
  {
    "objectID": "slides/w08-panel-regression.html#empirical-example",
    "href": "slides/w08-panel-regression.html#empirical-example",
    "title": "Panel Regression Analysis",
    "section": "Empirical example",
    "text": "Empirical example\n\nlibrary(tidyverse)\nlibrary(modelsummary)\nwage_dta <- read_csv(\"wage_unon_panel.csv\")\nglimpse(wage_dta)\n\n\n\nRows: 4,165\nColumns: 15\n$ id      <dbl> 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3,…\n$ time    <dbl> 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7,…\n$ exp     <dbl> 3, 4, 5, 6, 7, 8, 9, 30, 31, 32, 33, 34, 35, 36, 6, 7, 8, 9, 1…\n$ wks     <dbl> 32, 43, 40, 39, 42, 35, 32, 34, 27, 33, 30, 30, 37, 30, 50, 51…\n$ bluecol <chr> \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"yes\", \"yes\",…\n$ ind     <dbl> 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ south   <chr> \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"no\", \"no\", \"…\n$ smsa    <chr> \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"n…\n$ married <chr> \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\",…\n$ union   <chr> \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"…\n$ ed      <dbl> 9, 9, 9, 9, 9, 9, 9, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 1…\n$ black   <chr> \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"n…\n$ lwage   <dbl> 5.56068, 5.72031, 5.99645, 5.99645, 6.06146, 6.17379, 6.24417,…\n$ female  <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ wage    <dbl> 259.9996, 304.9995, 401.9992, 401.9992, 429.0013, 480.0019, 51…"
  },
  {
    "objectID": "slides/w08-panel-regression.html#least-squares-dummy-variable-lsdv",
    "href": "slides/w08-panel-regression.html#least-squares-dummy-variable-lsdv",
    "title": "Panel Regression Analysis",
    "section": "Least-squares dummy variable (LSDV)",
    "text": "Least-squares dummy variable (LSDV)\n\\[\nlog(\\text{Wage}_{it}) = \\beta_0 + \\beta_1 \\cdot \\text{Union}_{it} + \\beta_2 \\cdot {X_{it}} + \\beta_3 \\cdot \\color{Red}{\\delta_{i}} + \\epsilon_{it}\n\\]\n\n\nPooled regression plus dummy variable for each individual.\nThis is not a Fixed Effect Panel Regression!\n\n\nunion_fit_1 <- lm(log(wage) ~ union + ed + exp + wks + factor(id), data = wage_dta)\nunion_fit_1\n\n\n\n\nCall:\nlm(formula = log(wage) ~ union + ed + exp + wks + factor(id), \n    data = wage_dta)\n\nCoefficients:\n  (Intercept)       unionyes             ed            exp            wks  \n    4.5080310      0.0310064      0.0926571      0.0968921      0.0011035  \n  factor(id)2    factor(id)3    factor(id)4    factor(id)5    factor(id)6  \n   -2.2655222     -0.0679308     -2.3947793     -0.4109419     -1.3874675  \n  factor(id)7    factor(id)8    factor(id)9   factor(id)10   factor(id)11  \n   -1.1362786     -1.4477135      0.2723292     -0.1075228     -1.3842485  \n factor(id)12   factor(id)13   factor(id)14   factor(id)15   factor(id)16  \n   -1.2616563     -1.3172926     -0.8889020     -0.8432575     -1.4745899  \n factor(id)17   factor(id)18   factor(id)19   factor(id)20   factor(id)21  \n   -0.8441588     -1.2742693     -3.2082102     -1.7193082     -1.7458225  \n factor(id)22   factor(id)23   factor(id)24   factor(id)25   factor(id)26  \n   -1.2996716      0.1362159     -0.7882522     -0.4824655     -0.6052144  \n factor(id)27   factor(id)28   factor(id)29   factor(id)30   factor(id)31  \n   -0.1639394     -1.7505153     -1.8458438     -1.1971051     -1.2688614  \n factor(id)32   factor(id)33   factor(id)34   factor(id)35   factor(id)36  \n   -1.8010348     -0.4993946     -3.9776170     -0.1144741     -0.2558972  \n factor(id)37   factor(id)38   factor(id)39   factor(id)40   factor(id)41  \n   -0.2249786     -0.6736264     -0.0025109     -1.7826220     -0.9121282  \n factor(id)42   factor(id)43   factor(id)44   factor(id)45   factor(id)46  \n   -2.2237937      0.2413936      0.1624657     -0.8444194     -0.1974057  \n factor(id)47   factor(id)48   factor(id)49   factor(id)50   factor(id)51  \n   -1.6159192     -1.7035416     -1.8647062     -2.4251812     -1.9049737  \n factor(id)52   factor(id)53   factor(id)54   factor(id)55   factor(id)56  \n   -1.4889956     -0.3542162     -0.3625392     -1.7719093     -1.8379125  \n factor(id)57   factor(id)58   factor(id)59   factor(id)60   factor(id)61  \n   -1.5742322     -2.8782348     -1.6128343     -1.5560017     -1.8558689  \n factor(id)62   factor(id)63   factor(id)64   factor(id)65   factor(id)66  \n   -0.0537705     -0.1401364     -2.7097226     -1.7119739     -0.9377653  \n factor(id)67   factor(id)68   factor(id)69   factor(id)70   factor(id)71  \n   -2.4506638     -2.6116257     -2.0043879     -0.3437943      0.0062279  \n factor(id)72   factor(id)73   factor(id)74   factor(id)75   factor(id)76  \n   -0.7770393     -0.8324925     -0.6702294     -1.6657643     -1.0769969  \n factor(id)77   factor(id)78   factor(id)79   factor(id)80   factor(id)81  \n   -0.7243245     -3.5242011     -0.8338248      0.0349690     -0.1776675  \n factor(id)82   factor(id)83   factor(id)84   factor(id)85   factor(id)86  \n   -1.2737000     -0.7906868     -3.4418177     -0.5622796      0.3499131  \n factor(id)87   factor(id)88   factor(id)89   factor(id)90   factor(id)91  \n   -0.5045390      0.0380401     -0.6233465      0.2826281      0.2620449  \n factor(id)92   factor(id)93   factor(id)94   factor(id)95   factor(id)96  \n    0.0289977     -2.3216404     -0.5280710     -2.6695298     -1.8101172  \n factor(id)97   factor(id)98   factor(id)99  factor(id)100  factor(id)101  \n   -2.3424910     -0.9272796     -1.1399774     -0.3052923     -2.0064100  \nfactor(id)102  factor(id)103  factor(id)104  factor(id)105  factor(id)106  \n    0.3157009     -0.9088378     -1.1835804     -0.2772538      0.0581497  \nfactor(id)107  factor(id)108  factor(id)109  factor(id)110  factor(id)111  \n   -1.7912636     -0.8796059     -1.3243771     -2.8463802     -0.0460009  \nfactor(id)112  factor(id)113  factor(id)114  factor(id)115  factor(id)116  \n   -0.7305312     -1.7335455     -2.8425753     -0.3387171     -0.6042258  \nfactor(id)117  factor(id)118  factor(id)119  factor(id)120  factor(id)121  \n   -1.0761446     -1.3451631     -0.1660476     -0.6073506     -0.2294792  \nfactor(id)122  factor(id)123  factor(id)124  factor(id)125  factor(id)126  \n   -0.5827051     -1.5103279     -1.8115964     -0.8185827      0.2118703  \nfactor(id)127  factor(id)128  factor(id)129  factor(id)130  factor(id)131  \n   -1.7356276     -2.2764191     -2.3502707     -1.8838902     -1.7774889  \nfactor(id)132  factor(id)133  factor(id)134  factor(id)135  factor(id)136  \n   -1.1039652     -1.1578763     -0.2356162     -0.4350181     -0.7918764  \nfactor(id)137  factor(id)138  factor(id)139  factor(id)140  factor(id)141  \n   -1.1654461     -1.2049709      0.1297104     -0.0892276     -0.8983086  \nfactor(id)142  factor(id)143  factor(id)144  factor(id)145  factor(id)146  \n   -2.8736569     -2.5763786     -0.2206852     -1.0460118     -2.5755774  \nfactor(id)147  factor(id)148  factor(id)149  factor(id)150  factor(id)151  \n   -0.2364974     -0.1573803     -0.1374447     -0.8807033     -1.6957509  \nfactor(id)152  factor(id)153  factor(id)154  factor(id)155  factor(id)156  \n   -0.3889137     -0.8036954     -0.6832796      0.3284782     -2.3604420  \nfactor(id)157  factor(id)158  factor(id)159  factor(id)160  factor(id)161  \n   -0.0855476     -1.9158230     -2.2570551     -1.9574349     -0.3252022  \nfactor(id)162  factor(id)163  factor(id)164  factor(id)165  factor(id)166  \n   -0.5140634     -0.6273091     -0.4458526     -3.1347773     -0.2183026  \nfactor(id)167  factor(id)168  factor(id)169  factor(id)170  factor(id)171  \n   -1.2239655      0.4438626     -1.5297737     -2.0895504     -0.9346220  \nfactor(id)172  factor(id)173  factor(id)174  factor(id)175  factor(id)176  \n   -0.0871822     -1.6966359      0.1864583      0.0095013      0.3129948  \nfactor(id)177  factor(id)178  factor(id)179  factor(id)180  factor(id)181  \n   -1.4989524      0.3993597     -2.2172223     -0.2925061     -0.3059554  \nfactor(id)182  factor(id)183  factor(id)184  factor(id)185  factor(id)186  \n   -0.8910019     -1.0454580     -1.5991941     -1.4123302     -2.1886562  \nfactor(id)187  factor(id)188  factor(id)189  factor(id)190  factor(id)191  \n   -1.0673557     -0.0366404     -1.6422196      0.2895208     -1.4791178  \nfactor(id)192  factor(id)193  factor(id)194  factor(id)195  factor(id)196  \n    0.0743443     -2.1729540     -0.3371513     -0.6968244      0.1549564  \nfactor(id)197  factor(id)198  factor(id)199  factor(id)200  factor(id)201  \n   -1.4919082     -1.8806300     -0.1043902     -0.2540378     -1.4008996  \nfactor(id)202  factor(id)203  factor(id)204  factor(id)205  factor(id)206  \n   -0.4715793     -1.4354461     -2.0841560     -1.5908389      0.0974943  \nfactor(id)207  factor(id)208  factor(id)209  factor(id)210  factor(id)211  \n   -0.3734464     -1.5833722     -1.9386342     -2.1330802     -1.4607570  \nfactor(id)212  factor(id)213  factor(id)214  factor(id)215  factor(id)216  \n    0.0692388     -1.4362959     -1.8477752     -3.5018408     -0.9702975  \nfactor(id)217  factor(id)218  factor(id)219  factor(id)220  factor(id)221  \n   -0.1864633     -0.5583056     -0.7131139     -2.4184070     -1.9991062  \nfactor(id)222  factor(id)223  factor(id)224  factor(id)225  factor(id)226  \n    0.0252310     -0.9071315     -1.5476122     -0.4589192     -2.9925641  \nfactor(id)227  factor(id)228  factor(id)229  factor(id)230  factor(id)231  \n   -2.2095473      0.3349890     -1.1599356      0.3085006     -0.9650087  \nfactor(id)232  factor(id)233  factor(id)234  factor(id)235  factor(id)236  \n   -2.1620489     -1.8096153     -1.5562606     -1.1494961      0.0996193  \nfactor(id)237  factor(id)238  factor(id)239  factor(id)240  factor(id)241  \n   -0.5395924     -2.1478127     -1.3393016     -2.0580184     -0.5006547  \nfactor(id)242  factor(id)243  factor(id)244  factor(id)245  factor(id)246  \n   -0.5467203      0.3174743     -1.3575829     -2.4005959     -2.8705269  \nfactor(id)247  factor(id)248  factor(id)249  factor(id)250  factor(id)251  \n   -0.6680144     -1.0334925     -2.4259471     -1.2501498     -0.5113722  \nfactor(id)252  factor(id)253  factor(id)254  factor(id)255  factor(id)256  \n   -1.4304018     -0.0325546     -2.6562757      0.0656003     -0.1480848  \nfactor(id)257  factor(id)258  factor(id)259  factor(id)260  factor(id)261  \n   -2.1835748     -2.1795404     -0.6462803     -0.0226215     -0.7093865  \nfactor(id)262  factor(id)263  factor(id)264  factor(id)265  factor(id)266  \n   -0.4419561     -0.5194253     -0.0738510     -0.5043753     -1.3893602  \nfactor(id)267  factor(id)268  factor(id)269  factor(id)270  factor(id)271  \n   -0.5388876     -0.7334291     -0.8729609     -0.7036996     -0.3483834  \nfactor(id)272  factor(id)273  factor(id)274  factor(id)275  factor(id)276  \n   -0.1503155     -1.6624965     -0.8988126      0.0182398      0.3830323  \nfactor(id)277  factor(id)278  factor(id)279  factor(id)280  factor(id)281  \n   -2.0641063      0.2842053     -0.5571589     -2.7175197      0.2712452  \nfactor(id)282  factor(id)283  factor(id)284  factor(id)285  factor(id)286  \n   -1.2350275     -2.9457801     -2.1976344      0.2953396     -0.2979508  \nfactor(id)287  factor(id)288  factor(id)289  factor(id)290  factor(id)291  \n   -0.5060374     -0.1389330     -1.5606160     -0.0496262     -2.5588632  \nfactor(id)292  factor(id)293  factor(id)294  factor(id)295  factor(id)296  \n   -1.7353127     -0.4838662     -1.0950140     -1.3625605     -2.0039275  \nfactor(id)297  factor(id)298  factor(id)299  factor(id)300  factor(id)301  \n   -0.4900312     -0.6255233     -3.2764384     -2.7874789     -0.2320673  \nfactor(id)302  factor(id)303  factor(id)304  factor(id)305  factor(id)306  \n   -0.9204708      0.3208647     -0.4017891     -0.9748111     -1.1893175  \nfactor(id)307  factor(id)308  factor(id)309  factor(id)310  factor(id)311  \n    0.8071048     -1.7498639     -1.2960178     -0.6605396      0.2580088  \nfactor(id)312  factor(id)313  factor(id)314  factor(id)315  factor(id)316  \n   -0.7528944     -0.5506410     -0.3941113     -0.9229403     -1.8274180  \nfactor(id)317  factor(id)318  factor(id)319  factor(id)320  factor(id)321  \n   -0.8247448     -0.5802492     -1.5586915     -0.9323836     -0.2039401  \nfactor(id)322  factor(id)323  factor(id)324  factor(id)325  factor(id)326  \n   -0.2921334     -0.6743960      0.2622450     -3.1733395     -1.2746050  \nfactor(id)327  factor(id)328  factor(id)329  factor(id)330  factor(id)331  \n   -0.7198080     -1.9716337     -3.2765849     -2.8887402     -2.5642056  \nfactor(id)332  factor(id)333  factor(id)334  factor(id)335  factor(id)336  \n   -3.5439335      0.1442707     -2.5202972     -1.9535704     -2.2741985  \nfactor(id)337  factor(id)338  factor(id)339  factor(id)340  factor(id)341  \n    0.6128079     -0.5285042     -0.8065666     -1.9086103      0.2025478  \nfactor(id)342  factor(id)343  factor(id)344  factor(id)345  factor(id)346  \n   -1.8569598     -1.4207532     -0.8177345     -1.1359718     -1.5565812  \nfactor(id)347  factor(id)348  factor(id)349  factor(id)350  factor(id)351  \n   -1.1854475     -1.2825974      0.5210212     -2.6392656     -1.3267929  \nfactor(id)352  factor(id)353  factor(id)354  factor(id)355  factor(id)356  \n    0.1483095     -1.6234121     -1.2639442     -0.3914706      0.3968437  \nfactor(id)357  factor(id)358  factor(id)359  factor(id)360  factor(id)361  \n   -0.8625593     -1.2512616     -3.1090549     -1.6305911     -0.6092150  \nfactor(id)362  factor(id)363  factor(id)364  factor(id)365  factor(id)366  \n   -0.8371179     -0.9898744     -1.3642823     -2.6162066     -0.5612725  \nfactor(id)367  factor(id)368  factor(id)369  factor(id)370  factor(id)371  \n   -1.2495822     -0.2187092      0.4178998      0.0078933     -1.9408408  \nfactor(id)372  factor(id)373  factor(id)374  factor(id)375  factor(id)376  \n   -0.5027177     -1.0854904     -0.8835681     -1.9056648     -1.9023804  \nfactor(id)377  factor(id)378  factor(id)379  factor(id)380  factor(id)381  \n   -0.9639363      0.3406055     -0.9413693     -2.2397839     -1.7982803  \nfactor(id)382  factor(id)383  factor(id)384  factor(id)385  factor(id)386  \n   -1.4710100     -1.8264254     -2.4723075     -0.0768100     -0.5160529  \nfactor(id)387  factor(id)388  factor(id)389  factor(id)390  factor(id)391  \n   -0.2578493     -2.3248530     -1.0053658     -1.7411723     -2.5827446  \nfactor(id)392  factor(id)393  factor(id)394  factor(id)395  factor(id)396  \n   -1.4626183     -0.7086414      0.2463182     -2.2837493     -0.5884685  \nfactor(id)397  factor(id)398  factor(id)399  factor(id)400  factor(id)401  \n    0.2356664     -0.7086466     -1.7699758     -0.2998928      0.3625535  \nfactor(id)402  factor(id)403  factor(id)404  factor(id)405  factor(id)406  \n   -0.7479743     -0.8356708     -0.6119444      0.4318878      0.2769302  \nfactor(id)407  factor(id)408  factor(id)409  factor(id)410  factor(id)411  \n   -1.6938634     -0.3449254     -3.0796976     -1.9186589     -0.3448522  \nfactor(id)412  factor(id)413  factor(id)414  factor(id)415  factor(id)416  \n    0.5344691     -0.2754098     -2.9000592     -1.5516998     -2.1976862  \nfactor(id)417  factor(id)418  factor(id)419  factor(id)420  factor(id)421  \n   -0.3349106     -2.0011124     -0.2497253     -0.3094674     -1.3378014  \nfactor(id)422  factor(id)423  factor(id)424  factor(id)425  factor(id)426  \n   -0.4763015     -0.5240316      0.2449675      0.3554912      0.2639519  \nfactor(id)427  factor(id)428  factor(id)429  factor(id)430  factor(id)431  \n   -1.7757641     -2.9988700     -0.5174252     -0.3311522     -0.6388796  \nfactor(id)432  factor(id)433  factor(id)434  factor(id)435  factor(id)436  \n   -1.5023082     -1.8757057      0.2045655     -0.1300569     -0.2414767  \nfactor(id)437  factor(id)438  factor(id)439  factor(id)440  factor(id)441  \n   -1.0147852     -1.7232777     -0.2994657     -0.8008435     -2.2896857  \nfactor(id)442  factor(id)443  factor(id)444  factor(id)445  factor(id)446  \n   -3.5228084     -1.5956582      0.7607259     -0.9304985     -0.4575093  \nfactor(id)447  factor(id)448  factor(id)449  factor(id)450  factor(id)451  \n   -1.7447683     -2.3280084     -0.1979986      0.1968642     -0.2394808  \nfactor(id)452  factor(id)453  factor(id)454  factor(id)455  factor(id)456  \n   -0.6871992     -2.2832696      0.0553589     -1.6052274     -1.3604416  \nfactor(id)457  factor(id)458  factor(id)459  factor(id)460  factor(id)461  \n   -2.9330839     -0.7950325     -1.3101228      0.1845564     -2.1887668  \nfactor(id)462  factor(id)463  factor(id)464  factor(id)465  factor(id)466  \n   -2.6588027     -0.7844760     -3.6891419     -0.2048295     -2.1924740  \nfactor(id)467  factor(id)468  factor(id)469  factor(id)470  factor(id)471  \n   -1.7733473     -3.0882373     -4.6017230     -0.0156171     -2.2125824  \nfactor(id)472  factor(id)473  factor(id)474  factor(id)475  factor(id)476  \n   -0.3975606     -0.0023810     -0.5669108     -0.5452322     -2.1619484  \nfactor(id)477  factor(id)478  factor(id)479  factor(id)480  factor(id)481  \n   -0.2849685     -0.0642931     -0.7619497     -1.2719169     -1.2089276  \nfactor(id)482  factor(id)483  factor(id)484  factor(id)485  factor(id)486  \n   -0.2446362     -0.2852929      0.4205270     -1.0918704     -1.7871076  \nfactor(id)487  factor(id)488  factor(id)489  factor(id)490  factor(id)491  \n   -0.2186232     -1.6473050     -2.2352050     -1.6895014      0.0093056  \nfactor(id)492  factor(id)493  factor(id)494  factor(id)495  factor(id)496  \n   -1.8240991     -0.5181324     -3.7789715     -0.2995021     -1.5719156  \nfactor(id)497  factor(id)498  factor(id)499  factor(id)500  factor(id)501  \n   -0.4227857     -1.3891232     -2.5486749     -0.1770256     -2.0239836  \nfactor(id)502  factor(id)503  factor(id)504  factor(id)505  factor(id)506  \n   -0.8841524      0.4548914      0.0415338     -1.2715376      0.1577187  \nfactor(id)507  factor(id)508  factor(id)509  factor(id)510  factor(id)511  \n   -1.6492387     -2.6857073     -1.0222255      0.2682863     -0.7274089  \nfactor(id)512  factor(id)513  factor(id)514  factor(id)515  factor(id)516  \n    0.4763623     -2.6688387     -0.8605254     -0.5576397     -1.6446136  \nfactor(id)517  factor(id)518  factor(id)519  factor(id)520  factor(id)521  \n   -0.7938604     -1.0129184     -0.2512720     -1.9850805     -1.0799849  \nfactor(id)522  factor(id)523  factor(id)524  factor(id)525  factor(id)526  \n    0.4697658      0.3596404     -0.3399189     -0.5549849     -0.0701032  \nfactor(id)527  factor(id)528  factor(id)529  factor(id)530  factor(id)531  \n   -0.1188496     -0.3371026     -2.0618099     -0.0152701      0.5611969  \nfactor(id)532  factor(id)533  factor(id)534  factor(id)535  factor(id)536  \n   -0.3043215     -2.7035535     -0.4556794     -0.9822287     -0.1899453  \nfactor(id)537  factor(id)538  factor(id)539  factor(id)540  factor(id)541  \n   -0.3095643     -0.4033202      0.2336209     -1.3288472     -0.2192632  \nfactor(id)542  factor(id)543  factor(id)544  factor(id)545  factor(id)546  \n   -0.9016210     -0.0808041      0.3911389     -2.6144746     -0.1760991  \nfactor(id)547  factor(id)548  factor(id)549  factor(id)550  factor(id)551  \n    0.1351163     -0.2655471     -0.5352420     -0.0269963     -0.3822695  \nfactor(id)552  factor(id)553  factor(id)554  factor(id)555  factor(id)556  \n   -0.4154266     -0.2116434     -1.3752666     -1.5874831      0.4404262  \nfactor(id)557  factor(id)558  factor(id)559  factor(id)560  factor(id)561  \n   -0.9226223     -1.0725980     -3.2024279     -2.5480063      0.0979657  \nfactor(id)562  factor(id)563  factor(id)564  factor(id)565  factor(id)566  \n    0.2441815     -2.2284900      0.1573327     -1.1264256      0.2474139  \nfactor(id)567  factor(id)568  factor(id)569  factor(id)570  factor(id)571  \n   -3.7249593     -0.1598818     -0.9379940     -0.6580067      0.0098866  \nfactor(id)572  factor(id)573  factor(id)574  factor(id)575  factor(id)576  \n    0.0698586     -0.1342958      0.2927164     -0.5525955      0.2742423  \nfactor(id)577  factor(id)578  factor(id)579  factor(id)580  factor(id)581  \n   -0.7248171      0.0001654     -0.3764031      0.1028762     -0.2488291  \nfactor(id)582  factor(id)583  factor(id)584  factor(id)585  factor(id)586  \n    0.3608358      0.1716823      0.0733130     -0.0212577     -1.2216850  \nfactor(id)587  factor(id)588  factor(id)589  factor(id)590  factor(id)591  \n   -0.2479521     -0.2529055     -2.0992274      0.3733930     -2.2589147  \nfactor(id)592  factor(id)593  factor(id)594  factor(id)595  \n    0.0842891     -1.0349525      0.0308381             NA"
  },
  {
    "objectID": "slides/w08-panel-regression.html#cross-sectional-data-and-lsdv-1",
    "href": "slides/w08-panel-regression.html#cross-sectional-data-and-lsdv-1",
    "title": "Panel Regression Analysis",
    "section": "Cross-sectional data and LSDV (1)",
    "text": "Cross-sectional data and LSDV (1)\n\nCan we run a LSDV model with the cross-sectional data?\n\n\nAny ideas?\nWhy?….\nNO…\nBecause the number of independent variables have to be less then or equal to the number of observations."
  },
  {
    "objectID": "slides/w08-panel-regression.html#cross-sectional-data-and-lsdv-2",
    "href": "slides/w08-panel-regression.html#cross-sectional-data-and-lsdv-2",
    "title": "Panel Regression Analysis",
    "section": "Cross-sectional data and LSDV (2)",
    "text": "Cross-sectional data and LSDV (2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nY\nX1\nX2\n\\(\\delta_1\\)\n\\(\\delta_2\\)\n\\(\\delta_3\\)\n\\(\\delta_N\\)\n\n\n\n\n\\(1\\)\n\\(y_{1}\\)\n\\(x^{1}_{1}\\)\n\\(x^{2}_{1}\\)\n1\n0\n0\n0\n\n\n\\(2\\)\n\\(y_{2}\\)\n\\(x^{1}_{2}\\)\n\\(x^{2}_{2}\\)\n0\n1\n0\n0\n\n\n\\(3\\)\n\\(y_{3}\\)\n\\(x^{1}_{3}\\)\n\\(x^{2}_{3}\\)\n0\n0\n1\n0\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(N\\)\n\\(y_{N}\\)\n\\(x^{1}_{N}\\)\n\\(x^{1}_{N}\\)\n0\n0\n0\n1"
  },
  {
    "objectID": "slides/w08-panel-regression.html#panel-ata-and-lsdv",
    "href": "slides/w08-panel-regression.html#panel-ata-and-lsdv",
    "title": "Panel Regression Analysis",
    "section": "Panel ata and LSDV",
    "text": "Panel ata and LSDV\nLSDV model works with the panel data, but…\n\nit is inefficient! Any ideas why?…\n\nNumber of dummy variables is equal to the number of individuals + control variables.\n\nIf we have 5,000 individuals, we have 5,000+ regression coefficients.\nWhat if we have 100,000 individuals?\n\nHaving too many regressors remains unbiased, but complicates inference:\n\nnumber of degrees of freedom increases;\nadjusted \\(R^2\\) may shrink to zero;"
  },
  {
    "objectID": "slides/w08-panel-regression.html#fixed-effect-model-within-transformation",
    "href": "slides/w08-panel-regression.html#fixed-effect-model-within-transformation",
    "title": "Panel Regression Analysis",
    "section": "Fixed Effect Model: Within transformation",
    "text": "Fixed Effect Model: Within transformation\nWithin-transformation subtracts group means from each observation and run the regression.\n\n\\[\n\\begin{aligned}\nlog(\\text{Wage}_{it} - \\overline{\\text{Wage}_{i}}) & =  \\beta_0 \\\\\n& + \\beta_1 \\cdot (\\text{Union}_{it} -  \\overline{\\text{Union}_{i}}) \\\\\n& + \\beta_2 \\cdot (X_{it} - \\overline{\\text{X}_{i}}) \\\\\n& +  \\beta_3 \\cdot (\\text{Ability}_{i} - \\overline{\\text{Ability}_{i}}) \\\\\n& +  \\color{Red}{\\alpha_{i}} \\\\\n& +  (\\epsilon_{it} - \\overline{\\epsilon_{i}})\n\\end{aligned}\n\\]\n\nNote that time-invariant effects disappear.\n\nAny examples and ideas why?\n\\(\\text{Ability}_{i} - \\overline{\\text{Ability}_{i}} = 0\\).\nGender, race, individual characteristics …\nFE (Within) estimates are identical to the LSDV, but SE are more efficient;"
  },
  {
    "objectID": "slides/w08-panel-regression.html#fixed-effect-model-first-difference-transformation",
    "href": "slides/w08-panel-regression.html#fixed-effect-model-first-difference-transformation",
    "title": "Panel Regression Analysis",
    "section": "Fixed Effect Model: First Difference transformation",
    "text": "Fixed Effect Model: First Difference transformation\n\\[\n\\begin{aligned}\nlog(\\text{Wage}_{it} - {\\text{Wage}_{i,t-1}}) & = \\beta_0 \\\\\n& + \\beta_1 \\cdot (\\text{Union}_{it} -  {\\text{Union}_{it-1}}) \\\\\n& + \\beta_2 \\cdot (X_{it} - {{X}_{i,t-1}}) \\\\\n& + \\beta_3 \\cdot (\\text{Ability}_{i} - {\\text{Ability}_{i,t-1}}) \\\\\n& + (\\epsilon_{it} - {\\epsilon_{i,t-1}})\n\\end{aligned}\n\\]\nNote:\n\nSimilar to the within transformation, but sacrifices at least one time dimension.\nRelaxes the autocorrelation assumption.\nMay be not possible with unbalanced data."
  },
  {
    "objectID": "slides/w08-panel-regression.html#fixed-effect-model-assumptions-1",
    "href": "slides/w08-panel-regression.html#fixed-effect-model-assumptions-1",
    "title": "Panel Regression Analysis",
    "section": "Fixed Effect Model: assumptions (1)",
    "text": "Fixed Effect Model: assumptions (1)\n\nNOT ZERO CORRELATION between effects and regressors:\n\n\\(Cov(\\delta_{i},{X}_{it}) \\neq 0\\)\n\nStrict exogeneity (No endogeneity):\n\n\\(E[\\epsilon_{is}| {X}_{it}, \\delta_{i}] = 0\\)\n\\(Cov(\\epsilon_{is}, {X}_{it}) = 0\\) and\n\\(Cov(\\epsilon_{it}, {X}_{jt}) = 0\\) , where \\(j\\neq i\\) and \\(s\\neq t\\) ;\nResiduals (\\(\\epsilon\\)) do not correlate with all explanatory variable (\\(X\\)) in all time periods (\\(t\\)) and for all individuals (\\(i\\)).\n\nNo autocorrelation/serial correlation: \\(Cov(\\epsilon_{it}, {X}_{i,t-1}) = 0\\);\nNo cross-sectional dependence: \\(Cov(\\epsilon_{it}, {X}_{j,t}) = 0\\) (when individual observations react similarly to the common shocks or correlate in space);"
  },
  {
    "objectID": "slides/w08-panel-regression.html#panel-data-and-lsdv",
    "href": "slides/w08-panel-regression.html#panel-data-and-lsdv",
    "title": "Panel Regression Analysis",
    "section": "Panel data and LSDV",
    "text": "Panel data and LSDV\nLSDV model works with the panel data, but…\n\nit is inefficient! Any ideas why?…\n\nNumber of dummy variables is equal to the number of individuals + control variables.\n\nIf we have 5,000 individuals, we have 5,000+ regression coefficients.\nWhat if we have 100,000 individuals?\n\nHaving too many regressors remains unbiased, but complicates inference:\n\nnumber of degrees of freedom increases;\nadjusted \\(R^2\\) may shrink to zero;"
  },
  {
    "objectID": "slides/w08-panel-regression.html#fixed-effect-model-notations",
    "href": "slides/w08-panel-regression.html#fixed-effect-model-notations",
    "title": "Panel Regression Analysis",
    "section": "Fixed Effect Model (notations)",
    "text": "Fixed Effect Model (notations)\nIndividual Fixed effect model:\n\\[\ny_{it} =  \\beta_1 \\cdot x_{it} + \\color{Red}{ \\alpha_i } + \\epsilon_{it}\n\\]\n\n\\(\\color{Red}{ \\alpha_i }\\) is the individual-specific fixed effect;\nusually wihtout the intercept \\(\\beta_0\\);\n\n\nTwo-ways fixed effect model (individual + time effect):\n\\[\ny_{it} =  \\beta_1 \\cdot x_{it} + \\color{Red}{ \\alpha_i } + \\color{Blue}{ \\eta_t } + \\epsilon_{it}\n\\]\nTime Fixed Effect model:\n\\[\ny_{it} =  \\beta_1 \\cdot x_{it} + \\color{Blue}{ \\eta_t } + \\epsilon_{it}\n\\]"
  },
  {
    "objectID": "slides/w08-panel-regression.html#limitations-of-the-fixed-and-random-effect-models",
    "href": "slides/w08-panel-regression.html#limitations-of-the-fixed-and-random-effect-models",
    "title": "Panel Regression Analysis",
    "section": "Limitations of the Fixed and Random effect models",
    "text": "Limitations of the Fixed and Random effect models\n\nNOT the ultimate solution to Endogeneity.\nThere might still be some OVB even with the fixed effects.\n\nInstrumental Variables are possible within the panel regression context too.\n\nMeasurement error may cause endogeneity;"
  },
  {
    "objectID": "slides/w08-panel-regression.html#summary-on-the-panel-regression",
    "href": "slides/w08-panel-regression.html#summary-on-the-panel-regression",
    "title": "Panel Regression Analysis",
    "section": "Summary on the Panel Regression",
    "text": "Summary on the Panel Regression\n\n\nFixed Effect (within transformation)\n\nAssumes that Fixed Effects correlate with regressors!\nPartially resolved the OVB.\nIgnoring FE (using pooled regression) causes bias of estimates.\n\n\nRandom Effect\n\nAssumes that Random Effects do NOT correlate with regressors\nDo NOT resolved any OVB.\nProvides additional control strategy, but ignoring RE causes NO bias.\n\n\n\n\nBoth require valid Gauss–Markov assumptions.\n\n\nLimitations of the Fixed and Random effect models\n\nNOT the ultimate solution to Endogeneity.\nOVB may still remain after applying the fixed effects.\nMeasurement error is a problem in panel data."
  },
  {
    "objectID": "slides/w08-panel-regression.html#random-effect-model-individual-and-two-ways",
    "href": "slides/w08-panel-regression.html#random-effect-model-individual-and-two-ways",
    "title": "Panel Regression Analysis",
    "section": "Random Effect Model (individual and two-ways)",
    "text": "Random Effect Model (individual and two-ways)\n\nIntroduce random components \\(\\color{Red}{v_i}\\) and/or \\(\\color{Blue}{u_{t}}\\)\n\n\n\\[\ny_{it} = \\beta_0 + \\beta_1 \\cdot x_{it}  + \\color{Red}{v_{i}} + \\color{Blue}{u_{t}}  + \\epsilon_{it}\n\\]\n\nDifference from the fixed effect model:\n\nAssumes NO CORRELATION (ZERO CORRELATION) between effects and regressors:\n\n\\(Cov(v_{i},{X}_{it}) = 0\\)\n\nIgnoring RE causes no bias to the estimates;"
  },
  {
    "objectID": "slides/w08-panel-regression.html#general-algorithm",
    "href": "slides/w08-panel-regression.html#general-algorithm",
    "title": "Panel Regression Analysis",
    "section": "General algorithm",
    "text": "General algorithm\n\nPooled OLS\n\nChoose an appropriate functional form (log/level);\nValidate gauss-Markov assumption validation: Linearity, Collinearity, Random Sampling; Homoscedasticity;\nNote on the ‘No endogeneity’ assumption (if not validated, shows importance of the FE model)\n\nFE: Fixed Effect. Within-transformation. Individual, Time or Two-ways effects;\n\nF-test on FE consistency against pooled.\nLM test on FE Individual, Time or Two-ways effects consistency against each other.\nIf tests suggest the pooled model, but the theory emphasizes FE, discus and reason your choice.\n\nRE: Random Effect;\n\nHausman test on effects’ correlation with regressors of RE consistency against the FE;\nSimilar Chamberlain test, Angrist and Newey tests.\n\nSerial correlation and cross-sectional dependence tests;\n\nWooldridge's, Locally–Robust LM Test, Breusch–Godfrey Test,\nt > 3, we may have a serial correlation problem. Check it with a test.\nCould individuals be affected by common shocks? We might have a cross-sectional dependence problem.\n\nUse robust standard errors to correct for serial correlation and/or cross-sectional dependence:\n\nClustered SE and/or heteroscedasticity and/or autocorrelation robust SE;\n\nSummary and interpretation;"
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-1.a-pooled-ols",
    "href": "slides/w08-panel-regression.html#step-1.a-pooled-ols",
    "title": "Panel Regression Analysis",
    "section": "Step 1.a Pooled OLS",
    "text": "Step 1.a Pooled OLS\n\nlibrary(tidyverse)\nlibrary(modelsummary)\nlibrary(parameters)\nlibrary(performance)\nlibrary(lmtest)\nwage_dta <- read_csv(\"wage_unon_panel.csv\")\nglimpse(wage_dta)\n\n\nunion_fit_0 <- lm(log(wage) ~ union + ed + exp + wks, data = wage_dta)\nunion_fit_0\n\n\n\n\nCall:\nlm(formula = log(wage) ~ union + ed + exp + wks, data = wage_dta)\n\nCoefficients:\n(Intercept)     unionyes           ed          exp          wks  \n   4.904952     0.133921     0.082861     0.013193     0.008467"
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-1.b-assumptions-linearity-homoschedasticity",
    "href": "slides/w08-panel-regression.html#step-1.b-assumptions-linearity-homoschedasticity",
    "title": "Panel Regression Analysis",
    "section": "Step 1.b Assumptions (Linearity + Homoschedasticity)",
    "text": "Step 1.b Assumptions (Linearity + Homoschedasticity)\n\ncheck_model(union_fit_0, check = c(\"linearity\", \"homogeneity\"))"
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-1.b-assumptions-homoschedasticity",
    "href": "slides/w08-panel-regression.html#step-1.b-assumptions-homoschedasticity",
    "title": "Panel Regression Analysis",
    "section": "Step 1.b Assumptions (Homoschedasticity)",
    "text": "Step 1.b Assumptions (Homoschedasticity)\n\ncheck_heteroscedasticity(union_fit_0) \n\n\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p = 0.004).\n\n\n\n\nbptest(union_fit_0)\n\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  union_fit_0\nBP = 106.47, df = 4, p-value < 2.2e-16"
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-1.b-assumptions-collinearity",
    "href": "slides/w08-panel-regression.html#step-1.b-assumptions-collinearity",
    "title": "Panel Regression Analysis",
    "section": "Step 1.b Assumptions (Collinearity)",
    "text": "Step 1.b Assumptions (Collinearity)\n\ncheck_collinearity(union_fit_0)\n\n\n\n# Check for Multicollinearity\n\nLow Correlation\n\n  Term  VIF   VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n union 1.11 [1.08, 1.15]         1.05      0.90     [0.87, 0.93]\n    ed 1.13 [1.10, 1.18]         1.06      0.88     [0.85, 0.91]\n   exp 1.05 [1.03, 1.10]         1.03      0.95     [0.91, 0.97]\n   wks 1.03 [1.01, 1.09]         1.01      0.97     [0.92, 0.99]"
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-1.b-assumptions-no-endogeneity",
    "href": "slides/w08-panel-regression.html#step-1.b-assumptions-no-endogeneity",
    "title": "Panel Regression Analysis",
    "section": "Step 1.b Assumptions (No endogeneity)",
    "text": "Step 1.b Assumptions (No endogeneity)\n\\[\nlog(\\text{Wage}_{it}) = \\beta_0 + \\beta_1 \\cdot \\text{Union}_{it} + \\beta_2 \\cdot {X_{it}} + \\beta_3 \\cdot \\text{Ability}_{i} + \\epsilon_{it}\n\\]\n\\(\\text{Ability}_{i}\\) not observable and not measurable.\nOmitting the ability may cause the OVB.\n\nNo endogeneity assumption cannot be satisfied.\nWe should exploit the panel data structure."
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-2.-fe-fixed-effect-within",
    "href": "slides/w08-panel-regression.html#step-2.-fe-fixed-effect-within",
    "title": "Panel Regression Analysis",
    "section": "Step 2. FE: Fixed Effect (within)",
    "text": "Step 2. FE: Fixed Effect (within)\nNote, the new package: plm used for running panel regressions.\n\nlibrary(plm)\n\nDeclare data to be panel.\n\nwage_dta_pan <- pdata.frame(wage_dta, index = c(\"id\", \"time\"))\n\n\n\n\nCheck panel dimensions.\n\npdim(wage_dta_pan)\n\n\n\nBalanced Panel: n = 595, T = 7, N = 4165"
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-2.-fe-fixed-effect-within-1",
    "href": "slides/w08-panel-regression.html#step-2.-fe-fixed-effect-within-1",
    "title": "Panel Regression Analysis",
    "section": "Step 2. FE: Fixed Effect (within) (1)",
    "text": "Step 2. FE: Fixed Effect (within) (1)\nRerun the pooled regression with plm:\n\nunion_pooled <- plm(log(wage) ~ union + ed + exp + wks,\n                    data = wage_dta, model = \"pooling\")\nunion_pooled\n\n\n\n\nModel Formula: log(wage) ~ union + ed + exp + wks\n\nCoefficients:\n(Intercept)    unionyes          ed         exp         wks \n  4.9049523   0.1339212   0.0828611   0.0131927   0.0084667 \n\n\n\nFixed Effect (individual) model\n\nunion_fe_ind <- plm(log(wage) ~ union + ed + exp + wks,\n                    data = wage_dta, model = \"within\", effect = \"individual\")\nunion_fe_ind\n\n\n\n\nModel Formula: log(wage) ~ union + ed + exp + wks\n\nCoefficients:\n unionyes       exp       wks \n0.0310064 0.0968921 0.0011035"
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-2.-fe-fixed-effect-within-2",
    "href": "slides/w08-panel-regression.html#step-2.-fe-fixed-effect-within-2",
    "title": "Panel Regression Analysis",
    "section": "Step 2. FE: Fixed Effect (within) (2)",
    "text": "Step 2. FE: Fixed Effect (within) (2)\nFixed Effect (time) model\n\nunion_fe_time <- plm(log(wage) ~ union + ed + exp + wks,\n                     data = wage_dta, model = \"within\", effect = \"time\")\nunion_fe_time\n\n\n\n\nModel Formula: log(wage) ~ union + ed + exp + wks\n\nCoefficients:\n unionyes        ed       exp       wks \n0.1306555 0.0801044 0.0101620 0.0081754 \n\n\n\nFixed Effect (Two-ways) model\n\nunion_fe_twoways <- plm(log(wage) ~ union + ed + exp + wks,\n                        data = wage_dta, model = \"within\", effect = \"twoways\")\nunion_fe_twoways\n\n\n\n\nModel Formula: log(wage) ~ union + ed + exp + wks\n\nCoefficients:\n  unionyes        wks \n0.02793858 0.00091316"
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-2.-f-test-statistical-tests-1",
    "href": "slides/w08-panel-regression.html#step-2.-f-test-statistical-tests-1",
    "title": "Panel Regression Analysis",
    "section": "Step 2. F-test statistical tests (1)",
    "text": "Step 2. F-test statistical tests (1)\n\n\n\nCompares FE models (individual, time, two-ways) vs pooled\n\nPooled is always consistent vs FE\n\nTest logic:\n\nH0: One model is inconsistent.\nH1: Both models are equally consistent.\n\n\n\n\nRun the test. Check the p-value\n\np-value < 0.05: Pooled is better then the FE model. Use pooled for interpretation.\np-value >= 0.05: FE is as good as pooled. Not using FE model may cause bias."
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-2.-f-test-statistical-tests-2",
    "href": "slides/w08-panel-regression.html#step-2.-f-test-statistical-tests-2",
    "title": "Panel Regression Analysis",
    "section": "Step 2. F-test statistical tests (2)",
    "text": "Step 2. F-test statistical tests (2)\n\n\n\npFtest(union_fe_ind, union_pooled)\n\n\n    F test for individual effects\n\ndata:  log(wage) ~ union + ed + exp + wks\nF = 40.418, df1 = 593, df2 = 3567, p-value < 2.2e-16\nalternative hypothesis: significant effects\n\n\n\n\nPooled regression is preferred (no individual effect FE is needed)\n\n\n\n\n\n\n\npFtest(union_fe_twoways, union_pooled)\n\n\n    F test for twoways effects\n\ndata:  log(wage) ~ union + ed + exp + wks\nF = 40.517, df1 = 598, df2 = 3562, p-value < 2.2e-16\nalternative hypothesis: significant effects\n\n\n\n\nPooled regression is preferred (no two-ways FE is needed)\n\n\n\n\n\n\n\n\npFtest(union_fe_time, union_pooled)\n\n\n    F test for time effects\n\ndata:  log(wage) ~ union + ed + exp + wks\nF = 159.46, df1 = 6, df2 = 4154, p-value < 2.2e-16\nalternative hypothesis: significant effects\n\n\n\n\nPooled regression is preferred (no time FE is needed)"
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-2.-lm-test-statistical-tests-1",
    "href": "slides/w08-panel-regression.html#step-2.-lm-test-statistical-tests-1",
    "title": "Panel Regression Analysis",
    "section": "Step 2. LM test statistical tests (1)",
    "text": "Step 2. LM test statistical tests (1)\n\n\n\nplmtest(union_pooled, effect = \"individual\")\n\n\n    Lagrange Multiplier Test - (Honda)\n\ndata:  log(wage) ~ union + ed + exp + wks\nnormal = 70.501, p-value < 2.2e-16\nalternative hypothesis: significant effects\n\nplmtest(union_pooled, effect = \"twoway\")\n\n\n    Lagrange Multiplier Test - two-ways effects (Honda)\n\ndata:  log(wage) ~ union + ed + exp + wks\nnormal = 190.86, p-value < 2.2e-16\nalternative hypothesis: significant effects\n\nplmtest(union_pooled, effect = \"time\")\n\n\n    Lagrange Multiplier Test - time effects (Honda)\n\ndata:  log(wage) ~ union + ed + exp + wks\nnormal = 199.42, p-value < 2.2e-16\nalternative hypothesis: significant effects\n\n\n\n\nPooled regression is preferred (no individual effect FE is needed)\nIndividual fixed effect model is preferred (no two-ways FE is needed)\nIndividual fixed effect model is preferred (no time FE is needed)"
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-2.-lm-test-statistical-tests-1-1",
    "href": "slides/w08-panel-regression.html#step-2.-lm-test-statistical-tests-1-1",
    "title": "Panel Regression Analysis",
    "section": "Step 2. LM test statistical tests (1)",
    "text": "Step 2. LM test statistical tests (1)"
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-2.-lm-test-statistical-tests-2",
    "href": "slides/w08-panel-regression.html#step-2.-lm-test-statistical-tests-2",
    "title": "Panel Regression Analysis",
    "section": "Step 2. LM test statistical tests (2)",
    "text": "Step 2. LM test statistical tests (2)\n\n\n\nExist to compare FE models between each other assuming that:\n\nPooled is always consistent in pooled vs individual FE\nIndividual FE always consistent in individual FE vs time or two-ways FE\n\nTest logic:\n\nH0: One model is inconsistent.\nH1: Both models are equally consistent.\n\n\n\n\nRun the test (one or another or both). Check p-value:\n\np-value < 0.05: Pooled or individual FE is better then the alternative\np-value >= 0.05:\n\nIndividual FE is as good as pooled;\nTime or Two-ways model is as good as individual FE;"
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-2.-lm-test-statistical-tests-2-1",
    "href": "slides/w08-panel-regression.html#step-2.-lm-test-statistical-tests-2-1",
    "title": "Panel Regression Analysis",
    "section": "Step 2. LM test statistical tests (2)",
    "text": "Step 2. LM test statistical tests (2)\n\n\n\nplmtest(union_pooled, effect = \"individual\")\n\n\n    Lagrange Multiplier Test - (Honda)\n\ndata:  log(wage) ~ union + ed + exp + wks\nnormal = 70.501, p-value < 2.2e-16\nalternative hypothesis: significant effects\n\nplmtest(union_pooled, effect = \"twoway\")\n\n\n    Lagrange Multiplier Test - two-ways effects (Honda)\n\ndata:  log(wage) ~ union + ed + exp + wks\nnormal = 190.86, p-value < 2.2e-16\nalternative hypothesis: significant effects\n\nplmtest(union_pooled, effect = \"time\")\n\n\n    Lagrange Multiplier Test - time effects (Honda)\n\ndata:  log(wage) ~ union + ed + exp + wks\nnormal = 199.42, p-value < 2.2e-16\nalternative hypothesis: significant effects\n\n\n\n\nPooled regression is preferred (no individual FE is needed)\nIndividual FE is preferred (no two-ways FE is needed)\nIndividual FE is preferred (no time FE is needed)"
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-3.-random-effect-model-individual",
    "href": "slides/w08-panel-regression.html#step-3.-random-effect-model-individual",
    "title": "Panel Regression Analysis",
    "section": "Step 3. Random Effect model (individual)",
    "text": "Step 3. Random Effect model (individual)\n\nunion_rand_ind <- plm(log(wage) ~ union + ed + exp + wks,\n                    data = wage_dta, model = \"random\", effect = \"individual\")\nunion_rand_ind\n\n\n\n\nModel Formula: log(wage) ~ union + ed + exp + wks\n\nCoefficients:\n(Intercept)    unionyes          ed         exp         wks \n  3.9804910   0.0580121   0.1160369   0.0560612   0.0015205"
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-3.-hausma-test",
    "href": "slides/w08-panel-regression.html#step-3.-hausma-test",
    "title": "Panel Regression Analysis",
    "section": "Step 3. Hausma test",
    "text": "Step 3. Hausma test\n\n\n\nCompares Fixed Effect model with Random Effect:\n\nFixed effect model is always consistent\n\nTest logic:\n\nH0: One model is inconsistent. Use FE!\nH1: Both models are equally consistent. RE is as good as FE.\n\n\n\n\nRun the test Check the p-value.\n\np-value < 0.05: Use FE, discard RE.\np-value >= 0.05: Use FE or RE, both are good.\n\n\n\n\n\n\n\n\nphtest(union_fe_ind, union_rand_ind)\n\n\n    Hausman Test\n\ndata:  log(wage) ~ union + ed + exp + wks\nchisq = 8018.2, df = 3, p-value < 2.2e-16\nalternative hypothesis: one model is inconsistent\n\n\n\n\nFE is preferred instead of the RE model."
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-1.b-assumptions-linearity-homoscedasticity",
    "href": "slides/w08-panel-regression.html#step-1.b-assumptions-linearity-homoscedasticity",
    "title": "Panel Regression Analysis",
    "section": "Step 1.b Assumptions (Linearity + Homoscedasticity)",
    "text": "Step 1.b Assumptions (Linearity + Homoscedasticity)\n\ncheck_model(union_fit_0, check = c(\"linearity\", \"homogeneity\"))"
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-1.b-assumptions-homoscedasticity",
    "href": "slides/w08-panel-regression.html#step-1.b-assumptions-homoscedasticity",
    "title": "Panel Regression Analysis",
    "section": "Step 1.b Assumptions (Homoscedasticity)",
    "text": "Step 1.b Assumptions (Homoscedasticity)\n\ncheck_heteroscedasticity(union_fit_0) \n\n\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p = 0.004).\n\n\n\n\nbptest(union_fit_0)\n\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  union_fit_0\nBP = 106.47, df = 4, p-value < 2.2e-16"
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-3.-hausman-test",
    "href": "slides/w08-panel-regression.html#step-3.-hausman-test",
    "title": "Panel Regression Analysis",
    "section": "Step 3. Hausman test",
    "text": "Step 3. Hausman test\nWhich model to choose: Fixed effect or Random effect?\n\n\n\nCompares Fixed Effect model with Random Effect:\n\nFixed effect model is always consistent\n\nTest logic:\n\nH0: One model is inconsistent. Use FE!\nH1: Both models are equally consistent. RE is as good as FE.\n\n\n\n\nRun the test Check the p-value.\n\np-value < 0.05: Use FE, discard RE.\np-value >= 0.05: Use FE or RE, both are good.\n\n\n\n\n\n\n\n\nphtest(union_fe_ind, union_rand_ind)\n\n\n    Hausman Test\n\ndata:  log(wage) ~ union + ed + exp + wks\nchisq = 8018.2, df = 3, p-value < 2.2e-16\nalternative hypothesis: one model is inconsistent\n\n\n\n\nFE is preferred instead of the RE model."
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-4.1-serial-correlation-and-cross-sectional-dependence",
    "href": "slides/w08-panel-regression.html#step-4.1-serial-correlation-and-cross-sectional-dependence",
    "title": "Panel Regression Analysis",
    "section": "Step 4.1 Serial correlation and cross-sectional dependence",
    "text": "Step 4.1 Serial correlation and cross-sectional dependence\n\n\n\nWooldridge’s test for unobserved individual effects\n\nH0: no unobserved effects\nH1: some effects exist due to cross-sectional dependence and/or serial correlation\n\n\n\n\nRun the test Check the p-value.\n\np-value < 0.05: cross-sectional dependence and/or serial correlation are present\np-value >= 0.05: No cross-sectional dependency and/or serial correlation\n\n\n\n\n\n\n\n\npwtest(union_pooled, effect = \"individual\")\n\n\n    Wooldridge's test for unobserved individual effects\n\ndata:  formula\nz = 13.064, p-value < 2.2e-16\nalternative hypothesis: unobserved effect\n\npwtest(union_pooled, effect = \"time\")\n\n\n    Wooldridge's test for unobserved time effects\n\ndata:  formula\nz = 2.0133, p-value = 0.04408\nalternative hypothesis: unobserved effect\n\n\n\n\ncross-sectional dependence is present\nserial correlation is present"
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-4.2-lagrange-multiplier-tests-for-random-effects-andor-serial-correlation",
    "href": "slides/w08-panel-regression.html#step-4.2-lagrange-multiplier-tests-for-random-effects-andor-serial-correlation",
    "title": "Panel Regression Analysis",
    "section": "Step 4.2 Lagrange-Multiplier tests for random effects and/or serial correlation",
    "text": "Step 4.2 Lagrange-Multiplier tests for random effects and/or serial correlation\n\nH0: serial correlation is zero\nH1: some serial correlation\n\n\n\n\n\nLocally–Robust Lagrange Multiplier Tests for serial correlation\n\nH0: serial correlation is zero\nH1: some serial correlation is present\n\n\n\n\nRun the test Check the p-value.\n\np-value < 0.05: serial correlation need to be addressed\np-value >= 0.05: no serial correlation\n\n\n\n\n\n\n\n\n\npbsytest(union_pooled, test = \"ar\")\n\n\n    Bera, Sosa-Escudero and Yoon locally robust test\n\ndata:  formula\nchisq = 627.99, df = 1, p-value < 2.2e-16\nalternative hypothesis: AR(1) errors sub random effects\n\n\n\n\nserial correlation is present"
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-5.-correcting-for-serial-correlation-and-cross-sectional-dependence",
    "href": "slides/w08-panel-regression.html#step-5.-correcting-for-serial-correlation-and-cross-sectional-dependence",
    "title": "Panel Regression Analysis",
    "section": "Step 5. Correcting for serial correlation and cross-sectional dependence",
    "text": "Step 5. Correcting for serial correlation and cross-sectional dependence\nCross-sectional dependence and/or serial correlation violate the variance homogeneity assumption:\n\nEstimates are unbiased, but inefficient.\nStandard errors need to be corrected.\n\n\nWe can use:\n\nRobust SE\nClustered SE at the individual (group) level"
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-5.-correcting-cross-sectional-dependence",
    "href": "slides/w08-panel-regression.html#step-5.-correcting-cross-sectional-dependence",
    "title": "Panel Regression Analysis",
    "section": "Step 5. Correcting cross-sectional dependence",
    "text": "Step 5. Correcting cross-sectional dependence\n\nlibrary(lmtest)\nlibrary(car)\nlibrary(sandwich)\noptions(digits = 3, scipen = 6)\nunion_fe_ind\n\n\nModel Formula: log(wage) ~ union + ed + exp + wks\n\nCoefficients:\nunionyes      exp      wks \n  0.0310   0.0969   0.0011 \n\n\n\n\nRegular SE\n\ncoeftest(union_fe_ind, \n         vcov. = vcov(union_fe_ind))\n\n\nt test of coefficients:\n\n         Estimate Std. Error t value Pr(>|t|)    \nunionyes 0.031006   0.014929    2.08    0.038 *  \nexp      0.096892   0.001189   81.52   <2e-16 ***\nwks      0.001104   0.000603    1.83    0.067 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nNo cross–sectional dependence-robust\n\ncoeftest(union_fe_ind, \n         vcov. = vcovHC(union_fe_ind, method = \"white1\", \n                        type = \"HC0\", cluster = \"group\"))\n\n\nt test of coefficients:\n\n         Estimate Std. Error t value Pr(>|t|)    \nunionyes 0.031006   0.016137    1.92    0.055 .  \nexp      0.096892   0.001145   84.59   <2e-16 ***\nwks      0.001104   0.000753    1.46    0.143    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nmethods for cross–sectional “white1” and “white2” and for c-s + autocorrelation “arellano”;\ntype for sample size correction: “HC0”, “sss”, “HC1”, “HC2”, “HC3”, “HC4” (“HC3” is recommended);\ncluster enabled by default (“group” or “time”);"
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-6.-reporting-results-1",
    "href": "slides/w08-panel-regression.html#step-6.-reporting-results-1",
    "title": "Panel Regression Analysis",
    "section": "Step 6. Reporting results (1)",
    "text": "Step 6. Reporting results (1)\n\npooled_robust <- \n  coeftest(union_pooled,\n           vcov. = vcovHC(union_pooled, method = \"arellano\",\n                          type = \"HC3\", cluster = \"group\"))\n\npooled_cs_robust <- \n  coeftest(union_fe_ind,\n           vcov. = vcovHC(union_fe_ind, method = \"white1\", \n                          type = \"HC0\", cluster = \"group\"))\n\npooled_csac_robust <- \n  coeftest(union_fe_ind,\n           vcov. = vcovHC(union_fe_ind, method = \"arellano\", \n                          type = \"HC3\", cluster = \"group\"))\n\n\nmodelsummary(\n  list(\n    `Pooled (no SE correction)` = coeftest(union_pooled),\n    `Pooled (c/s dep. and aut.)` = pooled_robust,\n    `Ind. FE (no SE correction)` = coeftest(union_fe_ind),\n    `Ind. FE (c/s dep.)` = pooled_cs_robust, \n    `Ind. FE (c/s dep. and aut.)` = pooled_csac_robust\n    ), \n  fmt = 4, statistic = NULL,\n  estimate = \"{estimate}{stars} ({std.error})\")"
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-6.-reporting-results-1-output",
    "href": "slides/w08-panel-regression.html#step-6.-reporting-results-1-output",
    "title": "Panel Regression Analysis",
    "section": "Step 6. Reporting results (1)",
    "text": "Step 6. Reporting results (1)\n\n\n\n \n  \n      \n    Pooled (no SE correction) \n    Pooled (c/s dep. and aut.) \n     Ind. FE (no SE correction) \n     Ind. FE (c/s dep.) \n     Ind. FE (c/s dep. and aut.) \n  \n \n\n  \n    (Intercept) \n    4.9050*** (0.0696) \n    4.9050*** (0.1364) \n     \n     \n     \n  \n  \n    unionyes \n    0.1339*** (0.0134) \n    0.1339*** (0.0262) \n    0.0310* (0.0149) \n    0.0310+ (0.0161) \n    0.0310 (0.0262) \n  \n  \n    ed \n    0.0829*** (0.0023) \n    0.0829*** (0.0052) \n     \n     \n     \n  \n  \n    exp \n    0.0132*** (0.0006) \n    0.0132*** (0.0014) \n    0.0969*** (0.0012) \n    0.0969*** (0.0011) \n    0.0969*** (0.0018) \n  \n  \n    wks \n    0.0085*** (0.0012) \n    0.0085*** (0.0020) \n    0.0011+ (0.0006) \n    0.0011 (0.0008) \n    0.0011 (0.0009) \n  \n  \n    Num.Obs. \n    4165 \n    4165 \n    4165 \n    4165 \n    4165 \n  \n  \n    AIC \n    4081.6 \n    4081.6 \n    −4434.5 \n    −4434.5 \n    −4434.5 \n  \n  \n    BIC \n    4119.6 \n    4119.6 \n    −4409.2 \n    −4409.2 \n    −4409.2 \n  \n  \n    Log.Lik. \n    -2034.793 \n    -2034.793 \n    2221.250 \n    2221.250 \n    2221.250"
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-6.-reporting-gof-1",
    "href": "slides/w08-panel-regression.html#step-6.-reporting-gof-1",
    "title": "Panel Regression Analysis",
    "section": "Step 6. Reporting GOF (1)",
    "text": "Step 6. Reporting GOF (1)\n\nlibrary(performance)\ncompare_performance(list(Pooled = union_pooled, FE = union_fe_ind))\n\n\n\n# Comparison of Model Performance Indices\n\nName   | Model |   AIC (weights) |  AICc (weights) |   BIC (weights) |    R2 | R2 (adj.) |  RMSE | Sigma\n--------------------------------------------------------------------------------------------------------\nPooled |   plm | 59695.6 (<.001) | 59695.6 (<.001) | 59733.6 (<.001) | 0.270 |     0.269 | 0.394 | 0.395\nFE     |   plm | 51179.5 (>.999) | 51179.5 (>.999) | 51204.8 (>.999) | 0.651 |     0.593 | 0.142 | 0.142"
  },
  {
    "objectID": "slides/w08-panel-regression.html#takeaways-for-the-exam",
    "href": "slides/w08-panel-regression.html#takeaways-for-the-exam",
    "title": "Panel Regression Analysis",
    "section": "Takeaways for the exam",
    "text": "Takeaways for the exam\n\nSimpson’s paradox. What are the causes of it and solutions.\nData types (cross-section, repeated cross-section, balanced panel, unbalanced panel)\nPanel Regression\n\nPooled;\nLeast Squared Dummy Variable model;\nFixed effect (within transformation);\nWhy FE is so important?\nWhat is the key difference between FE and RE?\nWhen FE and when RE are appropriate?\n\nPanel Regression tests F-test, LM-test, Hausman test\nRobust and Clustered SE: why these are important and when do we need to use one?"
  },
  {
    "objectID": "slides/w08-panel-regression.html#homework",
    "href": "slides/w08-panel-regression.html#homework",
    "title": "Panel Regression Analysis",
    "section": "Homework",
    "text": "Homework\n\nReproduce code from the slides\nPerform practical exercises."
  },
  {
    "objectID": "slides/w08-panel-regression.html#fixed-effect-application-literature",
    "href": "slides/w08-panel-regression.html#fixed-effect-application-literature",
    "title": "Panel Regression Analysis",
    "section": "Fixed effect application: literature",
    "text": "Fixed effect application: literature\nSeminal papers: (Mundlak, 1961)\nClimate and agriculture: Bozzola, Massetti, Mendelsohn, & Capitanio (2017)\nChoice of irrigation: Chatzopoulos & Lippert (2015)\nCrop choice: Seo & Mendelsohn (2008b)\nLivestock choice: Seo & Mendelsohn (2008a)\nCross-sectional dependence: (Conley, 1999)"
  },
  {
    "objectID": "slides/w08-panel-regression.html#random-effect-model-individual-time-and-two-ways",
    "href": "slides/w08-panel-regression.html#random-effect-model-individual-time-and-two-ways",
    "title": "Panel Regression Analysis",
    "section": "Random Effect Model (individual, time and two-ways)",
    "text": "Random Effect Model (individual, time and two-ways)\n\nIntroduce random components \\(\\color{Red}{v_i}\\) and/or \\(\\color{Blue}{u_{t}}\\)\n\n\n\\[\ny_{it} = \\beta_0 + \\beta_1 \\cdot x_{it}  + \\color{Red}{v_{i}} + \\color{Blue}{u_{t}}  + \\epsilon_{it}\n\\]\n\nDifference from the fixed effect model:\n\nAssumes NO CORRELATION (ZERO CORRELATION) between effects and regressors:\n\n\\(Cov(v_{i},{X}_{it}) = 0\\)\n\nIgnoring RE causes no bias to the estimates;"
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-2.-f-test-1",
    "href": "slides/w08-panel-regression.html#step-2.-f-test-1",
    "title": "Panel Regression Analysis",
    "section": "Step 2. F-test (1)",
    "text": "Step 2. F-test (1)\nWhich model to choose: Pooled or FE?\n\n\n\nCompares FE models (individual, time, two-ways) vs pooled\n\nPooled is always consistent vs FE\n\nTest logic:\n\nH0: One model is inconsistent.\nH1: Both models are equally consistent.\n\n\n\n\nRun the test. Check the p-value\n\np-value < 0.05: Pooled is better than the FE model. Use pooled for interpretation.\np-value >= 0.05: FE is as good as pooled. Not using the FE model may cause bias."
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-2.-f-test-2",
    "href": "slides/w08-panel-regression.html#step-2.-f-test-2",
    "title": "Panel Regression Analysis",
    "section": "Step 2. F-test (2)",
    "text": "Step 2. F-test (2)\n\n\n\npFtest(union_fe_ind, union_pooled)\n\n\n    F test for individual effects\n\ndata:  log(wage) ~ union + ed + exp + wks\nF = 40.418, df1 = 593, df2 = 3567, p-value < 2.2e-16\nalternative hypothesis: significant effects\n\n\n\n\nPooled regression is preferred (no individual effect FE is needed)\n\n\n\n\n\n\n\npFtest(union_fe_twoways, union_pooled)\n\n\n    F test for twoways effects\n\ndata:  log(wage) ~ union + ed + exp + wks\nF = 40.517, df1 = 598, df2 = 3562, p-value < 2.2e-16\nalternative hypothesis: significant effects\n\n\n\n\nPooled regression is preferred (no two-ways FE is needed)\n\n\n\n\n\n\n\n\npFtest(union_fe_time, union_pooled)\n\n\n    F test for time effects\n\ndata:  log(wage) ~ union + ed + exp + wks\nF = 159.46, df1 = 6, df2 = 4154, p-value < 2.2e-16\nalternative hypothesis: significant effects\n\n\n\n\nPooled regression is preferred (no time FE is needed)\n\n\n\n\nF-test leads us to stick with the pooled regression. We can still argue that not accounting for the ability bias may invoke Simpson’s paradox and biased estimates. Thus, we can still choose the FE model, but this choice has to be well described."
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-2.-lm-test-lagrange-multiplier-test-2",
    "href": "slides/w08-panel-regression.html#step-2.-lm-test-lagrange-multiplier-test-2",
    "title": "Panel Regression Analysis",
    "section": "Step 2. LM test: Lagrange multiplier test (2)",
    "text": "Step 2. LM test: Lagrange multiplier test (2)\nWhich FE model to choose: individual, time or two-way?\n\n\n\nExist to compare FE models between each other assuming that:\n\nPooled is always consistent in pooled vs individual FE\nIndividual FE always consistent in individual FE vs time or two-way FE\n\nTest logic:\n\nH0: One model is inconsistent.\nH1: Both models are equally consistent.\n\n\n\n\nRun the test (one or another or both). Check p-value:\n\np-value < 0.05: Pooled or individual FE is better than the alternative\np-value >= 0.05:\n\nIndividual FE is as good as pooled;\nTime or Two-ways model is as good as individual FE;"
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-2.-lm-testlagrange-multiplier-2",
    "href": "slides/w08-panel-regression.html#step-2.-lm-testlagrange-multiplier-2",
    "title": "Panel Regression Analysis",
    "section": "Step 2. LM testLagrange multiplier (2)",
    "text": "Step 2. LM testLagrange multiplier (2)\n\n\n\nplmtest(union_pooled, effect = \"individual\")\n\n\n    Lagrange Multiplier Test - (Honda)\n\ndata:  log(wage) ~ union + ed + exp + wks\nnormal = 70.501, p-value < 2.2e-16\nalternative hypothesis: significant effects\n\nplmtest(union_pooled, effect = \"twoway\")\n\n\n    Lagrange Multiplier Test - two-ways effects (Honda)\n\ndata:  log(wage) ~ union + ed + exp + wks\nnormal = 190.86, p-value < 2.2e-16\nalternative hypothesis: significant effects\n\nplmtest(union_pooled, effect = \"time\")\n\n\n    Lagrange Multiplier Test - time effects (Honda)\n\ndata:  log(wage) ~ union + ed + exp + wks\nnormal = 199.42, p-value < 2.2e-16\nalternative hypothesis: significant effects\n\n\n\n\nPooled regression is preferred (no individual FE is needed)\nIndividual FE is preferred (no two-ways FE is needed)\nIndividual FE is preferred (no time FE is needed)\n\n\n\n\nAll tests point to the individual fixed effect regression (if not pooled)."
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-4.1-wooldridges-test-1",
    "href": "slides/w08-panel-regression.html#step-4.1-wooldridges-test-1",
    "title": "Panel Regression Analysis",
    "section": "Step 4.1 Wooldridge’s test (1)",
    "text": "Step 4.1 Wooldridge’s test (1)\nIs there serial correlation / cross-sectional dependence in the data?\n\n\n\nWooldridge’s test for unobserved individual effects\n\nH0: no unobserved effects\nH1: some effects exist due to cross-sectional dependence and/or serial correlation\n\n\n\n\nRun the test Check the p-value.\n\np-value < 0.05: cross-sectional dependence and/or serial correlation are present\np-value >= 0.05: No cross-sectional dependency and/or serial correlation"
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-4.1-wooldridges-test-2",
    "href": "slides/w08-panel-regression.html#step-4.1-wooldridges-test-2",
    "title": "Panel Regression Analysis",
    "section": "Step 4.1 Wooldridge’s test (2)",
    "text": "Step 4.1 Wooldridge’s test (2)\n\n\n\npwtest(union_pooled, effect = \"individual\")\n\n\n    Wooldridge's test for unobserved individual effects\n\ndata:  formula\nz = 13.064, p-value < 2.2e-16\nalternative hypothesis: unobserved effect\n\npwtest(union_pooled, effect = \"time\")\n\n\n    Wooldridge's test for unobserved time effects\n\ndata:  formula\nz = 2.0133, p-value = 0.04408\nalternative hypothesis: unobserved effect\n\n\n\n\ncross-sectional dependence is present\nserial correlation is present"
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-4.2-lagrange-multiplier-tests-1",
    "href": "slides/w08-panel-regression.html#step-4.2-lagrange-multiplier-tests-1",
    "title": "Panel Regression Analysis",
    "section": "Step 4.2 Lagrange-Multiplier tests (1)",
    "text": "Step 4.2 Lagrange-Multiplier tests (1)\nIs there serial correlation in the data?\n\n\n\nLocally–Robust Lagrange Multiplier Tests for serial correlation\n\nH0: serial correlation is zero\nH1: some serial correlation is present\n\n\n\n\nRun the test Check the p-value.\n\np-value < 0.05: serial correlation need to be addressed\np-value >= 0.05: no serial correlation"
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-4.2-lagrange-multiplier-tests-2",
    "href": "slides/w08-panel-regression.html#step-4.2-lagrange-multiplier-tests-2",
    "title": "Panel Regression Analysis",
    "section": "Step 4.2 Lagrange-Multiplier tests (2)",
    "text": "Step 4.2 Lagrange-Multiplier tests (2)\n\n\n\npbsytest(union_pooled, test = \"ar\")\n\n\n    Bera, Sosa-Escudero and Yoon locally robust test\n\ndata:  formula\nchisq = 627.99, df = 1, p-value < 2.2e-16\nalternative hypothesis: AR(1) errors sub random effects\n\n\n\n\nserial correlation is present"
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-5.-robust-inference",
    "href": "slides/w08-panel-regression.html#step-5.-robust-inference",
    "title": "Panel Regression Analysis",
    "section": "Step 5. Robust inference",
    "text": "Step 5. Robust inference\nSerial correlation and/or cross-sectional dependence render our Standard errors useless.\n\nCross-sectional dependence and/or serial correlation violate the variance homogeneity assumption:\n\nEstimates are unbiased, but inefficient.\nStandard errors need to be corrected.\n\n\n\nWe need to use:\n\nRobust Standard Errors, and/or\nClustered SE at the individual (group) level"
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-5.-robust-standard-error-1",
    "href": "slides/w08-panel-regression.html#step-5.-robust-standard-error-1",
    "title": "Panel Regression Analysis",
    "section": "Step 5. Robust Standard Error (1)",
    "text": "Step 5. Robust Standard Error (1)\n\nlibrary(lmtest)\nlibrary(car)\nlibrary(sandwich)\noptions(digits = 3, scipen = 6)\nunion_fe_ind\n\n\nModel Formula: log(wage) ~ union + ed + exp + wks\n\nCoefficients:\nunionyes      exp      wks \n  0.0310   0.0969   0.0011 \n\n\nCorrecting cross-sectional dependence:\n\n\nRegular SE\n\ncoeftest(union_fe_ind, \n         vcov. = vcov(union_fe_ind))\n\n\nt test of coefficients:\n\n         Estimate Std. Error t value Pr(>|t|)    \nunionyes 0.031006   0.014929    2.08    0.038 *  \nexp      0.096892   0.001189   81.52   <2e-16 ***\nwks      0.001104   0.000603    1.83    0.067 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nRobust SE\n\ncoeftest(union_fe_ind, \n         vcov. = vcovHC(union_fe_ind, method = \"white1\", \n                        type = \"HC0\", cluster = \"group\"))\n\n\nt test of coefficients:\n\n         Estimate Std. Error t value Pr(>|t|)    \nunionyes 0.031006   0.016137    1.92    0.055 .  \nexp      0.096892   0.001145   84.59   <2e-16 ***\nwks      0.001104   0.000753    1.46    0.143    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-5.-robust-standard-error-2",
    "href": "slides/w08-panel-regression.html#step-5.-robust-standard-error-2",
    "title": "Panel Regression Analysis",
    "section": "Step 5. Robust Standard Error (2)",
    "text": "Step 5. Robust Standard Error (2)\nWe produce new Variance-covariance matrix:\n\nvcovHC(union_fe_ind, \n       method = \"white1\", \n       type = \"HC0\", \n       cluster = \"group\")\n\n             unionyes           exp           wks\nunionyes  0.000260415 -0.0000006698 -0.0000005482\nexp      -0.000000670  0.0000013121  0.0000000824\nwks      -0.000000548  0.0000000824  0.0000005677\nattr(,\"cluster\")\n[1] \"group\"\n\n\n\nmethods for cross–sectional dependence “white1” and “white2” and for cross–sectional dependence and autocorrelation “arellano”;\ntype for sample size correction: “HC0”, “sss”, “HC1”, “HC2”, “HC3”, “HC4” (“HC3” is recommended);\ncluster enabled by default (“group” or “time”);"
  },
  {
    "objectID": "slides/w08-panel-regression.html#step-6.-reporting-results-1-1",
    "href": "slides/w08-panel-regression.html#step-6.-reporting-results-1-1",
    "title": "Panel Regression Analysis",
    "section": "Step 6. Reporting results (1)",
    "text": "Step 6. Reporting results (1)\n\n\n\n\n \n  \n      \n    Pooled (no SE correction) \n    Pooled (c/s dep. and aut.) \n     Ind. FE (no SE correction) \n     Ind. FE (c/s dep.) \n     Ind. FE (c/s dep. and aut.) \n  \n \n\n  \n    (Intercept) \n    4.9050*** (0.0696) \n    4.9050*** (0.1364) \n     \n     \n     \n  \n  \n    unionyes \n    0.1339*** (0.0134) \n    0.1339*** (0.0262) \n    0.0310* (0.0149) \n    0.0310+ (0.0161) \n    0.0310 (0.0262) \n  \n  \n    ed \n    0.0829*** (0.0023) \n    0.0829*** (0.0052) \n     \n     \n     \n  \n  \n    exp \n    0.0132*** (0.0006) \n    0.0132*** (0.0014) \n    0.0969*** (0.0012) \n    0.0969*** (0.0011) \n    0.0969*** (0.0018) \n  \n  \n    wks \n    0.0085*** (0.0012) \n    0.0085*** (0.0020) \n    0.0011+ (0.0006) \n    0.0011 (0.0008) \n    0.0011 (0.0009) \n  \n  \n    Num.Obs. \n    4165 \n    4165 \n    4165 \n    4165 \n    4165 \n  \n  \n    AIC \n    4081.6 \n    4081.6 \n    −4434.5 \n    −4434.5 \n    −4434.5 \n  \n  \n    BIC \n    4119.6 \n    4119.6 \n    −4409.2 \n    −4409.2 \n    −4409.2 \n  \n  \n    Log.Lik. \n    -2034.793 \n    -2034.793 \n    2221.250 \n    2221.250 \n    2221.250"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#simpsons-paradox-with-penguins",
    "href": "slides/w08a-panel-regression.html#simpsons-paradox-with-penguins",
    "title": "Panel Regression Analysis",
    "section": "Simpson’s paradox (with penguins)",
    "text": "Simpson’s paradox (with penguins)"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#let-us-investigate",
    "href": "slides/w08a-panel-regression.html#let-us-investigate",
    "title": "Panel Regression Analysis",
    "section": "Let us investigate…",
    "text": "Let us investigate…\nThe relationship between bill length and depth in penguins…"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#the-data",
    "href": "slides/w08a-panel-regression.html#the-data",
    "title": "Panel Regression Analysis",
    "section": "The data",
    "text": "The data\n\nlibrary(tidyverse)\nlibrary(modelsummary)\npenguins <- read_csv(\"penguins\")\npenguins %>% glimpse()\n\n\n\nRows: 344\nColumns: 8\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    <dbl> 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm <int> 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       <int> 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               <fct> male, female, female, NA, female, male, female, male…\n$ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#the-relationship",
    "href": "slides/w08a-panel-regression.html#the-relationship",
    "title": "Panel Regression Analysis",
    "section": "The relationship",
    "text": "The relationship\n\ngg_bill <- \n  penguins %>% ggplot() + \n  aes(x = bill_length_mm, y = bill_depth_mm) +\n  xlab(\"Bill length, mm\") + ylab(\"Bill depth, mm\") +\n  geom_point()\ngg_bill"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#the-trend",
    "href": "slides/w08a-panel-regression.html#the-trend",
    "title": "Panel Regression Analysis",
    "section": "The trend",
    "text": "The trend\n\ngg_bill + \n  geom_smooth(method = \"lm\", formula = y ~ x, colour = \"black\")"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#is-this-the-true-trend",
    "href": "slides/w08a-panel-regression.html#is-this-the-true-trend",
    "title": "Panel Regression Analysis",
    "section": "Is this the true trend?",
    "text": "Is this the true trend?\n\ngg_bill + \n  geom_smooth(method = \"lm\", formula = y ~ x, colour = \"black\") + \n  aes(colour = species)"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#the-true-trends",
    "href": "slides/w08a-panel-regression.html#the-true-trends",
    "title": "Panel Regression Analysis",
    "section": "The true trends",
    "text": "The true trends\n\ngg_bill + \n  geom_smooth(method = \"lm\", formula = y ~ x, colour = \"black\") + \n  aes(colour = species) + \n  geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE)"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#regression-results",
    "href": "slides/w08a-panel-regression.html#regression-results",
    "title": "Panel Regression Analysis",
    "section": "Regression results",
    "text": "Regression results\n\nfit1 <- lm(bill_depth_mm ~ bill_length_mm, data = penguins)\nfit2 <- lm(bill_depth_mm ~ bill_length_mm + species, data = penguins)\nfit3 <- lm(bill_depth_mm ~ -1 + bill_length_mm + species, data = penguins)\nmodelsummary(\n  list(fit1, fit2, fit3),\n  estimate = \"{estimate}{stars} ({std.error})\", \n  statistic = NULL, \n  gof_map = c(\"nobs\", \"adj.r.squared\")\n)\n\n\n\n\n\n \n  \n      \n     (1) \n      (2) \n      (3) \n  \n \n\n  \n    (Intercept) \n    20.885*** (0.844) \n    10.592*** (0.683) \n     \n  \n  \n    bill_length_mm \n    −0.085*** (0.019) \n    0.200*** (0.017) \n    0.200*** (0.017) \n  \n  \n    speciesChinstrap \n     \n    −1.933*** (0.224) \n    8.659*** (0.862) \n  \n  \n    speciesGentoo \n     \n    −5.106*** (0.191) \n    5.486*** (0.835) \n  \n  \n    speciesAdelie \n     \n     \n    10.592*** (0.683) \n  \n  \n    Num.Obs. \n    342 \n    342 \n    342 \n  \n  \n    R2 Adj. \n    0.052 \n    0.767 \n    0.997"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#simpsons-paradox-conclusion",
    "href": "slides/w08a-panel-regression.html#simpsons-paradox-conclusion",
    "title": "Panel Regression Analysis",
    "section": "Simpson’s paradox conclusion",
    "text": "Simpson’s paradox conclusion\n\nTrends or relationships are observed in the whole population, but they reverse or disappear, when each group is treated separately.\n\n\nCauses:\n\nUnobserved heterogeneity/differences between groups.\nUnderlining processes that are different between parts of the population.\n\n\n\nResolutions to the paradox:\n\nControl variables in the MRL.\nPanel data."
  },
  {
    "objectID": "slides/w08a-panel-regression.html#cross-sectional-data",
    "href": "slides/w08a-panel-regression.html#cross-sectional-data",
    "title": "Panel Regression Analysis",
    "section": "Cross-sectional data",
    "text": "Cross-sectional data\n\n\n\n\n\n\nID\nY\nX1\nX2\n\n\n\n\n\\(1\\)\n\\(y_{1}\\)\n\\(x^{1}_{1}\\)\n\\(x^{2}_{1}\\)\n\n\n\\(2\\)\n\\(y_{2}\\)\n\\(x^{1}_{2}\\)\n\\(x^{2}_{2}\\)\n\n\n\\(3\\)\n\\(y_{3}\\)\n\\(x^{1}_{3}\\)\n\\(x^{2}_{3}\\)\n\n\n\\(4\\)\n\\(y_{4}\\)\n\\(x^{1}_{4}\\)\n\\(x^{2}_{4}\\)\n\n\n\\(5\\)\n\\(y_{5}\\)\n\\(x^{1}_{5}\\)\n\\(x^{2}_{5}\\)\n\n\n\\(6\\)\n\\(y_{6}\\)\n\\(x^{1}_{6}\\)\n\\(x^{2}_{6}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(N\\)\n\\(y_{N}\\)\n\\(x^{1}_{N}\\)\n\\(x^{2}_{N}\\)\n\n\n\n\n\n\nData that we usually collect in a single data collection.\n\nEach individual is represented by one observation.\n\nCould be repeatedly collected multiple times (repeated cross-section),\n\nbut, in every repetition, there are different individuals!"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#panel-data",
    "href": "slides/w08a-panel-regression.html#panel-data",
    "title": "Panel Regression Analysis",
    "section": "Panel data",
    "text": "Panel data\n\n\n\n\n\nID\nTime\nY\nX1\nX2\n\n\n\n\n\\(1\\)\n\\(1\\)\n\\(y_{11}\\)\n\\(x^{1}_{11}\\)\n\\(x^{2}_{11}\\)\n\n\n\\(1\\)\n\\(2\\)\n\\(y_{12}\\)\n\\(x^{1}_{12}\\)\n\\(x^{2}_{12}\\)\n\n\n\\(1\\)\n\\(3\\)\n\\(y_{13}\\)\n\\(x^{1}_{13}\\)\n\\(x^{2}_{13}\\)\n\n\n\\(2\\)\n\\(2\\)\n\\(y_{22}\\)\n\\(x^{1}_{22}\\)\n\\(x^{2}_{22}\\)\n\n\n\\(2\\)\n\\(3\\)\n\\(y_{23}\\)\n\\(x^{1}_{23}\\)\n\\(x^{2}_{23}\\)\n\n\n\\(3\\)\n\\(1\\)\n\\(y_{31}\\)\n\\(x^{1}_{31}\\)\n\\(x^{2}_{31}\\)\n\n\n\\(3\\)\n\\(2\\)\n\\(y_{32}\\)\n\\(x^{1}_{32}\\)\n\\(x^{2}_{32}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(N\\)\n\\(1\\)\n\\(y_{N1}\\)\n\\(x^{1}_{N1}\\)\n\\(x^{1}_{N1}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(N\\)\n\\(T\\)\n\\(y_{NT}\\)\n\\(x^{1}_{NT}\\)\n\\(x^{2}_{NT}\\)\n\n\n\n\n\ntable with data, where\neach individual (cohort, e.i. region, country)\nis represented by multiple observations at different time periods."
  },
  {
    "objectID": "slides/w08a-panel-regression.html#panel-data-balanced-and-unbalanced",
    "href": "slides/w08a-panel-regression.html#panel-data-balanced-and-unbalanced",
    "title": "Panel Regression Analysis",
    "section": "Panel data: Balanced and Unbalanced",
    "text": "Panel data: Balanced and Unbalanced\n\n\nBalanced\nEach individual is represented in all time periods.\n\n\n\n\\(\\text{ID}\\)\n\\(\\text{Time}\\)\n\\(Y\\)\n\\(X\\)\n\n\n\n\n1\n1\n\\(Y_{11}\\)\n\\(X_{11}\\)\n\n\n1\n2\n\\(Y_{12}\\)\n\\(X_{12}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n1\n\\(T\\)\n\\(Y_{1T}\\)\n\\(X_{1T}\\)\n\n\n2\n1\n\\(Y_{21}\\)\n\\(X_{21}\\)\n\n\n2\n2\n\\(Y_{22}\\)\n\\(X_{22}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n2\n\\(T\\)\n\\(Y_{2T}\\)\n\\(X_{2T}\\)\n\n\n3\n1\n\\(Y_{31}\\)\n\\(X_{31}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(N\\)\n\\(T\\)\n\\(Y_{NT}\\)\n\\(X_{NT}\\)\n\n\n\n\nUn balanced\nEach individual only appears in some time periods (not all).\n\n\n\n\\(\\text{ID}\\)\n\\(\\text{Time}\\)\n\\(Y\\)\n\\(X\\)\n\n\n\n\n1\n1\n\\(Y_{11}\\)\n\\(X_{11}\\)\n\n\n1\n2\n\\(Y_{12}\\)\n\\(X_{12}\\)\n\n\n2\n2\n\\(Y_{22}\\)\n\\(X_{22}\\)\n\n\n2\n3\n\\(Y_{23}\\)\n\\(X_{23}\\)\n\n\n3\n3\n\\(Y_{33}\\)\n\\(X_{33}\\)\n\n\n4\n1\n\\(Y_{41}\\)\n\\(X_{41}\\)\n\n\n5\n2\n\\(Y_{52}\\)\n\\(X_{52}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(N\\)\n\\(T\\)\n\\(Y_{NT}\\)\n\\(X_{NT}\\)"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#problem-setting",
    "href": "slides/w08a-panel-regression.html#problem-setting",
    "title": "Panel Regression Analysis",
    "section": "Problem setting",
    "text": "Problem setting\nDoes the collective bargaining (union membership) has any effect on wages?\n\nSee: (Card, 1996; Freeman, 1984)\n\n\n\\[\nlog(\\text{Wage}_{it}) = \\beta_0 + \\beta_1 \\cdot \\text{Union}_{it} + \\beta_2 \\cdot {X_{it}} + \\beta_3 \\cdot \\text{Ability}_{i} + \\epsilon_{it}\n\\]\nwhere \\(i\\) is the individual and \\(t\\) is the time dimension;"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#is-there-an-endogeneity-problem",
    "href": "slides/w08a-panel-regression.html#is-there-an-endogeneity-problem",
    "title": "Panel Regression Analysis",
    "section": "Is there an endogeneity problem?",
    "text": "Is there an endogeneity problem?\n\\[\nlog(\\text{Wage}_{it}) = \\beta_0 + \\beta_1 \\cdot \\text{Union}_{it} + \\beta_2 \\cdot {X_{it}} + \\beta_3 \\cdot \\text{Ability}_{i} + \\epsilon_{it}\n\\]\n\nIs there a source of endogeneity / selection bias here?\n\nAny ideas?\n\n\\(\\text{Ability}_{i}\\) not observable and not measurable;\n\ntime invariant;\ncorrelates with \\(X\\) and \\(Y\\);\n\nOmitting ability causes bias"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#one-of-the-solutions",
    "href": "slides/w08a-panel-regression.html#one-of-the-solutions",
    "title": "Panel Regression Analysis",
    "section": "One of the solutions:",
    "text": "One of the solutions:\n\nAbility are time-invariant and unique to each individual;\n\nIf we have multiple observation per each individual (panel data),\nwe can introduce dummy variables for each individual, to approximate ability.\n\nThis is also called Fixed Effect - regression model\n\nor a within transformation model\nor Difference in Difference"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#other-solutions",
    "href": "slides/w08a-panel-regression.html#other-solutions",
    "title": "Panel Regression Analysis",
    "section": "Other solutions:",
    "text": "Other solutions:\n\nAny ideas?\nIntroduce control variables that are proxy of ability.\nEmploy specific research design:\n\nRCT\nRDD"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#empirical-example",
    "href": "slides/w08a-panel-regression.html#empirical-example",
    "title": "Panel Regression Analysis",
    "section": "Empirical example",
    "text": "Empirical example\n\nlibrary(tidyverse)\nlibrary(modelsummary)\nwage_dta <- read_csv(\"wage_unon_panel.csv\")\nglimpse(wage_dta)\n\n\n\nRows: 4,165\nColumns: 15\n$ id      <dbl> 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3,…\n$ year    <dbl> 82, 83, 84, 85, 86, 87, 88, 82, 83, 84, 85, 86, 87, 88, 82, 83…\n$ exper   <dbl> 3, 4, 5, 6, 7, 8, 9, 30, 31, 32, 33, 34, 35, 36, 6, 7, 8, 9, 1…\n$ hours   <dbl> 32, 43, 40, 39, 42, 35, 32, 34, 27, 33, 30, 30, 37, 30, 50, 51…\n$ bluecol <chr> \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"yes\", \"yes\",…\n$ ind     <dbl> 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ south   <chr> \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"no\", \"no\", \"…\n$ smsa    <chr> \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"n…\n$ married <chr> \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\",…\n$ union   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,…\n$ educ    <dbl> 9, 9, 9, 9, 9, 9, 9, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 1…\n$ black   <chr> \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"n…\n$ lwage   <dbl> 5.56068, 5.72031, 5.99645, 5.99645, 6.06146, 6.17379, 6.24417,…\n$ female  <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ wage    <dbl> 259.9996, 304.9995, 401.9992, 401.9992, 429.0013, 480.0019, 51…"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#the-data-1",
    "href": "slides/w08a-panel-regression.html#the-data-1",
    "title": "Panel Regression Analysis",
    "section": "The data",
    "text": "The data\n\n\n\n\n \n  \n    id \n    year \n    exper \n    hours \n    bluecol \n    ind \n    south \n    smsa \n    married \n    union \n    educ \n    black \n    lwage \n    female \n    wage \n  \n \n\n  \n    5 \n    82 \n    10 \n    50 \n    yes \n    0 \n    no \n    no \n    yes \n    1 \n    16 \n    no \n    6.43775 \n    1 \n    624.9990 \n  \n  \n    5 \n    83 \n    11 \n    46 \n    yes \n    0 \n    no \n    no \n    yes \n    1 \n    16 \n    no \n    6.62007 \n    1 \n    749.9976 \n  \n  \n    5 \n    84 \n    12 \n    40 \n    yes \n    0 \n    no \n    no \n    yes \n    1 \n    16 \n    no \n    6.63332 \n    1 \n    760.0012 \n  \n  \n    5 \n    85 \n    13 \n    50 \n    no \n    0 \n    no \n    no \n    yes \n    0 \n    16 \n    no \n    6.98286 \n    1 \n    1077.9970 \n  \n  \n    5 \n    86 \n    14 \n    47 \n    yes \n    0 \n    no \n    yes \n    yes \n    0 \n    16 \n    no \n    7.04752 \n    1 \n    1150.0032 \n  \n  \n    5 \n    87 \n    15 \n    47 \n    no \n    0 \n    no \n    no \n    yes \n    0 \n    16 \n    no \n    7.31322 \n    1 \n    1499.9994 \n  \n  \n    5 \n    88 \n    16 \n    49 \n    no \n    0 \n    no \n    no \n    yes \n    0 \n    16 \n    no \n    7.29574 \n    1 \n    1474.0073 \n  \n  \n    168 \n    82 \n    3 \n    40 \n    no \n    0 \n    no \n    yes \n    yes \n    0 \n    17 \n    no \n    6.23245 \n    1 \n    509.0010 \n  \n  \n    168 \n    83 \n    4 \n    42 \n    no \n    0 \n    no \n    yes \n    yes \n    0 \n    17 \n    no \n    6.57925 \n    1 \n    719.9991 \n  \n  \n    168 \n    84 \n    5 \n    44 \n    no \n    0 \n    no \n    yes \n    yes \n    1 \n    17 \n    no \n    6.65286 \n    1 \n    774.9977 \n  \n  \n    168 \n    85 \n    6 \n    48 \n    no \n    0 \n    no \n    yes \n    yes \n    1 \n    17 \n    no \n    6.74524 \n    1 \n    850.0031 \n  \n  \n    168 \n    86 \n    7 \n    48 \n    no \n    0 \n    no \n    yes \n    yes \n    0 \n    17 \n    no \n    7.49554 \n    1 \n    1799.9965 \n  \n  \n    168 \n    87 \n    8 \n    48 \n    no \n    0 \n    no \n    yes \n    yes \n    0 \n    17 \n    no \n    8.16052 \n    1 \n    3500.0061 \n  \n  \n    168 \n    88 \n    9 \n    50 \n    no \n    0 \n    no \n    yes \n    yes \n    0 \n    17 \n    no \n    8.30820 \n    1 \n    4057.0038"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#pooled-regression",
    "href": "slides/w08a-panel-regression.html#pooled-regression",
    "title": "Panel Regression Analysis",
    "section": "Pooled Regression",
    "text": "Pooled Regression\n\\[\nlog(\\text{Wage}_{it}) = \\beta_0 + \\beta_1 \\cdot \\text{Union}_{it} + \\beta_2 \\cdot {X_{it}} + \\epsilon_{it}\n\\]\n\n\nRegression model on all observations in the panel data set without any individual effects.\n\n\nunion_fit_0 <- lm(log(wage) ~ union + educ + exper + I(exper^2) + hours , \n                  data = wage_dta)\nunion_fit_0\n\n\n\n\nCall:\nlm(formula = log(wage) ~ union + educ + exper + I(exper^2) + \n    hours, data = wage_dta)\n\nCoefficients:\n(Intercept)        union         educ        exper   I(exper^2)        hours  \n  4.7054380    0.1261467    0.0819744    0.0437155   -0.0006932    0.0077042"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#least-squares-dummy-variable-lsdv",
    "href": "slides/w08a-panel-regression.html#least-squares-dummy-variable-lsdv",
    "title": "Panel Regression Analysis",
    "section": "Least-squares dummy variable (LSDV)",
    "text": "Least-squares dummy variable (LSDV)\n\\[\nlog(\\text{Wage}_{it}) = \\beta_0 + \\beta_1 \\cdot \\text{Union}_{it} + \\beta_2 \\cdot {X_{it}} + \\beta_3 \\cdot \\color{Red}{\\delta_{i}} + \\epsilon_{it}\n\\]\n\n\nPooled regression plus dummy variable for each individual.\nThis is not a Fixed Effect Panel Regression!\n\n\nunion_fit_1 <- lm(log(wage) ~ union + educ + exper + I(exper^2) + hours + factor(id), \n                  data = wage_dta)\nunion_fit_1\n\n\n\n\nCall:\nlm(formula = log(wage) ~ union + educ + exper + I(exper^2) + \n    hours + factor(id), data = wage_dta)\n\nCoefficients:\n  (Intercept)          union           educ          exper     I(exper^2)  \n    4.3485732      0.0300295      0.1023231      0.1137052     -0.0004234  \n        hours    factor(id)2    factor(id)3    factor(id)4    factor(id)5  \n    0.0007980     -2.2946317     -0.1234092     -2.3978310     -0.5366805  \n  factor(id)6    factor(id)7    factor(id)8    factor(id)9   factor(id)10  \n   -1.4597761     -1.2411420     -1.5180049      0.2078530     -0.1734390  \n factor(id)11   factor(id)12   factor(id)13   factor(id)14   factor(id)15  \n   -1.4704689     -1.3608691     -1.3890339     -1.0439450     -0.9439428  \n factor(id)16   factor(id)17   factor(id)18   factor(id)19   factor(id)20  \n   -1.5809655     -0.9700846     -1.3922150     -3.0892451     -1.8367302  \n factor(id)21   factor(id)22   factor(id)23   factor(id)24   factor(id)25  \n   -1.7226414     -1.4223248      0.0054199     -0.8357823     -0.6064977  \n factor(id)26   factor(id)27   factor(id)28   factor(id)29   factor(id)30  \n   -0.7379286     -0.2772751     -1.8216457     -1.8871665     -1.3042601  \n factor(id)31   factor(id)32   factor(id)33   factor(id)34   factor(id)35  \n   -1.4134898     -1.8734643     -0.6413820     -3.7105381     -0.1984895  \n factor(id)36   factor(id)37   factor(id)38   factor(id)39   factor(id)40  \n   -0.3421001     -0.3410200     -0.7597065     -0.0688068     -1.7787978  \n factor(id)41   factor(id)42   factor(id)43   factor(id)44   factor(id)45  \n   -1.0532843     -2.2573678      0.1550164      0.0478652     -0.9543507  \n factor(id)46   factor(id)47   factor(id)48   factor(id)49   factor(id)50  \n   -0.2541933     -1.7069053     -1.8137723     -1.9258846     -2.4484643  \n factor(id)51   factor(id)52   factor(id)53   factor(id)54   factor(id)55  \n   -1.9760363     -1.6064527     -0.4646712     -0.4521180     -1.7422486  \n factor(id)56   factor(id)57   factor(id)58   factor(id)59   factor(id)60  \n   -1.9282946     -1.6643194     -2.8323912     -1.7329928     -1.6527519  \n factor(id)61   factor(id)62   factor(id)63   factor(id)64   factor(id)65  \n   -1.9914748     -0.1579195     -0.2806420     -2.6926172     -1.7074951  \n factor(id)66   factor(id)67   factor(id)68   factor(id)69   factor(id)70  \n   -1.0021172     -2.5046091     -2.5941980     -2.0777134     -0.4562135  \n factor(id)71   factor(id)72   factor(id)73   factor(id)74   factor(id)75  \n   -0.0984012     -0.9192300     -0.9709685     -0.7050966     -1.7193154  \n factor(id)76   factor(id)77   factor(id)78   factor(id)79   factor(id)80  \n   -1.1672133     -0.7934024     -3.4592869     -0.8823052     -0.0604197  \n factor(id)81   factor(id)82   factor(id)83   factor(id)84   factor(id)85  \n   -0.2728831     -1.3694580     -0.8873794     -3.4094508     -0.6181149  \n factor(id)86   factor(id)87   factor(id)88   factor(id)89   factor(id)90  \n    0.3230099     -0.6208690     -0.0384965     -0.7156531      0.2819298  \n factor(id)91   factor(id)92   factor(id)93   factor(id)94   factor(id)95  \n    0.2219382     -0.0389122     -2.2703900     -0.6713527     -2.5658913  \n factor(id)96   factor(id)97   factor(id)98   factor(id)99  factor(id)100  \n   -1.8929945     -2.3668088     -1.0815051     -1.2371669     -0.3689453  \nfactor(id)101  factor(id)102  factor(id)103  factor(id)104  factor(id)105  \n   -2.0715701      0.2419079     -1.0534000     -1.2417073     -0.3892415  \nfactor(id)106  factor(id)107  factor(id)108  factor(id)109  factor(id)110  \n    0.0013452     -1.8143890     -0.9852634     -1.4033012     -2.7813619  \nfactor(id)111  factor(id)112  factor(id)113  factor(id)114  factor(id)115  \n   -0.1122968     -0.8453668     -1.8469750     -2.9350313     -0.4420842  \nfactor(id)116  factor(id)117  factor(id)118  factor(id)119  factor(id)120  \n   -0.7258607     -1.2304082     -1.3950810     -0.3204476     -0.7104122  \nfactor(id)121  factor(id)122  factor(id)123  factor(id)124  factor(id)125  \n   -0.3685394     -0.6871320     -1.5356558     -1.9044675     -0.9214093  \nfactor(id)126  factor(id)127  factor(id)128  factor(id)129  factor(id)130  \n    0.1863200     -1.8088881     -2.3404805     -2.3327080     -1.8672439  \nfactor(id)131  factor(id)132  factor(id)133  factor(id)134  factor(id)135  \n   -1.7733156     -1.2515247     -1.2618072     -0.2715418     -0.5405965  \nfactor(id)136  factor(id)137  factor(id)138  factor(id)139  factor(id)140  \n   -0.9040781     -1.2464677     -1.3348897      0.0546240     -0.1468334  \nfactor(id)141  factor(id)142  factor(id)143  factor(id)144  factor(id)145  \n   -0.9653104     -2.8803152     -2.5587496     -0.3057205     -1.1895399  \nfactor(id)146  factor(id)147  factor(id)148  factor(id)149  factor(id)150  \n   -2.5564297     -0.3336869     -0.2135711     -0.2358201     -0.9902041  \nfactor(id)151  factor(id)152  factor(id)153  factor(id)154  factor(id)155  \n   -1.8371688     -0.3919091     -0.9059245     -0.8288426      0.2639585  \nfactor(id)156  factor(id)157  factor(id)158  factor(id)159  factor(id)160  \n   -2.3669688     -0.1579770     -1.9607783     -2.2903756     -2.0867318  \nfactor(id)161  factor(id)162  factor(id)163  factor(id)164  factor(id)165  \n   -0.4052899     -0.5879060     -0.7129091     -0.5237673     -3.0691480  \nfactor(id)166  factor(id)167  factor(id)168  factor(id)169  factor(id)170  \n   -0.3346194     -1.2951818      0.3693013     -1.6563817     -2.1460479  \nfactor(id)171  factor(id)172  factor(id)173  factor(id)174  factor(id)175  \n   -1.0321605     -0.2130777     -1.7902925      0.1377581     -0.0160055  \nfactor(id)176  factor(id)177  factor(id)178  factor(id)179  factor(id)180  \n    0.2569786     -1.6326400      0.4028510     -2.3587711     -0.3845946  \nfactor(id)181  factor(id)182  factor(id)183  factor(id)184  factor(id)185  \n   -0.4108327     -0.9834346     -1.0972550     -1.6781968     -1.5026082  \nfactor(id)186  factor(id)187  factor(id)188  factor(id)189  factor(id)190  \n   -2.1577766     -1.1647283     -0.1239156     -1.7815425      0.2619762  \nfactor(id)191  factor(id)192  factor(id)193  factor(id)194  factor(id)195  \n   -1.4993833      0.0160729     -2.2211180     -0.4287428     -0.8272276  \nfactor(id)196  factor(id)197  factor(id)198  factor(id)199  factor(id)200  \n    0.1178582     -1.5464632     -1.9258471     -0.1875497     -0.3310429  \nfactor(id)201  factor(id)202  factor(id)203  factor(id)204  factor(id)205  \n   -1.4934470     -0.5369335     -1.5108323     -2.0658719     -1.6709767  \nfactor(id)206  factor(id)207  factor(id)208  factor(id)209  factor(id)210  \n   -0.0090120     -0.5221725     -1.6804807     -2.0796591     -2.0767268  \nfactor(id)211  factor(id)212  factor(id)213  factor(id)214  factor(id)215  \n   -1.4748328     -0.0448817     -1.4898530     -1.9329652     -3.5076699  \nfactor(id)216  factor(id)217  factor(id)218  factor(id)219  factor(id)220  \n   -1.1255642     -0.2327264     -0.6513809     -0.8178650     -2.4156438  \nfactor(id)221  factor(id)222  factor(id)223  factor(id)224  factor(id)225  \n   -2.1017309      0.0331950     -1.0231728     -1.4645738     -0.5458016  \nfactor(id)226  factor(id)227  factor(id)228  factor(id)229  factor(id)230  \n   -3.0261551     -2.2929047      0.2832356     -1.3169281      0.2964316  \nfactor(id)231  factor(id)232  factor(id)233  factor(id)234  factor(id)235  \n   -1.0946220     -2.2743788     -1.8237789     -1.6554734     -1.2081230  \nfactor(id)236  factor(id)237  factor(id)238  factor(id)239  factor(id)240  \n   -0.0145449     -0.6384647     -2.1569987     -1.4416687     -2.0393688  \nfactor(id)241  factor(id)242  factor(id)243  factor(id)244  factor(id)245  \n   -0.5874512     -0.5934676      0.2367489     -1.4340693     -2.3045132  \nfactor(id)246  factor(id)247  factor(id)248  factor(id)249  factor(id)250  \n   -2.7880182     -0.7003395     -1.1107962     -2.4985511     -1.3364049  \nfactor(id)251  factor(id)252  factor(id)253  factor(id)254  factor(id)255  \n   -0.6067969     -1.5576645     -0.0986338     -2.6540530      0.0383125  \nfactor(id)256  factor(id)257  factor(id)258  factor(id)259  factor(id)260  \n   -0.2349775     -2.1928398     -2.3075449     -0.7143545     -0.1095476  \nfactor(id)261  factor(id)262  factor(id)263  factor(id)264  factor(id)265  \n   -0.7968060     -0.5774167     -0.6080097     -0.1404218     -0.6091514  \nfactor(id)266  factor(id)267  factor(id)268  factor(id)269  factor(id)270  \n   -1.5381684     -0.6415396     -0.8257309     -0.9420411     -0.8422041  \nfactor(id)271  factor(id)272  factor(id)273  factor(id)274  factor(id)275  \n   -0.4524016     -0.2456966     -1.7472768     -1.0506405     -0.0310502  \nfactor(id)276  factor(id)277  factor(id)278  factor(id)279  factor(id)280  \n    0.3472376     -2.0224341      0.1895913     -0.6614986     -2.7705095  \nfactor(id)281  factor(id)282  factor(id)283  factor(id)284  factor(id)285  \n    0.2245524     -1.3899214     -2.7102791     -2.2411764      0.2478444  \nfactor(id)286  factor(id)287  factor(id)288  factor(id)289  factor(id)290  \n   -0.3704373     -0.5927907     -0.2396347     -1.6533998     -0.0958892  \nfactor(id)291  factor(id)292  factor(id)293  factor(id)294  factor(id)295  \n   -2.5107176     -1.7989286     -0.5703390     -1.2011109     -1.4576638  \nfactor(id)296  factor(id)297  factor(id)298  factor(id)299  factor(id)300  \n   -1.9986368     -0.5870901     -0.7176117     -3.2822239     -2.7838597  \nfactor(id)301  factor(id)302  factor(id)303  factor(id)304  factor(id)305  \n   -0.3324034     -1.0640144      0.2979783     -0.4636494     -1.1197335  \nfactor(id)306  factor(id)307  factor(id)308  factor(id)309  factor(id)310  \n   -1.2954014      0.8212389     -1.8036660     -1.3745703     -0.7606135  \nfactor(id)311  factor(id)312  factor(id)313  factor(id)314  factor(id)315  \n    0.2105132     -0.8752712     -0.6629730     -0.4802000     -1.0600291  \nfactor(id)316  factor(id)317  factor(id)318  factor(id)319  factor(id)320  \n   -1.9089897     -0.9238980     -0.6972992     -1.6439082     -0.9770545  \nfactor(id)321  factor(id)322  factor(id)323  factor(id)324  factor(id)325  \n   -0.2597240     -0.3929224     -0.8099876      0.2160266     -3.1693716  \nfactor(id)326  factor(id)327  factor(id)328  factor(id)329  factor(id)330  \n   -1.3707289     -0.8099548     -1.9741880     -3.2232714     -2.9081193  \nfactor(id)331  factor(id)332  factor(id)333  factor(id)334  factor(id)335  \n   -2.4720627     -3.4967480      0.0880504     -2.4198878     -2.0780789  \nfactor(id)336  factor(id)337  factor(id)338  factor(id)339  factor(id)340  \n   -2.3475630      0.5764727     -0.6084878     -0.8872653     -1.9636734  \nfactor(id)341  factor(id)342  factor(id)343  factor(id)344  factor(id)345  \n    0.0883837     -1.8502495     -1.4848842     -0.9710595     -1.2319603  \nfactor(id)346  factor(id)347  factor(id)348  factor(id)349  factor(id)350  \n   -1.7024146     -1.3434399     -1.3863174      0.4956454     -2.5654191  \nfactor(id)351  factor(id)352  factor(id)353  factor(id)354  factor(id)355  \n   -1.3784243      0.0816134     -1.7059876     -1.3627206     -0.4565893  \nfactor(id)356  factor(id)357  factor(id)358  factor(id)359  factor(id)360  \n    0.3603776     -0.9685689     -1.3991295     -3.0412550     -1.6851473  \nfactor(id)361  factor(id)362  factor(id)363  factor(id)364  factor(id)365  \n   -0.7058692     -0.9195798     -1.0953487     -1.4769641     -2.6742985  \nfactor(id)366  factor(id)367  factor(id)368  factor(id)369  factor(id)370  \n   -0.6578946     -1.2999971     -0.3382028      0.3863243     -0.0720030  \nfactor(id)371  factor(id)372  factor(id)373  factor(id)374  factor(id)375  \n   -1.9432209     -0.6223422     -1.1822873     -0.9548354     -2.0004202  \nfactor(id)376  factor(id)377  factor(id)378  factor(id)379  factor(id)380  \n   -1.9659526     -1.0630786      0.2863626     -0.9885524     -2.2528915  \nfactor(id)381  factor(id)382  factor(id)383  factor(id)384  factor(id)385  \n   -1.8595097     -1.5724380     -1.8713639     -2.5344664     -0.1624537  \nfactor(id)386  factor(id)387  factor(id)388  factor(id)389  factor(id)390  \n   -0.6421035     -0.3629400     -2.3279087     -1.1232122     -1.7657519  \nfactor(id)391  factor(id)392  factor(id)393  factor(id)394  factor(id)395  \n   -2.5207200     -1.6091356     -0.8136357      0.2210733     -2.2452954  \nfactor(id)396  factor(id)397  factor(id)398  factor(id)399  factor(id)400  \n   -0.6756564      0.1621353     -0.8481867     -1.7341285     -0.3817257  \nfactor(id)401  factor(id)402  factor(id)403  factor(id)404  factor(id)405  \n    0.2949123     -0.8927519     -0.9338798     -0.7166768      0.3456415  \nfactor(id)406  factor(id)407  factor(id)408  factor(id)409  factor(id)410  \n    0.2419052     -1.8216061     -0.4228484     -2.9393318     -2.0225911  \nfactor(id)411  factor(id)412  factor(id)413  factor(id)414  factor(id)415  \n   -0.4310985      0.4951506     -0.3954707     -2.8343863     -1.6226128  \nfactor(id)416  factor(id)417  factor(id)418  factor(id)419  factor(id)420  \n   -2.2101251     -0.3836277     -2.0551281     -0.3642385     -0.4022928  \nfactor(id)421  factor(id)422  factor(id)423  factor(id)424  factor(id)425  \n   -1.4834110     -0.5625562     -0.6107236      0.1691667      0.3211209  \nfactor(id)426  factor(id)427  factor(id)428  factor(id)429  factor(id)430  \n    0.2279390     -1.8875790     -3.0841075     -0.6159943     -0.3415796  \nfactor(id)431  factor(id)432  factor(id)433  factor(id)434  factor(id)435  \n   -0.7118149     -1.5641616     -1.9801839      0.1450722     -0.2017439  \nfactor(id)436  factor(id)437  factor(id)438  factor(id)439  factor(id)440  \n   -0.3145338     -1.1190083     -1.7823379     -0.3957654     -0.9003941  \nfactor(id)441  factor(id)442  factor(id)443  factor(id)444  factor(id)445  \n   -2.3437182     -3.4028128     -1.7003478      0.7524891     -1.0873692  \nfactor(id)446  factor(id)447  factor(id)448  factor(id)449  factor(id)450  \n   -0.5228583     -1.8009627     -2.2900269     -0.2715184      0.1416728  \nfactor(id)451  factor(id)452  factor(id)453  factor(id)454  factor(id)455  \n   -0.3214500     -0.7690325     -2.2657456     -0.0174455     -1.6515788  \nfactor(id)456  factor(id)457  factor(id)458  factor(id)459  factor(id)460  \n   -1.4418559     -2.8946224     -0.9420793     -1.4296444      0.1185207  \nfactor(id)461  factor(id)462  factor(id)463  factor(id)464  factor(id)465  \n   -2.2330238     -2.5544505     -0.9337258     -3.5688408     -0.2714708  \nfactor(id)466  factor(id)467  factor(id)468  factor(id)469  factor(id)470  \n   -2.2471530     -1.8575604     -3.0165280     -4.3707337     -0.1020631  \nfactor(id)471  factor(id)472  factor(id)473  factor(id)474  factor(id)475  \n   -2.2962452     -0.4362524     -0.0682312     -0.7113828     -0.6509739  \nfactor(id)476  factor(id)477  factor(id)478  factor(id)479  factor(id)480  \n   -2.2159030     -0.3722269     -0.1277137     -0.8617902     -1.3784264  \nfactor(id)481  factor(id)482  factor(id)483  factor(id)484  factor(id)485  \n   -1.2668125     -0.3221145     -0.3694822      0.3742597     -1.1971520  \nfactor(id)486  factor(id)487  factor(id)488  factor(id)489  factor(id)490  \n   -1.7667638     -0.3340940     -1.7205133     -2.2568467     -1.8137394  \nfactor(id)491  factor(id)492  factor(id)493  factor(id)494  factor(id)495  \n   -0.0582747     -1.9190715     -0.6342174     -3.6974693     -0.3921098  \nfactor(id)496  factor(id)497  factor(id)498  factor(id)499  factor(id)500  \n   -1.5298775     -0.5476525     -1.4930554     -2.5228905     -0.2511241  \nfactor(id)501  factor(id)502  factor(id)503  factor(id)504  factor(id)505  \n   -2.0783921     -1.0366267      0.3891613     -0.0326843     -1.4013691  \nfactor(id)506  factor(id)507  factor(id)508  factor(id)509  factor(id)510  \n    0.1109218     -1.7455372     -2.6679473     -1.1284840      0.1729925  \nfactor(id)511  factor(id)512  factor(id)513  factor(id)514  factor(id)515  \n   -0.8348753      0.4589049     -2.5682984     -0.9878010     -0.5940622  \nfactor(id)516  factor(id)517  factor(id)518  factor(id)519  factor(id)520  \n   -1.7425469     -0.8692734     -1.1061395     -0.2754527     -2.0568223  \nfactor(id)521  factor(id)522  factor(id)523  factor(id)524  factor(id)525  \n   -1.2248788      0.4854992      0.3539625     -0.4260779     -0.6468819  \nfactor(id)526  factor(id)527  factor(id)528  factor(id)529  factor(id)530  \n   -0.0903952     -0.2045723     -0.4261753     -2.0586098     -0.1378354  \nfactor(id)531  factor(id)532  factor(id)533  factor(id)534  factor(id)535  \n    0.5090071     -0.3524712     -2.7228941     -0.5025199     -1.0624910  \nfactor(id)536  factor(id)537  factor(id)538  factor(id)539  factor(id)540  \n   -0.3035858     -0.4137570     -0.5286483      0.1598279     -1.4470984  \nfactor(id)541  factor(id)542  factor(id)543  factor(id)544  factor(id)545  \n   -0.3111554     -1.0187912     -0.1758000      0.3356895     -2.5147919  \nfactor(id)546  factor(id)547  factor(id)548  factor(id)549  factor(id)550  \n   -0.2690522      0.0699535     -0.3780972     -0.5916537     -0.1192377  \nfactor(id)551  factor(id)552  factor(id)553  factor(id)554  factor(id)555  \n   -0.4489108     -0.5228657     -0.2531003     -1.4951698     -1.6801842  \nfactor(id)556  factor(id)557  factor(id)558  factor(id)559  factor(id)560  \n    0.3760810     -1.0286751     -1.1760088     -3.2089989     -2.5710271  \nfactor(id)561  factor(id)562  factor(id)563  factor(id)564  factor(id)565  \n    0.0315641      0.1818021     -2.2825744      0.0822954     -1.1950727  \nfactor(id)566  factor(id)567  factor(id)568  factor(id)569  factor(id)570  \n    0.1940021     -3.5865482     -0.2442893     -1.0425527     -0.7318057  \nfactor(id)571  factor(id)572  factor(id)573  factor(id)574  factor(id)575  \n   -0.0624118      0.0528832     -0.1803426      0.2557702     -0.6287755  \nfactor(id)576  factor(id)577  factor(id)578  factor(id)579  factor(id)580  \n    0.2609783     -0.8120050     -0.0670469     -0.4403119      0.0559130  \nfactor(id)581  factor(id)582  factor(id)583  factor(id)584  factor(id)585  \n   -0.3724249      0.3466553      0.1053244      0.0263852     -0.1070240  \nfactor(id)586  factor(id)587  factor(id)588  factor(id)589  factor(id)590  \n   -1.3060257     -0.3438155     -0.2917004     -2.1209831      0.3472921  \nfactor(id)591  factor(id)592  factor(id)593  factor(id)594  factor(id)595  \n   -2.2030413      0.0168668     -1.0857457     -0.0342374             NA"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#data-structure-in-the-lsdv",
    "href": "slides/w08a-panel-regression.html#data-structure-in-the-lsdv",
    "title": "Panel Regression Analysis",
    "section": "Data structure in the LSDV",
    "text": "Data structure in the LSDV\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nTime\nY\nX1\nX2\n\\(\\delta_1\\)\n\\(\\delta_2\\)\n\\(\\delta_N\\)\n\n\n\n\n\\(1\\)\n\\(1\\)\n\\(y_{11}\\)\n\\(x^{1}_{11}\\)\n\\(x^{2}_{11}\\)\n1\n0\n0\n\n\n\\(1\\)\n\\(2\\)\n\\(y_{12}\\)\n\\(x^{1}_{12}\\)\n\\(x^{2}_{12}\\)\n1\n0\n0\n\n\n\\(1\\)\n\\(3\\)\n\\(y_{13}\\)\n\\(x^{1}_{13}\\)\n\\(x^{2}_{13}\\)\n1\n0\n0\n\n\n\\(2\\)\n\\(2\\)\n\\(y_{22}\\)\n\\(x^{1}_{22}\\)\n\\(x^{2}_{22}\\)\n0\n1\n0\n\n\n\\(2\\)\n\\(3\\)\n\\(y_{23}\\)\n\\(x^{1}_{23}\\)\n\\(x^{2}_{23}\\)\n0\n1\n0\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(N\\)\n\\(1\\)\n\\(y_{N1}\\)\n\\(x^{1}_{N1}\\)\n\\(x^{1}_{N1}\\)\n0\n0\n1\n\n\n\\(N\\)\n\\(2\\)\n\\(y_{N2}\\)\n\\(x^{1}_{N2}\\)\n\\(x^{2}_{N2}\\)\n0\n0\n1"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#results",
    "href": "slides/w08a-panel-regression.html#results",
    "title": "Panel Regression Analysis",
    "section": "Results",
    "text": "Results\n\nmodelsummary(\n  list(\n    `Pooled` = union_fit_0, \n    `Least-squares dummy variable` = union_fit_1),\n  estimate = \"{estimate}{stars} ({std.error})\", \n  statistic = NULL,\n  coef_map = c(\"(Intercept)\", \"union\", \"educ\", \"exper\", \"hours\", \"tenure\"),\n  gof_map = c(\"nobs\", \"adj.r.squared\" , \"df\"),\n  notes = \"In the Least-squares dummy variable model we omitted all individual-related variables\"\n)\n\n\n\n\n\n \n  \n      \n    Pooled \n    Least-squares dummy variable \n  \n \n\n  \n    (Intercept) \n    4.705*** (0.070) \n    4.349*** (0.289) \n  \n  \n    union \n    0.126*** (0.013) \n    0.030* (0.015) \n  \n  \n    educ \n    0.082*** (0.002) \n    0.102*** (0.027) \n  \n  \n    exper \n    0.044*** (0.002) \n    0.114*** (0.002) \n  \n  \n    hours \n    0.008*** (0.001) \n    0.001 (0.001) \n  \n  \n    Num.Obs. \n    4165 \n    4165 \n  \n  \n    R2 Adj. \n    0.298 \n    0.891 \n  \n  \n    DF \n    5 \n    598 \n  \n\n\n In the Least-squares dummy variable model we omitted all individual-related variables"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#cross-sectional-data-and-lsdv-1",
    "href": "slides/w08a-panel-regression.html#cross-sectional-data-and-lsdv-1",
    "title": "Panel Regression Analysis",
    "section": "Cross-sectional data and LSDV (1)",
    "text": "Cross-sectional data and LSDV (1)\n\nCan we run a LSDV model with the cross-sectional data?\n\n\nAny ideas?\nWhy?….\nNO…\nBecause the number of independent variables have to be less then or equal to the number of observations."
  },
  {
    "objectID": "slides/w08a-panel-regression.html#cross-sectional-data-and-lsdv-2",
    "href": "slides/w08a-panel-regression.html#cross-sectional-data-and-lsdv-2",
    "title": "Panel Regression Analysis",
    "section": "Cross-sectional data and LSDV (2)",
    "text": "Cross-sectional data and LSDV (2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nY\nX1\nX2\n\\(\\delta_1\\)\n\\(\\delta_2\\)\n\\(\\delta_3\\)\n\\(\\delta_N\\)\n\n\n\n\n\\(1\\)\n\\(y_{1}\\)\n\\(x^{1}_{1}\\)\n\\(x^{2}_{1}\\)\n1\n0\n0\n0\n\n\n\\(2\\)\n\\(y_{2}\\)\n\\(x^{1}_{2}\\)\n\\(x^{2}_{2}\\)\n0\n1\n0\n0\n\n\n\\(3\\)\n\\(y_{3}\\)\n\\(x^{1}_{3}\\)\n\\(x^{2}_{3}\\)\n0\n0\n1\n0\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(N\\)\n\\(y_{N}\\)\n\\(x^{1}_{N}\\)\n\\(x^{1}_{N}\\)\n0\n0\n0\n1"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#panel-data-and-lsdv",
    "href": "slides/w08a-panel-regression.html#panel-data-and-lsdv",
    "title": "Panel Regression Analysis",
    "section": "Panel data and LSDV",
    "text": "Panel data and LSDV\nLSDV model works with the panel data, but…\n\nit is inefficient! Any ideas why?…\n\nNumber of dummy variables is equal to the number of individuals + control variables.\n\nIf we have 5,000 individuals, we have 5,000+ regression coefficients.\nWhat if we have 100,000 individuals?\n\nHaving too many regressors remains unbiased, but complicates inference:\n\nnumber of degrees of freedom increases;\nadjusted \\(R^2\\) may shrink to zero;"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#readings",
    "href": "slides/w08a-panel-regression.html#readings",
    "title": "Panel Regression Analysis",
    "section": "Readings",
    "text": "Readings\n\nKey readings:\n\nMundlak (1961)\nAngrist & Pischke (2009) Ch. 5\nJ. M. Wooldridge (2010);\nM. J. Wooldridge (2020);\nSöderbom, Teal, & Eberhardt (2014), Ch. 9-11\n\nOther readings:\n\nCroissant & Millo (2018)"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#terminology",
    "href": "slides/w08a-panel-regression.html#terminology",
    "title": "Panel Regression Analysis",
    "section": "Terminology",
    "text": "Terminology\nPanel data has:\n\n\\(i\\) individuals (groups);\n\\(t\\) time periods for each individual; and\n\\(k\\) independent variables \\(x\\)\n\nPanel Regression could be:\n\nPooled OLS (regression without any panel structure);\nFixed Effect:\n\nLeast-squares dummy variable (Pooled OLS + individual dummies);\nWithin-transformation panel regression most commonly used\nFirst-difference, Between transformation panel regressions (look it up in (Croissant & Millo, 2018))\n\nRandom Effect panel regression"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#pooled-ols",
    "href": "slides/w08a-panel-regression.html#pooled-ols",
    "title": "Panel Regression Analysis",
    "section": "Pooled OLS",
    "text": "Pooled OLS\nOLS regression on the entire data set with panel structure.\n\\[\ny_{it} = \\beta_0 + \\beta_1 \\cdot x_{1it} + \\beta_2 \\cdot x_{2it}  + \\dots + \\beta_k  \\cdot x_{kit} +  \\epsilon_{it}\n\\]\n\nEstimates are biased because of the OVB.\nWe assume the OVB to be time-invariant."
  },
  {
    "objectID": "slides/w08a-panel-regression.html#least-squares-dummy-variable-model",
    "href": "slides/w08a-panel-regression.html#least-squares-dummy-variable-model",
    "title": "Panel Regression Analysis",
    "section": "Least-squares dummy variable model",
    "text": "Least-squares dummy variable model\n\\[\ny_{it} = \\beta_0 + \\beta_1 \\cdot x_{1it} + \\beta_2 \\cdot x_{2it}  + \\dots + \\beta_k  \\cdot x_{kit} + \\gamma_i \\cdot \\color{Red}{\\delta_{i}} + \\epsilon_{it}\n\\]\n\nIntroduces a vector of dummy variables \\(\\color{Red}{\\delta}\\) and estimated coefficients \\(\\gamma_i\\) for each dummy variable.\nEstimates \\(\\hat \\beta\\) and \\(\\hat \\gamma\\) are unbiased (consistent) but inefficient.\nWhen there are too many \\(\\color{Red}{\\delta_{i}}\\) (5000 or more), computer will have difficulties with estimating the coefficients…"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#fixed-effect-model-notations",
    "href": "slides/w08a-panel-regression.html#fixed-effect-model-notations",
    "title": "Panel Regression Analysis",
    "section": "Fixed Effect Model (notations)",
    "text": "Fixed Effect Model (notations)\nIndividual Fixed effect model:\n\\[\ny_{it} =  \\beta_1 \\cdot x_{it} + \\color{Red}{ \\alpha_i } + \\epsilon_{it}\n\\]\n\n\\(\\color{Red}{ \\alpha_i }\\) is the individual-specific fixed effect;\nusually wihtout the intercept \\(\\beta_0\\);\n\n\nTwo-ways fixed effect model (individual + time effect):\n\\[\ny_{it} =  \\beta_1 \\cdot x_{it} + \\color{Red}{ \\alpha_i } + \\color{Blue}{ \\eta_t } + \\epsilon_{it}\n\\]\nTime Fixed Effect model:\n\\[\ny_{it} =  \\beta_1 \\cdot x_{it} + \\color{Blue}{ \\eta_t } + \\epsilon_{it}\n\\]"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#fixed-effect-model-within-transformation",
    "href": "slides/w08a-panel-regression.html#fixed-effect-model-within-transformation",
    "title": "Panel Regression Analysis",
    "section": "Fixed Effect Model: Within transformation",
    "text": "Fixed Effect Model: Within transformation\nWithin-transformation subtracts group means from each observation and run the regression.\n\n\\[\n\\begin{aligned}\nlog(\\text{Wage}_{it} - \\overline{\\text{Wage}_{i}}) & =  \\beta_0 \\\\\n& + \\beta_1 \\cdot (\\text{Union}_{it} -  \\overline{\\text{Union}_{i}}) \\\\\n& + \\beta_2 \\cdot (X_{it} - \\overline{\\text{X}_{i}}) \\\\\n& +  \\beta_3 \\cdot (\\text{Ability}_{i} - \\overline{\\text{Ability}_{i}}) \\\\\n& +  \\color{Red}{\\alpha_{i}} \\\\\n& +  (\\epsilon_{it} - \\overline{\\epsilon_{i}})\n\\end{aligned}\n\\]\n\nNote that time-invariant effects disappear.\n\nAny examples and ideas why?\n\\(\\text{Ability}_{i} - \\overline{\\text{Ability}_{i}} = 0\\).\nGender, race, individual characteristics …\nFE (Within) estimates are identical to the LSDV, but SE are more efficient;"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#fixed-effect-model-first-difference-transformation",
    "href": "slides/w08a-panel-regression.html#fixed-effect-model-first-difference-transformation",
    "title": "Panel Regression Analysis",
    "section": "Fixed Effect Model: First Difference transformation",
    "text": "Fixed Effect Model: First Difference transformation\n\\[\n\\begin{aligned}\nlog(\\text{Wage}_{it} - {\\text{Wage}_{i,t-1}}) & = \\beta_0 \\\\\n& + \\beta_1 \\cdot (\\text{Union}_{it} -  {\\text{Union}_{it-1}}) \\\\\n& + \\beta_2 \\cdot (X_{it} - {{X}_{i,t-1}}) \\\\\n& + \\beta_3 \\cdot (\\text{Ability}_{i} - {\\text{Ability}_{i,t-1}}) \\\\\n& + (\\epsilon_{it} - {\\epsilon_{i,t-1}})\n\\end{aligned}\n\\]\nNote:\n\nSimilar to the within transformation, but sacrifices at least one time dimension.\nRelaxes the autocorrelation assumption.\nMay be not possible with unbalanced data."
  },
  {
    "objectID": "slides/w08a-panel-regression.html#fixed-effect-model-assumptions-1",
    "href": "slides/w08a-panel-regression.html#fixed-effect-model-assumptions-1",
    "title": "Panel Regression Analysis",
    "section": "Fixed Effect Model: assumptions (1)",
    "text": "Fixed Effect Model: assumptions (1)\n\nNOT ZERO correlation between fixed effects \\(\\alpha_i\\) and (not de-meaned) regressors \\(x_{kit}\\):\n\n\\(Cov(\\alpha_i, {x}_{kit}) \\neq 0\\)\n\nStrict exogeneity (No endogeneity):\n\n\\(E[\\epsilon_{is}| {x}_{kit}, \\alpha_i] = 0\\)\n\\(Cov(\\epsilon_{is}, {x}_{kit}) = 0\\) and \\(Cov(\\epsilon_{it}, {x}_{kjt}) = 0\\) , where \\(j\\neq i\\) and \\(s\\neq t\\) ;\nResiduals (\\(\\epsilon\\)) do not correlate with all explanatory variable (\\(x_k\\)) in all time periods (\\(t\\)) and for all individuals (\\(i\\)).\n\nVariance homogeneity:\n\nNo autocorrelation/serial correlation: \\(Cov(\\epsilon_{it}, {X}_{i,t-1}) = 0\\);\nNo cross-sectional dependence: \\(Cov(\\epsilon_{it}, {X}_{j,t}) = 0\\) (when individual observations react similarly to the common shocks or correlate in space);"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#panel-regression-fe-model-not-less-important-assumptions-2",
    "href": "slides/w08a-panel-regression.html#panel-regression-fe-model-not-less-important-assumptions-2",
    "title": "Panel Regression Analysis",
    "section": "Panel Regression FE model not less important assumptions (2)",
    "text": "Panel Regression FE model not less important assumptions (2)\n\nAll Gauss-Markov assumptions\n\nLinearity\nRandom sampling\nNo endogeneity\nNo collinearity\n\nHomoscedasticity of error terms: \\(Var(\\delta_{i}|{X}_{it}) = \\sigma^2_{\\delta}\\)\nNormality of the residuals"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#fixed-effect-application-literature",
    "href": "slides/w08a-panel-regression.html#fixed-effect-application-literature",
    "title": "Panel Regression Analysis",
    "section": "Fixed effect application: literature",
    "text": "Fixed effect application: literature\nSeminal papers: (Mundlak, 1961)\nClimate and agriculture: Bozzola, Massetti, Mendelsohn, & Capitanio (2017)\nChoice of irrigation: Chatzopoulos & Lippert (2015)\nCrop choice: Seo & Mendelsohn (2008b)\nLivestock choice: Seo & Mendelsohn (2008a)\nCross-sectional dependence: (Conley, 1999)"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#random-effect-model-individual-time-and-two-ways",
    "href": "slides/w08a-panel-regression.html#random-effect-model-individual-time-and-two-ways",
    "title": "Panel Regression Analysis",
    "section": "Random Effect Model (individual, time and two-ways)",
    "text": "Random Effect Model (individual, time and two-ways)\n\nIntroduce random components \\(\\color{Red}{v_i}\\) and/or \\(\\color{Blue}{u_{t}}\\)\n\n\n\\[\ny_{it} = \\beta_0 + \\beta_1 \\cdot x_{1it} + \\dots + \\beta_k \\cdot x_{kit}  \\\\\n+ \\color{Red}{v_{i}} + \\color{Blue}{u_{t}}  + \\epsilon_{it}\n\\]\n\nDifference from the fixed effect model:\n\nAssumes NO CORRELATION (ZERO CORRELATION) between random effects and regressors:\n\n\\(Cov(v_{i},{X}_{it}) = 0\\)\n\nIgnoring RE causes no bias to the estimates;"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#summary-on-the-panel-regression",
    "href": "slides/w08a-panel-regression.html#summary-on-the-panel-regression",
    "title": "Panel Regression Analysis",
    "section": "Summary on the Panel Regression",
    "text": "Summary on the Panel Regression\n\n\nFixed Effect (within transformation)\n\nAssumes that Fixed Effects correlate with regressors!\nPartially resolves the OVB.\nIgnoring FE (using pooled regression) causes bias of estimates.\n\n\nRandom Effect\n\nAssumes that Random Effects do NOT correlate with regressors\nDo NOT resolved any OVB.\nProvides additional control strategy, but ignoring RE causes NO bias.\n\n\n\n\nBoth require valid Gauss–Markov assumptions.\n\n\nLimitations of the Fixed and Random effect models\n\nNOT the ultimate solution to Endogeneity.\nOVB may still remain after applying the fixed effects.\nMeasurement error is a problem in panel data."
  },
  {
    "objectID": "slides/w08a-panel-regression.html#general-algorithm",
    "href": "slides/w08a-panel-regression.html#general-algorithm",
    "title": "Panel Regression Analysis",
    "section": "General algorithm",
    "text": "General algorithm\n\nPooled OLS\n\nChoose an appropriate functional form (log/level);\nValidate gauss-Markov assumption validation: Linearity, Collinearity, Random Sampling; Homoscedasticity;\nNote on the ‘No endogeneity’ assumption (if not validated, shows importance of the FE model)\n\nFE: Fixed Effect. Within-transformation. Individual, Time or Two-ways effects;\n\nF-test on FE consistency against pooled.\nLM test on FE Individual, Time or Two-ways effects consistency against each other.\nIf tests suggest the pooled model, but the theory emphasizes FE, discus and reason your choice.\n\nRE: Random Effect;\n\nHausman test on effects’ correlation with regressors of RE consistency against the FE;\nSimilar Chamberlain test, Angrist and Newey tests.\n\nSerial correlation and cross-sectional dependence tests;\n\nWooldridge's, Locally–Robust LM Test, Breusch–Godfrey Test,\nt > 3, we may have a serial correlation problem. Check it with a test.\nCould individuals be affected by common shocks? We might have a cross-sectional dependence problem.\n\nUse robust standard errors to correct for serial correlation and/or cross-sectional dependence:\n\nClustered SE and/or heteroscedasticity and/or autocorrelation robust SE;\n\nSummary and interpretation;"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#step-1.a-pooled-ols",
    "href": "slides/w08a-panel-regression.html#step-1.a-pooled-ols",
    "title": "Panel Regression Analysis",
    "section": "Step 1.a Pooled OLS",
    "text": "Step 1.a Pooled OLS\n\nlibrary(tidyverse)\nlibrary(modelsummary)\nlibrary(parameters)\nlibrary(performance)\nlibrary(lmtest)\nwage_dta <- read_csv(\"wage_unon_panel.csv\")\nglimpse(wage_dta)\n\n\nunion_fit_0 <- \n  lm(log(wage) ~ union + educ + exper + I(exper^2) + hours , \n     data = wage_dta)\nunion_fit_0\n\n\n\n\nCall:\nlm(formula = log(wage) ~ union + educ + exper + I(exper^2) + \n    hours, data = wage_dta)\n\nCoefficients:\n(Intercept)        union         educ        exper   I(exper^2)        hours  \n  4.7054380    0.1261467    0.0819744    0.0437155   -0.0006932    0.0077042"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#step-1.b-assumptions-linearity-homoscedasticity",
    "href": "slides/w08a-panel-regression.html#step-1.b-assumptions-linearity-homoscedasticity",
    "title": "Panel Regression Analysis",
    "section": "Step 1.b Assumptions (Linearity + Homoscedasticity)",
    "text": "Step 1.b Assumptions (Linearity + Homoscedasticity)\n\ncheck_model(union_fit_0, check = c(\"linearity\", \"homogeneity\"))"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#step-1.b-assumptions-homoscedasticity",
    "href": "slides/w08a-panel-regression.html#step-1.b-assumptions-homoscedasticity",
    "title": "Panel Regression Analysis",
    "section": "Step 1.b Assumptions (Homoscedasticity)",
    "text": "Step 1.b Assumptions (Homoscedasticity)\n\ncheck_heteroscedasticity(union_fit_0) \n\n\n\nOK: Error variance appears to be homoscedastic (p = 0.647).\n\n\n\n\nbptest(union_fit_0)\n\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  union_fit_0\nBP = 92.71, df = 5, p-value < 2.2e-16"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#step-1.b-assumptions-collinearity",
    "href": "slides/w08a-panel-regression.html#step-1.b-assumptions-collinearity",
    "title": "Panel Regression Analysis",
    "section": "Step 1.b Assumptions (Collinearity)",
    "text": "Step 1.b Assumptions (Collinearity)\n\ncheck_collinearity(union_fit_0)\n\n\n\n# Check for Multicollinearity\n\nLow Correlation\n\n  Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n union 1.11 [ 1.08,  1.15]         1.05      0.90     [0.87, 0.93]\n  educ 1.13 [ 1.10,  1.18]         1.06      0.88     [0.85, 0.91]\n hours 1.03 [ 1.01,  1.09]         1.02      0.97     [0.92, 0.99]\n\nHigh Correlation\n\n       Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n      exper 18.81 [17.73, 19.95]         4.34      0.05     [0.05, 0.06]\n I(exper^2) 18.81 [17.73, 19.95]         4.34      0.05     [0.05, 0.06]"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#step-1.b-assumptions-no-endogeneity",
    "href": "slides/w08a-panel-regression.html#step-1.b-assumptions-no-endogeneity",
    "title": "Panel Regression Analysis",
    "section": "Step 1.b Assumptions (No endogeneity)",
    "text": "Step 1.b Assumptions (No endogeneity)\n\\[\nlog(\\text{Wage}_{it}) = \\beta_0 + \\beta_1 \\cdot \\text{Union}_{it} + \\beta_2 \\cdot {X_{it}} + \\beta_3 \\cdot \\text{Ability}_{i} + \\epsilon_{it}\n\\]\n\\(\\text{Ability}_{i}\\) not observable and not measurable.\nOmitting the ability may cause the OVB.\n\nNo endogeneity assumption cannot be satisfied.\nWe should exploit the panel data structure."
  },
  {
    "objectID": "slides/w08a-panel-regression.html#step-2.-fe-fixed-effect-within",
    "href": "slides/w08a-panel-regression.html#step-2.-fe-fixed-effect-within",
    "title": "Panel Regression Analysis",
    "section": "Step 2. FE: Fixed Effect (within)",
    "text": "Step 2. FE: Fixed Effect (within)\nNote, the new package: plm used for running panel regressions.\n\nlibrary(plm)\n\nDeclare data to be panel.\n\nwage_dta_pan <- pdata.frame(wage_dta, index = c(\"id\", \"year\"))\n\n\n\n\nCheck panel dimensions.\n\npdim(wage_dta_pan)\n\n\n\nBalanced Panel: n = 595, T = 7, N = 4165"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#step-2.-fe-fixed-effect-within-1",
    "href": "slides/w08a-panel-regression.html#step-2.-fe-fixed-effect-within-1",
    "title": "Panel Regression Analysis",
    "section": "Step 2. FE: Fixed Effect (within) (1)",
    "text": "Step 2. FE: Fixed Effect (within) (1)\nRerun the pooled regression with plm:\n\nunion_pooled <- \n  plm(log(wage) ~ union + educ + exper + I(exper^2) + hours ,\n      data = wage_dta, model = \"pooling\")\nunion_pooled\n\n\n\n\nModel Formula: log(wage) ~ union + educ + exper + I(exper^2) + hours\n\nCoefficients:\n(Intercept)       union        educ       exper  I(exper^2)       hours \n 4.70543801  0.12614668  0.08197441  0.04371549 -0.00069316  0.00770422 \n\n\n\nFixed Effect (individual) model\n\nunion_fe_ind <- \n  plm(log(wage)  ~ union + educ + exper + I(exper^2) + hours ,\n      data = wage_dta, model = \"within\", effect = \"individual\")\nunion_fe_ind\n\n\n\n\nModel Formula: log(wage) ~ union + educ + exper + I(exper^2) + hours\n\nCoefficients:\n      union       exper  I(exper^2)       hours \n 0.03002946  0.11370518 -0.00042343  0.00079804"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#step-2.-fe-fixed-effect-within-2",
    "href": "slides/w08a-panel-regression.html#step-2.-fe-fixed-effect-within-2",
    "title": "Panel Regression Analysis",
    "section": "Step 2. FE: Fixed Effect (within) (2)",
    "text": "Step 2. FE: Fixed Effect (within) (2)\nFixed Effect (time) model\n\nunion_fe_time <- \n  plm(log(wage) ~ union + educ + exper + I(exper^2) + hours ,\n      data = wage_dta, model = \"within\", effect = \"time\")\nunion_fe_time\n\n\n\n\nModel Formula: log(wage) ~ union + educ + exper + I(exper^2) + hours\n\nCoefficients:\n      union        educ       exper  I(exper^2)       hours \n 0.12428314  0.07944664  0.03582803 -0.00058079  0.00756342 \n\n\n\nFixed Effect (Two-ways) model\n\nunion_fe_twoways <- \n  plm(log(wage) ~ union + educ + exper + I(exper^2) + hours ,\n      data = wage_dta, model = \"within\", effect = \"twoways\")\nunion_fe_twoways\n\n\n\n\nModel Formula: log(wage) ~ union + educ + exper + I(exper^2) + hours\n\nCoefficients:\n      union  I(exper^2)       hours \n 0.02712246 -0.00040431  0.00064611"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#step-2.-f-test-1",
    "href": "slides/w08a-panel-regression.html#step-2.-f-test-1",
    "title": "Panel Regression Analysis",
    "section": "Step 2. F-test (1)",
    "text": "Step 2. F-test (1)\nWhich model to choose: Pooled or FE?\n\n\n\nCompares FE models (individual, time, two-ways) vs pooled\n\nPooled is always consistent vs FE\n\nTest logic:\n\nH0: One model is inconsistent. (no individual/time/two-way effects)\nH1: Both models are equally consistent.\n\n\n\n\nRun the test. Check the p-value\n\np-value < 0.05: FE is as good as pooled. Not using the FE model may lead to the bias.\np-value >= 0.05: Pooled is better than the FE model. Use pooled for interpretation."
  },
  {
    "objectID": "slides/w08a-panel-regression.html#step-2.-f-test-2",
    "href": "slides/w08a-panel-regression.html#step-2.-f-test-2",
    "title": "Panel Regression Analysis",
    "section": "Step 2. F-test (2)",
    "text": "Step 2. F-test (2)\n\n\n\npFtest(union_fe_ind, union_pooled)\n\n\n    F test for individual effects\n\ndata:  log(wage) ~ union + educ + exper + I(exper^2) + hours\nF = 39.274, df1 = 593, df2 = 3566, p-value < 2.2e-16\nalternative hypothesis: significant effects\n\n\n\n\nFE is preferred (pooled is biased)\n\n\n\n\n\n\n\npFtest(union_fe_twoways, union_pooled)\n\n\n    F test for twoways effects\n\ndata:  log(wage) ~ union + educ + exper + I(exper^2) + hours\nF = 39.309, df1 = 598, df2 = 3561, p-value < 2.2e-16\nalternative hypothesis: significant effects\n\n\n\n\nTwo-ways is preferred (pooled is biased)\n\n\n\n\n\n\n\n\npFtest(union_fe_time, union_pooled)\n\n\n    F test for time effects\n\ndata:  log(wage) ~ union + educ + exper + I(exper^2) + hours\nF = 154.34, df1 = 6, df2 = 4153, p-value < 2.2e-16\nalternative hypothesis: significant effects\n\n\n\n\nTime FE is preferred (pooled is biased)\n\n\n\n\nF-test leads us to stick with the FE individual, two-ways or time regression."
  },
  {
    "objectID": "slides/w08a-panel-regression.html#step-2.-lm-test-lagrange-multiplier-test-2",
    "href": "slides/w08a-panel-regression.html#step-2.-lm-test-lagrange-multiplier-test-2",
    "title": "Panel Regression Analysis",
    "section": "Step 2. LM test: Lagrange multiplier test (2)",
    "text": "Step 2. LM test: Lagrange multiplier test (2)\nWhich FE model to choose: individual, time or two-way?\n\n\n\nExist to compare FE models between each other assuming that:\n\nPooled is always consistent in pooled vs individual FE\nIndividual FE always consistent in individual FE vs time or two-way FE\n\nTest logic:\n\nH0: One model is inconsistent.\nH1: Both models are equally consistent.\n\n\n\n\nRun the test (one or another or both). Check p-value:\n\np-value < 0.05:\n\nIndividual FE is as good as pooled;\nTime or Two-ways model is as good as individual FE;\n\np-value >= 0.05: Pooled or individual FE is better than the alternative"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#step-2.-lm-testlagrange-multiplier-2",
    "href": "slides/w08a-panel-regression.html#step-2.-lm-testlagrange-multiplier-2",
    "title": "Panel Regression Analysis",
    "section": "Step 2. LM testLagrange multiplier (2)",
    "text": "Step 2. LM testLagrange multiplier (2)\n\n\n\nplmtest(union_pooled, effect = \"individual\")\n\n\n    Lagrange Multiplier Test - (Honda)\n\ndata:  log(wage) ~ union + educ + exper + I(exper^2) + hours\nnormal = 70.727, p-value < 2.2e-16\nalternative hypothesis: significant effects\n\nplmtest(union_pooled, effect = \"twoway\")\n\n\n    Lagrange Multiplier Test - two-ways effects (Honda)\n\ndata:  log(wage) ~ union + educ + exper + I(exper^2) + hours\nnormal = 186.47, p-value < 2.2e-16\nalternative hypothesis: significant effects\n\nplmtest(union_pooled, effect = \"time\")\n\n\n    Lagrange Multiplier Test - time effects (Honda)\n\ndata:  log(wage) ~ union + educ + exper + I(exper^2) + hours\nnormal = 192.98, p-value < 2.2e-16\nalternative hypothesis: significant effects\n\n\n\n\nIndividual FE is preferred (pooled is biased)\nIndividual FE and two-ways are both consistent. We can choose any of those two.\nIndividual FE and time FE are both consistent. We can choose any of those two.\n\n\n\n\nAll tests suggest that individual, time and two-ways fixed effect models are equally consistent."
  },
  {
    "objectID": "slides/w08a-panel-regression.html#step-3.-random-effect-model-individual",
    "href": "slides/w08a-panel-regression.html#step-3.-random-effect-model-individual",
    "title": "Panel Regression Analysis",
    "section": "Step 3. Random Effect model (individual)",
    "text": "Step 3. Random Effect model (individual)\n\nunion_rand_ind <- \n  plm(log(wage) ~ union + educ + exper + I(exper^2) + hours ,\n      data = wage_dta, model = \"random\", effect = \"individual\")\nunion_rand_ind\n\n\n\n\nModel Formula: log(wage) ~ union + educ + exper + I(exper^2) + hours\n\nCoefficients:\n(Intercept)       union        educ       exper  I(exper^2)       hours \n 3.80500063  0.05488827  0.11346029  0.08804403 -0.00077520  0.00095463"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#step-3.-hausman-test",
    "href": "slides/w08a-panel-regression.html#step-3.-hausman-test",
    "title": "Panel Regression Analysis",
    "section": "Step 3. Hausman test",
    "text": "Step 3. Hausman test\nWhich model to choose: Fixed effect or Random effect?\n\n\n\nCompares Fixed Effect model with Random Effect:\n\nFixed effect model is always consistent\n\nTest logic:\n\nH0: One model is inconsistent. Use FE!\nH1: Both models are equally consistent. RE is as good as FE.\n\n\n\n\nRun the test. Check the p-value.\n\np-value < 0.05: Use FE or RE, both are good.\np-value >= 0.05: Use FE, discard RE.\n\n\n\n\n\n\n\n\nphtest(union_fe_ind, union_rand_ind)\n\n\n    Hausman Test\n\ndata:  log(wage) ~ union + educ + exper + I(exper^2) + hours\nchisq = 6183.7, df = 4, p-value < 2.2e-16\nalternative hypothesis: one model is inconsistent\n\n\n\n\nFE is preferred instead of the RE model."
  },
  {
    "objectID": "slides/w08a-panel-regression.html#step-4.1-wooldridges-test-1",
    "href": "slides/w08a-panel-regression.html#step-4.1-wooldridges-test-1",
    "title": "Panel Regression Analysis",
    "section": "Step 4.1 Wooldridge’s test (1)",
    "text": "Step 4.1 Wooldridge’s test (1)\nIs there serial correlation / cross-sectional dependence in the data?\n\n\n\nWooldridge’s test for unobserved individual effects\n\nH0: no unobserved effects\nH1: some effects exist due to cross-sectional dependence and/or serial correlation\n\n\n\n\nRun the test Check the p-value.\n\np-value < 0.05: cross-sectional dependence and/or serial correlation are present\np-value >= 0.05: No cross-sectional dependency and/or serial correlation"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#step-4.1-wooldridges-test-2",
    "href": "slides/w08a-panel-regression.html#step-4.1-wooldridges-test-2",
    "title": "Panel Regression Analysis",
    "section": "Step 4.1 Wooldridge’s test (2)",
    "text": "Step 4.1 Wooldridge’s test (2)\n\n\n\npwtest(union_pooled, effect = \"individual\")\n\n\n    Wooldridge's test for unobserved individual effects\n\ndata:  formula\nz = 13.865, p-value < 2.2e-16\nalternative hypothesis: unobserved effect\n\npwtest(union_pooled, effect = \"time\")\n\n\n    Wooldridge's test for unobserved time effects\n\ndata:  formula\nz = 2.015, p-value = 0.04391\nalternative hypothesis: unobserved effect\n\n\n\n\ncross-sectional dependence is present\nserial correlation is present"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#step-4.2-lagrange-multiplier-tests-1",
    "href": "slides/w08a-panel-regression.html#step-4.2-lagrange-multiplier-tests-1",
    "title": "Panel Regression Analysis",
    "section": "Step 4.2 Lagrange-Multiplier tests (1)",
    "text": "Step 4.2 Lagrange-Multiplier tests (1)\nIs there serial correlation in the data?\n\n\n\nLocally–Robust Lagrange Multiplier Tests for serial correlation\n\nH0: serial correlation is zero\nH1: some serial correlation is present\n\n\n\n\nRun the test Check the p-value.\n\np-value < 0.05: serial correlation need to be addressed\np-value >= 0.05: no serial correlation"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#step-4.2-lagrange-multiplier-tests-2",
    "href": "slides/w08a-panel-regression.html#step-4.2-lagrange-multiplier-tests-2",
    "title": "Panel Regression Analysis",
    "section": "Step 4.2 Lagrange-Multiplier tests (2)",
    "text": "Step 4.2 Lagrange-Multiplier tests (2)\n\n\n\npbsytest(union_pooled, test = \"ar\")\n\n\n    Bera, Sosa-Escudero and Yoon locally robust test\n\ndata:  formula\nchisq = 608.98, df = 1, p-value < 2.2e-16\nalternative hypothesis: AR(1) errors sub random effects\n\n\n\n\nserial correlation is present"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#step-5.-robust-inference",
    "href": "slides/w08a-panel-regression.html#step-5.-robust-inference",
    "title": "Panel Regression Analysis",
    "section": "Step 5. Robust inference",
    "text": "Step 5. Robust inference\nSerial correlation and/or cross-sectional dependence render our Standard errors useless.\n\nCross-sectional dependence and/or serial correlation violate the variance homogeneity assumption:\n\nEstimates are unbiased, but inefficient.\nStandard errors need to be corrected.\n\n\n\nWe need to use:\n\nRobust Standard Errors, and/or\nClustered SE at the individual (group) level"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#step-5.-robust-standard-error-1",
    "href": "slides/w08a-panel-regression.html#step-5.-robust-standard-error-1",
    "title": "Panel Regression Analysis",
    "section": "Step 5. Robust Standard Error (1)",
    "text": "Step 5. Robust Standard Error (1)\n\nlibrary(lmtest)\nlibrary(car)\nlibrary(sandwich)\noptions(digits = 3, scipen = 6)\nunion_fe_ind\n\n\nModel Formula: log(wage) ~ union + educ + exper + I(exper^2) + hours\n\nCoefficients:\n     union      exper I(exper^2)      hours \n  0.030029   0.113705  -0.000423   0.000798 \n\n\nCorrecting cross-sectional dependence:\n\n\nRegular SE\n\ncoeftest(union_fe_ind, \n         vcov. = vcov(union_fe_ind))\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value Pr(>|t|)    \nunion       0.0300295  0.0148074    2.03    0.043 *  \nexper       0.1137052  0.0024681   46.07  < 2e-16 ***\nI(exper^2) -0.0004234  0.0000546   -7.75  1.2e-14 ***\nhours       0.0007980  0.0005997    1.33    0.183    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nRobust SE\n\ncoeftest(union_fe_ind, \n         vcov. = vcovHC(union_fe_ind, method = \"white1\", \n                        type = \"HC0\", cluster = \"group\"))\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value Pr(>|t|)    \nunion       0.0300295  0.0159202    1.89    0.059 .  \nexper       0.1137052  0.0025948   43.82  < 2e-16 ***\nI(exper^2) -0.0004234  0.0000539   -7.86  5.1e-15 ***\nhours       0.0007980  0.0007524    1.06    0.289    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#step-5.-robust-standard-error-2",
    "href": "slides/w08a-panel-regression.html#step-5.-robust-standard-error-2",
    "title": "Panel Regression Analysis",
    "section": "Step 5. Robust Standard Error (2)",
    "text": "Step 5. Robust Standard Error (2)\nWe produce new Variance-covariance matrix:\n\nvcovHC(union_fe_ind, \n       method = \"white1\", \n       type = \"HC0\", \n       cluster = \"group\")\n\n                   union         exper      I(exper^2)           hours\nunion       0.0002534526 -0.0000028527  0.000000054371 -0.000000443094\nexper      -0.0000028527  0.0000067329 -0.000000126361  0.000000043707\nI(exper^2)  0.0000000544 -0.0000001264  0.000000002902  0.000000000683\nhours      -0.0000004431  0.0000000437  0.000000000683  0.000000566054\nattr(,\"cluster\")\n[1] \"group\"\n\n\n\nmethods for cross–sectional dependence “white1” and “white2” and for cross–sectional dependence and autocorrelation “arellano”;\ntype for sample size correction: “HC0”, “sss”, “HC1”, “HC2”, “HC3”, “HC4” (“HC3” is recommended);\ncluster enabled by default (“group” or “time”);"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#step-6.-reporting-results-1",
    "href": "slides/w08a-panel-regression.html#step-6.-reporting-results-1",
    "title": "Panel Regression Analysis",
    "section": "Step 6. Reporting results (1)",
    "text": "Step 6. Reporting results (1)\n\npooled_robust <- \n  coeftest(union_pooled,\n           vcov. = vcovHC(union_pooled, method = \"arellano\",\n                          type = \"HC3\", cluster = \"group\"))\n\npooled_cs_robust <- \n  coeftest(union_fe_ind,\n           vcov. = vcovHC(union_fe_ind, method = \"white1\", \n                          type = \"HC0\", cluster = \"group\"))\n\npooled_csac_robust <- \n  coeftest(union_fe_ind,\n           vcov. = vcovHC(union_fe_ind, method = \"arellano\", \n                          type = \"HC3\", cluster = \"group\"))\n\n\nmodelsummary(\n  list(\n    `Pooled (no SE correction)` = coeftest(union_pooled),\n    `Pooled (c/s dep. and aut.)` = pooled_robust,\n    `Ind. FE (no SE correction)` = coeftest(union_fe_ind),\n    `Ind. FE (c/s dep.)` = pooled_cs_robust, \n    `Ind. FE (c/s dep. and aut.)` = pooled_csac_robust\n    ), \n  fmt = 4, statistic = NULL,\n  estimate = \"{estimate}{stars} ({std.error})\")"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#step-6.-reporting-results-1-1",
    "href": "slides/w08a-panel-regression.html#step-6.-reporting-results-1-1",
    "title": "Panel Regression Analysis",
    "section": "Step 6. Reporting results (1)",
    "text": "Step 6. Reporting results (1)\n\n\n\n\n \n  \n      \n    Pooled (no SE correction) \n    Pooled (c/s dep. and aut.) \n     Ind. FE (no SE correction) \n     Ind. FE (c/s dep.) \n     Ind. FE (c/s dep. and aut.) \n  \n \n\n  \n    (Intercept) \n    4.7054*** (0.0699) \n    4.7054*** (0.1383) \n     \n     \n     \n  \n  \n    union \n    0.1261*** (0.0131) \n    0.1261*** (0.0255) \n    0.0300* (0.0148) \n    0.0300+ (0.0159) \n    0.0300 (0.0256) \n  \n  \n    educ \n    0.0820*** (0.0023) \n    0.0820*** (0.0052) \n     \n     \n     \n  \n  \n    exper \n    0.0437*** (0.0024) \n    0.0437*** (0.0053) \n    0.1137*** (0.0025) \n    0.1137*** (0.0026) \n    0.1137*** (0.0040) \n  \n  \n    I(exper^2) \n    −0.0007*** (0.0001) \n    −0.0007*** (0.0001) \n    −0.0004*** (0.0001) \n    −0.0004*** (0.0001) \n    −0.0004*** (0.0001) \n  \n  \n    hours \n    0.0077*** (0.0012) \n    0.0077*** (0.0019) \n    0.0008 (0.0006) \n    0.0008 (0.0008) \n    0.0008 (0.0009) \n  \n  \n    Num.Obs. \n    4165 \n    4165 \n    4165 \n    4165 \n    4165 \n  \n  \n    AIC \n    3911.1 \n    3911.1 \n    −4502.1 \n    −4502.1 \n    −4502.1 \n  \n  \n    BIC \n    3955.5 \n    3955.5 \n    −4470.5 \n    −4470.5 \n    −4470.5 \n  \n  \n    Log.Lik. \n    -1948.564 \n    -1948.564 \n    2256.066 \n    2256.066 \n    2256.066"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#step-6.-reporting-gof-1",
    "href": "slides/w08a-panel-regression.html#step-6.-reporting-gof-1",
    "title": "Panel Regression Analysis",
    "section": "Step 6. Reporting GOF (1)",
    "text": "Step 6. Reporting GOF (1)\n\nlibrary(performance)\ncompare_performance(list(Pooled = union_pooled, FE = union_fe_ind))\n\n\n\n# Comparison of Model Performance Indices\n\nName   | Model |   AIC (weights) |  AICc (weights) |   BIC (weights) |    R2 | R2 (adj.) |  RMSE | Sigma\n--------------------------------------------------------------------------------------------------------\nPooled |   plm | 59525.1 (<.001) | 59525.1 (<.001) | 59569.4 (<.001) | 0.299 |     0.298 | 0.386 | 0.387\nFE     |   plm | 51111.8 (>.999) | 51111.8 (>.999) | 51143.5 (>.999) | 0.657 |     0.600 | 0.141 | 0.141"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#takeaways-for-the-exam",
    "href": "slides/w08a-panel-regression.html#takeaways-for-the-exam",
    "title": "Panel Regression Analysis",
    "section": "Takeaways for the exam",
    "text": "Takeaways for the exam\n\nSimpson’s paradox. What are the causes of it and solutions.\nData types (cross-section, repeated cross-section, balanced panel, unbalanced panel)\nPanel Regression\n\nPooled;\nLeast Squared Dummy Variable model;\nFixed effect (within transformation);\nWhy FE is so important?\nWhat is the key difference between FE and RE?\nWhen FE and when RE are appropriate?\n\nPanel Regression tests F-test, LM-test, Hausman test\nRobust and Clustered SE:\n\nWhy these are important and when do we need to use one?"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#homework",
    "href": "slides/w08a-panel-regression.html#homework",
    "title": "Panel Regression Analysis",
    "section": "Homework",
    "text": "Homework\n\nReproduce code from the slides\nPerform practical exercises."
  },
  {
    "objectID": "slides/w08a-panel-regression.html#references",
    "href": "slides/w08a-panel-regression.html#references",
    "title": "Panel Regression Analysis",
    "section": "References",
    "text": "References\n\n\n\nMP223-Applied Econometrics (SoSe 2023). Author: Eduard Bukin (2023). | GitHub | StudIP | ILIAS.\n\n\n\nAngrist, J. D., & Pischke, J.-S. (2009). Mostly harmless econometrics. Princeton University Press. http://doi.org/10.1515/9781400829828\n\n\nBlanc, E., & Schlenker, W. (2017). The use of panel models in assessments of climate impacts on agriculture. Review of Environmental Economics and Policy, 11(2), 258–279. http://doi.org/10.1093/reep/rex016\n\n\nBozzola, M., Massetti, E., Mendelsohn, R., & Capitanio, F. (2017). A ricardian analysis of the impact of climate change on italian agriculture. European Review of Agricultural Economics, 45(1), 57–79. http://doi.org/10.1093/erae/jbx023\n\n\nCard, D. (1996). The effect of unions on the structure of wages: A longitudinal analysis. Econometrica, 64(4), 957. http://doi.org/10.2307/2171852\n\n\nChatzopoulos, T., & Lippert, C. (2015). Endogenous farm-type selection, endogenous irrigation, and spatial effects in ricardian models of climate change. European Review of Agricultural Economics, 43(2), 217–235. http://doi.org/10.1093/erae/jbv014\n\n\nConley, T. G. (1999). GMM estimation with cross sectional dependence. Journal of Econometrics, 92(1), 1–45. http://doi.org/10.1016/s0304-4076(98)00084-0\n\n\nCroissant, Y., & Millo, G. (2018). Panel data econometrics with r. John Wiley & Sons.\n\n\nFreeman, R. B. (1984). Longitudinal analyses of the effects of trade unions. Journal of Labor Economics, 2(1), 1–26. http://doi.org/10.1086/298021\n\n\nKurukulasuriya, P., Kala, N., & Mendelsohn, R. (2011). Adaptation and climate change impacts: A structural ricardian model of irrigation and farm income in africa. Climate Change Economics, 2(02), 149–174.\n\n\nKurukulasuriya, P., Mendelsohn, R., Kurukulasuriya, P., & Mendelsohn, R. (2008). Crop switching as a strategy for adapting to climate change. http://doi.org/10.22004/AG.ECON.56970\n\n\nMendelsohn, R., Nordhaus, W. D., & Shaw, D. (1994). The impact of global warming on agriculture: A ricardian analysis. The American Economic Review, 753–771.\n\n\nMundlak, Y. (1961). Empirical production function free of management bias. Journal of Farm Economics, 43(1), 44. http://doi.org/10.2307/1235460\n\n\nSeo, S. N., & Mendelsohn, R. (2008a). An analysis of crop choice: Adapting to climate change in south american farms. Ecological Economics, 67(1), 109–116. http://doi.org/10.1016/j.ecolecon.2007.12.007\n\n\nSeo, S. N., & Mendelsohn, R. (2008b). Measuring impacts and adaptations to climate change: A structural ricardian model of african livestock management. Agricultural Economics, 38(2), 151–165. http://doi.org/10.1111/j.1574-0862.2008.00289.x\n\n\nSeo, S. N., Mendelsohn, R., Seo, S. N., & Mendelsohn, R. (2008). Animal husbandry in africa: Climate change impacts and adaptations. http://doi.org/10.22004/AG.ECON.56968\n\n\nSöderbom, M., Teal, F., & Eberhardt, M. (2014). Empirical development economics. ROUTLEDGE. Retrieved from https://www.ebook.de/de/product/21466458/mans_soederbom_francis_teal_markus_eberhardt_empirical_development_economics.html\n\n\nWooldridge, J. M. (2010). Econometric analysis of cross section and panel data. MIT press.\n\n\nWooldridge, M. J. (2020). Introductory econometrics: A modern approach. South-Western. Retrieved from https://www.cengage.uk/shop/isbn/9781337558860"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#problem-setting-1",
    "href": "slides/w08a-panel-regression.html#problem-setting-1",
    "title": "Panel Regression Analysis",
    "section": "Problem setting",
    "text": "Problem setting\nDoes the collective bargaining (union membership) has any effect on wages?\n\nSee: (Card, 1996; Freeman, 1984)\n\n\n\\[\nlog(\\text{Wage}_{it}) = \\beta_0 + \\beta_1 \\cdot \\text{Union}_{it} + \\beta_2 \\cdot {X_{it}} + \\beta_3 \\cdot \\text{Ability}_{i} + \\epsilon_{it}\n\\]\nwhere \\(i\\) is the individual and \\(t\\) is the time dimension;"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#part-1.-theoretical-basis",
    "href": "slides/w08b-panel-regression-production-function.html#part-1.-theoretical-basis",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Part 1. Theoretical basis",
    "text": "Part 1. Theoretical basis\nWe employ the Cobb-Douglas Production function:\n\\[\n\\ln y = \\ln \\beta_0 + \\sum_{n = 1}^{N}  \\beta_n \\ln x_n + \\sum_{k = 1}^{K} \\gamma_k \\delta_k + \\epsilon\n\\]\nwhere,\n\n\\(y\\) is the output and \\(x_n\\) are the inputs all in physical mass (or monetary value);\n\\(N\\) is the number of independent variables;\n\\(\\delta_k\\) are the shift parameters of additional dummy variables;\n\\(\\beta_0\\) , \\(\\beta_n\\) , \\(\\gamma_n\\) are the estimated coefficients;"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#estimation-strategy.-part-1.",
    "href": "slides/w08b-panel-regression-production-function.html#estimation-strategy.-part-1.",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Estimation strategy. Part 1.",
    "text": "Estimation strategy. Part 1.\nPooled OLS production function:\n\\[\n\\begin{aligned}\n\\ln \\text{output}_{it} & = A_0 + \\beta_1 \\cdot \\ln \\text{land}_{it}\n+ \\beta_2 \\cdot \\ln \\text{labor}_{it} \\\\\n& + \\beta_3 \\cdot \\ln \\text{seed}_{it} + \\beta_4 \\cdot \\ln  \\text{urea}_{it} \\\\\n& + \\beta_5 \\cdot \\ln \\text{pesticide}_{it} + e_{it}\n\\end{aligned}\n\\]\n\nWhat are the ex-ante expectations about the regression coefficients?\n\nIdeas? …\nProbably all \\(\\beta\\) should be positive."
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#what-about-ovb-3",
    "href": "slides/w08b-panel-regression-production-function.html#what-about-ovb-3",
    "title": "Panel Regression Analysis: Micro application",
    "section": "What about OVB? (3)",
    "text": "What about OVB? (3)\n\nWhat omitted variables could cause bias of our estimates?\n\nAny!? …\nAny!? …\nAny OVB!? …\nCapital, Ability, Climate, Geography…"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#applying-the-ovb-formula-1",
    "href": "slides/w08b-panel-regression-production-function.html#applying-the-ovb-formula-1",
    "title": "Panel Regression Analysis: Micro application",
    "section": "Applying the OVB formula (1)",
    "text": "Applying the OVB formula (1)\n\nLet us make an educated guess about the effect of OVB on the estimates of land-related coefficient \\(\\beta_1\\)?\n\n\n\n\nShort model:\nLong model:\nAuxiliary regression\nOVB formula:\n\n\n\n\\(\\text{output}_{it} = A_0^s + \\beta_1^s \\cdot \\text{land}_{it} + \\beta_2^s \\cdot \\text{labor}_{it} + \\beta_3^s \\cdot \\text{seed}_{it} + \\beta_4^s \\cdot \\text{urea}_{it} + \\beta_5^s \\cdot \\text{pesticide}_{it} + e_{it}\\)\n\\(\\text{output}_{it} = A_0 + \\beta_1 \\cdot \\text{land}_{it} + \\beta_2 \\cdot \\text{labor}_{it} + \\beta_3 \\cdot \\text{seed}_{it} + \\beta_4 \\cdot \\text{urea}_{it} + \\beta_5 \\cdot \\text{pesticide}_{it} + \\gamma \\text{Ability}_i + e_{it}\\)\n\\(\\text{Ability}_i = \\pi_0 + \\pi_1 \\cdot \\text{land}_{it} + \\pi_2 \\cdot \\text{labor}_{it} + \\pi_3 \\cdot \\text{seed}_{it} + \\pi_4 \\cdot \\text{urea}_{it} + \\pi_5 \\cdot \\text{pesticide}_{it} + e_{it}\\)\n\\(\\text{OVB}_{\\text{land}} = \\beta_1^s - \\beta_1 = \\pi_1 * \\gamma\\)"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#applying-the-ovb-formula-2",
    "href": "slides/w08b-panel-regression-production-function.html#applying-the-ovb-formula-2",
    "title": "Panel Regression Analysis: Micro application",
    "section": "Applying the OVB formula (2)",
    "text": "Applying the OVB formula (2)\n\nEducated guess about bias of the estimates.\n\n\\(\\pi_1 > 0\\)\n\\(\\gamma > 0\\)\n\\(\\text{OVB} = (+) \\times (+) > 0\\)\n\nNot controlling for the omitted ability may cause overestimation of the effect of the farm size \\(\\beta_1\\)."
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#how-to-resolve-the-ovb",
    "href": "slides/w08b-panel-regression-production-function.html#how-to-resolve-the-ovb",
    "title": "Panel Regression Analysis: Micro application",
    "section": "How to resolve the OVB?",
    "text": "How to resolve the OVB?\n\nIntroduce a proxy variable for ability?\n\nNo such data.\n\nRely on the panel structure of the data.\n\nUse the individual fixed effect model, for example."
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#estimation-strategy.-part-2.",
    "href": "slides/w08b-panel-regression-production-function.html#estimation-strategy.-part-2.",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Estimation strategy. Part 2.",
    "text": "Estimation strategy. Part 2.\nIndividual fixed effect panel regression production function.\n\\[\n\\begin{aligned}\n\\ln \\text{output}_{it} & = A_0 + \\beta_1 \\cdot \\ln \\text{land}_{it}\n+ \\beta_2 \\cdot \\ln \\text{labor}_{it} \\\\\n& + \\beta_3 \\cdot \\ln \\text{seed}_{it} + \\beta_4 \\cdot \\ln  \\text{urea}_{it} \\\\\n& + \\beta_5 \\cdot \\ln \\text{pesticide}_{it} \\\\\n& + \\color{Red}{\\alpha_i} + e_{it}\n\\end{aligned}\n\\]\n\nWhat are the ex-ante expectations about the regression coefficients?\n\nIdeas? …\nProbably the same as before: all \\(\\beta\\) should be positive."
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#part-2.-exploratory-data-analysis-and-data-description",
    "href": "slides/w08b-panel-regression-production-function.html#part-2.-exploratory-data-analysis-and-data-description",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Part 2. Exploratory data analysis and data description",
    "text": "Part 2. Exploratory data analysis and data description\nWe operate a farm-level data with following variables:\n\noutput - gross output of rice in kg\nland - the total area cultivated with rice, measured in hectares\nseed - seed in kilogram\nurea - urea in kilogram\npesticide - urea in kilogram\nlabor - total labor in hours (excluding harvest labor)"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#data-preparation",
    "href": "slides/w08b-panel-regression-production-function.html#data-preparation",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Data preparation",
    "text": "Data preparation\nLoad data and calculate rice yields and logs of all variables.\n\nlibrary(tidyverse)\nlibrary(plm)\nlibrary(modelsummary)\nlibrary(performance)\nlibrary(lmtest)\n\nfarm_dta <- read_csv(\"farm_panel.csv\")\n\nglimpse(farm_dta)\n\n\n\nRows: 1,026\nColumns: 15\n$ id         <dbl> 101001, 101001, 101001, 101001, 101001, 101001, 101017, 101…\n$ time       <dbl> 1999, 2000, 2001, 2002, 2003, 2004, 1999, 2000, 2001, 2002,…\n$ output     <dbl> 7980, 4083, 2650, 4500, 16300, 17424, 3840, 2800, 950, 240,…\n$ land       <dbl> 3.000, 2.000, 1.000, 2.000, 3.572, 3.572, 1.420, 1.420, 0.4…\n$ labor      <dbl> 2915, 2155, 1075, 2091, 3889, 3519, 810, 855, 460, 109, 230…\n$ hiredlabor <dbl> 2875, 2110, 980, 2081, 3889, 3519, 670, 805, 380, 40, 210, …\n$ famlabor   <dbl> 40, 45, 95, 10, 1, 1, 140, 50, 80, 69, 20, 1, 108, 63, 57, …\n$ seed       <dbl> 90, 40, 100, 60, 105, 105, 50, 20, 15, 7, 15, 15, 5, 10, 10…\n$ urea       <dbl> 900, 600, 700, 600, 400, 400, 120, 100, 150, 50, 100, 100, …\n$ pest       <dbl> 6000, 3000, 5000, 5000, 10200, 10200, 0, 0, 900, 0, 2000, 2…\n$ varieties  <chr> \"mixed\", \"trad\", \"high\", \"high\", \"high\", \"high\", \"trad\", \"h…\n$ status     <chr> \"owner\", \"owner\", \"owner\", \"owner\", \"share\", \"share\", \"mixe…\n$ bimas      <chr> \"mixed\", \"mixed\", \"mixed\", \"mixed\", \"no\", \"no\", \"mixed\", \"m…\n$ region     <chr> \"wargabinangun\", \"wargabinangun\", \"wargabinangun\", \"wargabi…\n$ price      <dbl> 60, 60, 65, 70, 120, 140, 60, 50, 62, 60, 110, 130, 65, 50,…"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#computing-yeilds-and-logs",
    "href": "slides/w08b-panel-regression-production-function.html#computing-yeilds-and-logs",
    "title": "Panel Regression Analysis: Micro application",
    "section": "Computing yeilds and logs",
    "text": "Computing yeilds and logs\n\nfarm_dta_log <- farm_dta %>%\n  mutate(\n    l_output = log(output),\n    l_land = log(land),\n    l_seed = log(seed),\n    l_urea = log(urea),\n    l_pest = log(pest),\n    l_labor = log(labor),\n    yields_mt_ha = output / land / 1000\n  )"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#sumary-statistics",
    "href": "slides/w08b-panel-regression-production-function.html#sumary-statistics",
    "title": "Panel Regression Analysis: Micro application",
    "section": "Sumary statistics",
    "text": "Sumary statistics\n\nn_inf <- function(x) sum(is.infinite(x))\nn_missing <- function(x) sum(is.na(x)|is.nan(x))\ndatasummary(\n  l_output + l_land + l_seed + l_urea + l_pest + l_labor + yields_mt_ha ~ \n    N + n_missing + n_inf + Mean + SD + Median + Min + Max,\n  data = farm_dta_log)\n\n\n\n \n  \n      \n    N \n    n_missing \n    n_inf \n    Mean \n    SD \n    Median \n    Min \n    Max \n  \n \n\n  \n    l_output \n    1026 \n    0.00 \n    0.00 \n    6.73 \n    0.99 \n    6.79 \n    3.74 \n    9.95 \n  \n  \n    l_land \n    1026 \n    0.00 \n    0.00 \n    −1.30 \n    0.95 \n    −1.25 \n    −4.61 \n    1.67 \n  \n  \n    l_seed \n    1026 \n    0.00 \n    0.00 \n    2.37 \n    0.94 \n    2.30 \n    0.00 \n    7.13 \n  \n  \n    l_urea \n    1026 \n    0.00 \n    0.00 \n    3.98 \n    1.16 \n    4.09 \n    0.00 \n    7.13 \n  \n  \n    l_pest \n    1026 \n    0.00 \n    713.00 \n     \n     \n     \n     \n    11.04 \n  \n  \n    l_labor \n    1026 \n    0.00 \n    0.00 \n    5.56 \n    0.85 \n    5.53 \n    2.83 \n    8.47 \n  \n  \n    yields_mt_ha \n    1026 \n    0.00 \n    0.00 \n    3.38 \n    1.68 \n    3.16 \n    0.40 \n    27.50 \n  \n\n\n\n\n\n\nAny problems with data?"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#any-problems-with-data",
    "href": "slides/w08b-panel-regression-production-function.html#any-problems-with-data",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Any problems with data?",
    "text": "Any problems with data?\n\nAny? …\npest, when transformed with logs, produces -Inf values.\n\nWhy is that so?\nAny? …\nBecause there are zero values of pesticides application \\(\\ln 0 = - \\infty\\).\n\nHow to resolve the \\(\\ln 0\\) problem?"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#infinity-in-logs-lazy-solution",
    "href": "slides/w08b-panel-regression-production-function.html#infinity-in-logs-lazy-solution",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "-Infinity in logs: lazy solution",
    "text": "-Infinity in logs: lazy solution\n\nBefore log transformation, substitute any zero with a small value, for example 0.0001;\n\n\n\nfarm_dta_log <- farm_dta_log %>%\n  mutate(l_pest_lazy = ifelse(is.infinite(l_pest), log(0.0001), l_pest))"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#infinity-in-logs-smart-solution",
    "href": "slides/w08b-panel-regression-production-function.html#infinity-in-logs-smart-solution",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "-Infinity in logs: smart solution",
    "text": "-Infinity in logs: smart solution\n\nIntroduce reverse dummy variables for each variable with log of zero, see: Battese (1997);\nSubstitute negative infinity with zero. . . .\n\n\nfarm_dta_log <- farm_dta_log %>%\n  mutate(pest_revdum = ifelse(is.infinite(l_pest), 1, 0),\n         l_pest_smart = ifelse(is.infinite(l_pest), 0, l_pest))"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#summary-staistics-after-data-cleaning",
    "href": "slides/w08b-panel-regression-production-function.html#summary-staistics-after-data-cleaning",
    "title": "Panel Regression Analysis: Micro application",
    "section": "Summary staistics after data cleaning",
    "text": "Summary staistics after data cleaning\n\ndatasummary(l_pest + l_pest_lazy + l_pest_smart +  pest_revdum ~ \n    N + n_missing + n_inf + Mean + SD + Median + Min + Max, \n    data = farm_dta_log)\n\n\n\n \n  \n      \n    N \n    n_missing \n    n_inf \n    Mean \n    SD \n    Median \n    Min \n    Max \n  \n \n\n  \n    l_pest \n    1026 \n    0.00 \n    713.00 \n     \n     \n     \n     \n    11.04 \n  \n  \n    l_pest_lazy \n    1026 \n    0.00 \n    0.00 \n    −4.37 \n    7.35 \n    −9.21 \n    −9.21 \n    11.04 \n  \n  \n    l_pest_smart \n    1026 \n    0.00 \n    0.00 \n    2.04 \n    3.15 \n    0.00 \n    0.00 \n    11.04 \n  \n  \n    pest_revdum \n    1026 \n    0.00 \n    0.00 \n    0.69 \n    0.46 \n    1.00 \n    0.00 \n    1.00"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#farm-size-vs-rice-yields",
    "href": "slides/w08b-panel-regression-production-function.html#farm-size-vs-rice-yields",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Farm size vs rice yields",
    "text": "Farm size vs rice yields\n\nyield_size <- \n  farm_dta_log %>% \n  ggplot() + \n  aes(x = land, y = yields_mt_ha) + \n  geom_point() + \n  geom_smooth() + \n  scale_x_log10(\"Farm size, ha\", \n                breaks = c(0.01, 0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10)) + \n  scale_y_log10(\"Rice yeilds, MT/ha\", \n                breaks = c(0.2, 0.5, 1, 2, 3, 4, 5, 7, 10, 15, 20, 30)) + \n  annotation_logticks() + \n  labs(title = \"Relationship between farm size and yield of rice\",\n       caption = \"Loess non-parametric smoothing line highlights the trend\")\nyield_size"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#farm-size-vs-rice-yields-output",
    "href": "slides/w08b-panel-regression-production-function.html#farm-size-vs-rice-yields-output",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Farm size vs rice yields",
    "text": "Farm size vs rice yields"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#farm-size-vs-rice-yields-by-year",
    "href": "slides/w08b-panel-regression-production-function.html#farm-size-vs-rice-yields-by-year",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Farm size vs rice yields by year",
    "text": "Farm size vs rice yields by year\n\nyield_size + facet_wrap(. ~ time)"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#part-3.-estimating-the-models",
    "href": "slides/w08b-panel-regression-production-function.html#part-3.-estimating-the-models",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Part 3. Estimating the models",
    "text": "Part 3. Estimating the models\nStep 1. Pooled OLS (with lm() and plm() functions);\n\nValidate all assumptions (linearity, collinearity, homogeneity)\n\nStep 2. Fixed Effect and Random Effect models (with plm());\n\nChoose a consistent model (models) relying on: F-test (pftest()), Lagrange multiplier test (plmtest()), Hausman test (phtest());\n\nStep 3. Validate homogeneity assumption (cross-sectional dependency and autocorrelation)\n\nWooldridge’s test (pwtest()) and Lagrange-Multiplier tests (pbsytest()).\n\nStep 4. Robust inference and results interpretation."
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#step-1-pooled-ols",
    "href": "slides/w08b-panel-regression-production-function.html#step-1-pooled-ols",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Step 1 Pooled OLS",
    "text": "Step 1 Pooled OLS\n\nrice_pooled <- \n  plm(l_output ~ l_land + l_labor + l_seed + l_urea + \n        l_pest_smart + pest_revdum, \n      data = farm_dta_log, model = \"pooling\", index = c(\"id\", \"time\"))\n\nrice_pooled_2 <- \n  lm(l_output ~ l_land + l_labor + l_seed + l_urea + \n       l_pest_smart + pest_revdum, \n      data = farm_dta_log)\n\nrice_pooled\n\n\nModel Formula: l_output ~ l_land + l_labor + l_seed + l_urea + l_pest_smart + \n    pest_revdum\n\nCoefficients:\n (Intercept)       l_land      l_labor       l_seed       l_urea l_pest_smart \n    4.314437     0.434792     0.253482     0.148404     0.171475     0.091171 \n pest_revdum \n    0.512910"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#step-1-linearity-and-homoscedasticity",
    "href": "slides/w08b-panel-regression-production-function.html#step-1-linearity-and-homoscedasticity",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Step 1 Linearity and homoscedasticity",
    "text": "Step 1 Linearity and homoscedasticity\n\ncheck_model(rice_pooled_2, check = c(\"linearity\", \"homogeneity\"))"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#step-1-collinearity",
    "href": "slides/w08b-panel-regression-production-function.html#step-1-collinearity",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Step 1 Collinearity",
    "text": "Step 1 Collinearity\n\ncheck_collinearity(rice_pooled_2)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n    Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n l_labor 4.77 [ 4.28,  5.32]         2.18      0.21     [0.19, 0.23]\n  l_urea 2.67 [ 2.42,  2.95]         1.63      0.37     [0.34, 0.41]\n\nModerate Correlation\n\n   Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n l_land 6.94 [ 6.21,  7.78]         2.64      0.14     [0.13, 0.16]\n l_seed 5.07 [ 4.55,  5.66]         2.25      0.20     [0.18, 0.22]\n\nHigh Correlation\n\n         Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n l_pest_smart 25.04 [22.24, 28.20]         5.00      0.04     [0.04, 0.04]\n  pest_revdum 24.07 [21.38, 27.11]         4.91      0.04     [0.04, 0.05]"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#step-2-fixed-effect",
    "href": "slides/w08b-panel-regression-production-function.html#step-2-fixed-effect",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Step 2 Fixed Effect",
    "text": "Step 2 Fixed Effect\n\nrice_fe <- \n  plm(l_output ~ l_land + l_labor + l_seed + l_urea + \n        l_pest_smart + pest_revdum, \n      data = farm_dta_log,\n      model = \"within\", \n      effect = \"individual\", \n      index = c(\"id\", \"time\"))\nrice_fe\n\n\nModel Formula: l_output ~ l_land + l_labor + l_seed + l_urea + l_pest_smart + \n    pest_revdum\n\nCoefficients:\n      l_land      l_labor       l_seed       l_urea l_pest_smart  pest_revdum \n     0.41984      0.27084      0.12381      0.16291      0.11029      0.63498"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#step-2-fe-with-lazy-log0",
    "href": "slides/w08b-panel-regression-production-function.html#step-2-fe-with-lazy-log0",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Step 2 FE with lazy log(0)",
    "text": "Step 2 FE with lazy log(0)\n\nrice_fe_lazy <- \n  plm(l_output ~ l_land + l_labor + l_seed + l_urea + l_pest_lazy , \n      data = farm_dta_log,\n      model = \"within\",\n      effect = \"individual\",\n      index = c(\"id\", \"time\"))\nrice_fe_lazy\n\n\nModel Formula: l_output ~ l_land + l_labor + l_seed + l_urea + l_pest_lazy\n\nCoefficients:\n     l_land     l_labor      l_seed      l_urea l_pest_lazy \n   0.421042    0.263876    0.134373    0.174145    0.007703"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#step-2.2-f-test-for-individual-effects",
    "href": "slides/w08b-panel-regression-production-function.html#step-2.2-f-test-for-individual-effects",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Step 2.2 F test for individual effects",
    "text": "Step 2.2 F test for individual effects\n\nCompares FE model to OLS. OLS is always consistent, when Gauss-Markov assumptions are satisfied.\n\nH0: One model is inconsistent.\nH1: Both models are equally consistent.\n\n\n\npFtest(rice_fe, rice_pooled)\n\n\n    F test for individual effects\n\ndata:  l_output ~ l_land + l_labor + l_seed + l_urea + l_pest_smart +  ...\nF = 1.4988, df1 = 170, df2 = 849, p-value = 0.0001704\nalternative hypothesis: significant effects"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#step-2.3-lagrange-multiplier-tests",
    "href": "slides/w08b-panel-regression-production-function.html#step-2.3-lagrange-multiplier-tests",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Step 2.3 Lagrange Multiplier Tests",
    "text": "Step 2.3 Lagrange Multiplier Tests\n\nCompares FE model to OLS. OLS is always consistent, when Gauss-Markov assumptions are satisfied.\n\nH0: One model is inconsistent.\nH1: Both models are equally consistent.\n\n\n\nplmtest(rice_pooled, effect = \"individual\", type = \"honda\")\n\n\n    Lagrange Multiplier Test - (Honda)\n\ndata:  l_output ~ l_land + l_labor + l_seed + l_urea + l_pest_smart +  ...\nnormal = 3.7129, p-value = 0.0001025\nalternative hypothesis: significant effects\n\nplmtest(rice_pooled, effect = \"individual\", type = \"bp\")\n\n\n    Lagrange Multiplier Test - (Breusch-Pagan)\n\ndata:  l_output ~ l_land + l_labor + l_seed + l_urea + l_pest_smart +  ...\nchisq = 13.785, df = 1, p-value = 0.0002049\nalternative hypothesis: significant effects"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#step-2-random-effect",
    "href": "slides/w08b-panel-regression-production-function.html#step-2-random-effect",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Step 2 Random Effect",
    "text": "Step 2 Random Effect\n\nrice_re <- \n  plm(l_output ~ l_land + l_labor + l_seed + l_urea + \n        l_pest_smart + pest_revdum, \n      data = farm_dta_log,\n      model = \"random\",\n      effect = \"individual\",\n      index = c(\"id\", \"time\"))\nrice_re\n\n\nModel Formula: l_output ~ l_land + l_labor + l_seed + l_urea + l_pest_smart + \n    pest_revdum\n\nCoefficients:\n (Intercept)       l_land      l_labor       l_seed       l_urea l_pest_smart \n    4.276601     0.433744     0.257738     0.142589     0.169423     0.096608 \n pest_revdum \n    0.546997"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#step-2-hausman-test-for-panel-models",
    "href": "slides/w08b-panel-regression-production-function.html#step-2-hausman-test-for-panel-models",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Step 2 Hausman Test for Panel Models",
    "text": "Step 2 Hausman Test for Panel Models\n\nCompares RE to FE model. FE is assumed to be consistent\n\nH0: One model is inconsistent.\nH1: Both models are equally consistent.\n\n\n\n\nphtest(rice_fe, rice_re)\n\n\n    Hausman Test\n\ndata:  l_output ~ l_land + l_labor + l_seed + l_urea + l_pest_smart +  ...\nchisq = 7.285, df = 6, p-value = 0.2953\nalternative hypothesis: one model is inconsistent\n\n\n\nFixed Effect model is recommended"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#step-3-serial-correlation-and-cross-sectional-dependence",
    "href": "slides/w08b-panel-regression-production-function.html#step-3-serial-correlation-and-cross-sectional-dependence",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Step 3 Serial correlation and cross-sectional dependence",
    "text": "Step 3 Serial correlation and cross-sectional dependence\n\nWooldridge’s test for unobserved individual effects\n\nH0: no unobserved effects\nH1: some effects also dues to serial correlation\n\n\n\npwtest(rice_pooled, effect = \"individual\")\n\n\n    Wooldridge's test for unobserved individual effects\n\ndata:  formula\nz = 2.1603, p-value = 0.03075\nalternative hypothesis: unobserved effect\n\npwtest(rice_pooled, effect = \"time\")\n\n\n    Wooldridge's test for unobserved time effects\n\ndata:  formula\nz = 1.6899, p-value = 0.09105\nalternative hypothesis: unobserved effect"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#step-3-lm-tests-for-random-effects-andor-serial-correlation",
    "href": "slides/w08b-panel-regression-production-function.html#step-3-lm-tests-for-random-effects-andor-serial-correlation",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Step 3 lm tests for random effects and/or serial correlation",
    "text": "Step 3 lm tests for random effects and/or serial correlation\n\nH0: serial correlation is zero\nH1: some serial correlation\n\n\n\npbsytest(rice_pooled)\n\n\n    Bera, Sosa-Escudero and Yoon locally robust test\n\ndata:  formula\nchisq = 20.988, df = 1, p-value = 4.622e-06\nalternative hypothesis: AR(1) errors sub random effects"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#step-4.-robust-inference",
    "href": "slides/w08b-panel-regression-production-function.html#step-4.-robust-inference",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Step 4. Robust inference",
    "text": "Step 4. Robust inference\n\nlibrary(lmtest)\n\nrice_pooled_robust <- coeftest(\n  rice_pooled,\n  vcovHC(rice_pooled, method = \"arellano\", type = \"HC3\", cluster = \"group\")\n)\n\nrice_fe_robust <- coeftest(\n  rice_fe,\n  vcovHC(rice_fe, method = \"arellano\", type = \"HC3\", cluster = \"group\")\n)\n\nrice_felazy_robust <- coeftest(\n  rice_fe_lazy,\n  vcovHC(rice_fe_lazy, method = \"arellano\", type = \"HC3\", cluster = \"group\")\n)"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#step-4.-robust-inference-1",
    "href": "slides/w08b-panel-regression-production-function.html#step-4.-robust-inference-1",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Step 4. Robust inference",
    "text": "Step 4. Robust inference\n\nmodelsummary(\n  list(\n    `Pooled` = rice_pooled_robust,\n    `FE (rev. dum.)` = rice_fe_robust,\n    `FE (lazy)` = rice_felazy_robust\n    ), \n  fmt = 4, statistic = NULL,\n  estimate = \"{estimate}{stars} ({std.error})\",\n  notes = \"Robust standard errors clustered at the group level are reported in the brackets.\")\n\n\n\n \n  \n      \n    Pooled \n    FE (rev. dum.) \n    FE (lazy) \n  \n \n\n  \n    (Intercept) \n    4.3144*** (0.2691) \n     \n     \n  \n  \n    l_land \n    0.4348*** (0.0418) \n    0.4198*** (0.0479) \n    0.4210*** (0.0486) \n  \n  \n    l_labor \n    0.2535*** (0.0333) \n    0.2708*** (0.0347) \n    0.2639*** (0.0348) \n  \n  \n    l_seed \n    0.1484*** (0.0344) \n    0.1238*** (0.0370) \n    0.1344*** (0.0386) \n  \n  \n    l_urea \n    0.1715*** (0.0228) \n    0.1629*** (0.0262) \n    0.1741*** (0.0269) \n  \n  \n    l_pest_smart \n    0.0912*** (0.0172) \n    0.1103*** (0.0185) \n     \n  \n  \n    pest_revdum \n    0.5129*** (0.1141) \n    0.6350*** (0.1290) \n     \n  \n  \n    l_pest_lazy \n     \n     \n    0.0077*** (0.0020) \n  \n  \n    Num.Obs. \n    1026 \n    1026 \n    1026 \n  \n  \n    AIC \n    2796.9 \n    2187.6 \n    2222.6 \n  \n  \n    BIC \n    7824.1 \n    6376.1 \n    6416.0 \n  \n\n\n Robust standard errors clustered at the group level are reported in the brackets."
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#computing-yields-and-logs",
    "href": "slides/w08b-panel-regression-production-function.html#computing-yields-and-logs",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Computing yields and logs",
    "text": "Computing yields and logs\n\nfarm_dta_log <- farm_dta %>%\n  mutate(\n    l_output = log(output),\n    l_land = log(land),\n    l_seed = log(seed),\n    l_urea = log(urea),\n    l_pest = log(pest),\n    l_labor = log(labor),\n    yields_mt_ha = output / land / 1000\n  )"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#summary-statistics",
    "href": "slides/w08b-panel-regression-production-function.html#summary-statistics",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Summary statistics",
    "text": "Summary statistics\n\nn_inf <- function(x) sum(is.infinite(x))\nn_missing <- function(x) sum(is.na(x)|is.nan(x))\ndatasummary(\n  l_output + l_land + l_seed + l_urea + l_pest + l_labor + yields_mt_ha ~ \n    N + n_missing + n_inf + Mean + SD + Median + Min + Max,\n  data = farm_dta_log)\n\n\n\n \n  \n      \n    N \n    n_missing \n    n_inf \n    Mean \n    SD \n    Median \n    Min \n    Max \n  \n \n\n  \n    l_output \n    1026 \n    0.00 \n    0.00 \n    6.73 \n    0.99 \n    6.79 \n    3.74 \n    9.95 \n  \n  \n    l_land \n    1026 \n    0.00 \n    0.00 \n    −1.30 \n    0.95 \n    −1.25 \n    −4.61 \n    1.67 \n  \n  \n    l_seed \n    1026 \n    0.00 \n    0.00 \n    2.37 \n    0.94 \n    2.30 \n    0.00 \n    7.13 \n  \n  \n    l_urea \n    1026 \n    0.00 \n    0.00 \n    3.98 \n    1.16 \n    4.09 \n    0.00 \n    7.13 \n  \n  \n    l_pest \n    1026 \n    0.00 \n    713.00 \n     \n     \n     \n     \n    11.04 \n  \n  \n    l_labor \n    1026 \n    0.00 \n    0.00 \n    5.56 \n    0.85 \n    5.53 \n    2.83 \n    8.47 \n  \n  \n    yields_mt_ha \n    1026 \n    0.00 \n    0.00 \n    3.38 \n    1.68 \n    3.16 \n    0.40 \n    27.50 \n  \n\n\n\n\n\n\nAny problems with data?"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#summary-statistics-after-data-cleaning",
    "href": "slides/w08b-panel-regression-production-function.html#summary-statistics-after-data-cleaning",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Summary statistics after data cleaning",
    "text": "Summary statistics after data cleaning\n\ndatasummary(l_pest + l_pest_lazy + l_pest_smart +  pest_revdum ~ \n    N + n_missing + n_inf + Mean + SD + Median + Min + Max, \n    data = farm_dta_log)\n\n\n\n \n  \n      \n    N \n    n_missing \n    n_inf \n    Mean \n    SD \n    Median \n    Min \n    Max \n  \n \n\n  \n    l_pest \n    1026 \n    0.00 \n    713.00 \n     \n     \n     \n     \n    11.04 \n  \n  \n    l_pest_lazy \n    1026 \n    0.00 \n    0.00 \n    −4.37 \n    7.35 \n    −9.21 \n    −9.21 \n    11.04 \n  \n  \n    l_pest_smart \n    1026 \n    0.00 \n    0.00 \n    2.04 \n    3.15 \n    0.00 \n    0.00 \n    11.04 \n  \n  \n    pest_revdum \n    1026 \n    0.00 \n    0.00 \n    0.69 \n    0.46 \n    1.00 \n    0.00 \n    1.00"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#step-4.-robust-inference-2",
    "href": "slides/w08b-panel-regression-production-function.html#step-4.-robust-inference-2",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Step 4. Robust inference",
    "text": "Step 4. Robust inference\n\ncompare_performance(\n  list(\n    `Pooled` = rice_pooled,\n    `FE with rev. dum` = rice_fe,\n    `FE lazy` = rice_fe_lazy\n  )\n) \n\n# Comparison of Model Performance Indices\n\nName             | Model | AIC (weights) | AICc (weights) | BIC (weights) |    R2 | R2 (adj.) |  RMSE | Sigma\n-------------------------------------------------------------------------------------------------------------\nPooled           |   plm | 774.9 (<.001) |  775.1 (<.001) | 814.4 (<.001) | 0.876 |     0.875 | 0.350 | 0.351\nFE with rev. dum |   plm | 503.6 (>.999) |  503.7 (>.999) | 538.2 (>.999) | 0.732 |     0.676 | 0.307 | 0.308\nFE lazy          |   plm | 534.6 (<.001) |  534.6 (<.001) | 564.2 (<.001) | 0.723 |     0.666 | 0.312 | 0.313"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#what-about-ovb",
    "href": "slides/w08b-panel-regression-production-function.html#what-about-ovb",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "What about OVB?",
    "text": "What about OVB?\n\\[\n\\begin{aligned}\n\\ln \\text{output}_{it} & = A_0 + \\beta_1 \\cdot \\ln \\text{land}_{it}\n+ \\beta_2 \\cdot \\ln \\text{labor}_{it} \\\\\n& + \\beta_3 \\cdot \\ln \\text{seed}_{it} + \\beta_4 \\cdot \\ln  \\text{urea}_{it} \\\\\n& + \\beta_5 \\cdot \\ln \\text{pesticide}_{it} + e_{it}\n\\end{aligned}\n\\]\nWhat omitted variables could cause bias of our estimates?\n\nAny!? …\nAny!? …\nAny OVB!? …\nCapital, Ability, Climate, Geography…"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#using-the-ovb-formula",
    "href": "slides/w08b-panel-regression-production-function.html#using-the-ovb-formula",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Using the OVB formula",
    "text": "Using the OVB formula\n\nLet us make an educated guess about the effect of OVB on the estimates of land-related coefficient \\(\\beta_1\\)?\n\n\n\n\nShort:\nLong:\nAux.\nOVB:\n\n\n\n\\(\\text{output}_{it} = A_0^s + \\beta_1^s \\cdot \\text{land}_{it} + \\beta_2^s \\cdot \\text{labor}_{it} + \\beta_3^s \\cdot \\text{seed}_{it} + \\beta_4^s \\cdot \\text{urea}_{it} + \\beta_5^s \\cdot \\text{pesticide}_{it} + e_{it}\\)\n\\(\\text{output}_{it} = A_0 + \\beta_1 \\cdot \\text{land}_{it} + \\beta_2 \\cdot \\text{labor}_{it} + \\beta_3 \\cdot \\text{seed}_{it} + \\beta_4 \\cdot \\text{urea}_{it} + \\beta_5 \\cdot \\text{pesticide}_{it} + \\color{Red}{\\gamma \\text{Ability}_i} + e_{it}\\)\n\\(\\text{Ability}_i = \\pi_0 + \\color{Blue}{\\pi_1 \\cdot \\text{land}_{it}} + \\pi_2 \\cdot \\text{labor}_{it} + \\pi_3 \\cdot \\text{seed}_{it} + \\pi_4 \\cdot \\text{urea}_{it} + \\pi_5 \\cdot \\text{pesticide}_{it} + e_{it}\\)\n\\(\\text{OVB}_{\\text{land}} = \\beta_1^s - \\beta_1 = \\color{Red}{\\pi_1} * \\color{Blue}{\\gamma}\\)\n\n\n\n\nEducated guess about the bias of the estimates.\nWhat are the signs of \\(\\pi_1\\) and \\(\\gamma\\)?\n\n\\(\\pi_1 > 0\\)\n\\(\\gamma > 0\\)\n\\(\\text{OVB} = (+) \\times (+) > 0\\)\n\nNot controlling for the OV \\(\\text{ability}_i\\) may cause overestimation of the effect of the farm size \\(\\beta_1\\).\n\\(\\text{ability}_i\\) does not vary over time for each farm!"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#how-to-resolve-such-ovb",
    "href": "slides/w08b-panel-regression-production-function.html#how-to-resolve-such-ovb",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "How to resolve such OVB?",
    "text": "How to resolve such OVB?\n\nIntroduce a proxy variable for ability?\n\nNo such data.\n\nRely on the panel structure of the data.\n\nUse the individual fixed effect model, for example."
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#estimation-strategy.-part-3.-return-to-scale-1",
    "href": "slides/w08b-panel-regression-production-function.html#estimation-strategy.-part-3.-return-to-scale-1",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Estimation strategy. Part 3. Return to scale (1)",
    "text": "Estimation strategy. Part 3. Return to scale (1)\nAre larger farms more productive?\nTo understand this, we need to calculate how joint increase of all inputs change the output.\n\nIf increase of all inputs by 1% increases output also by the same 1%, we have a constant return to scale.\nIf increase of all inputs by 1% increases output also by more than 1%, we have an increasing return to scale.\nIf increase of all inputs by 1% increases output also by less than 1%, we have an decreasing return to scale."
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#estimation-strategy.-part-3.-return-to-scale-2",
    "href": "slides/w08b-panel-regression-production-function.html#estimation-strategy.-part-3.-return-to-scale-2",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Estimation strategy. Part 3. Return to scale (2)",
    "text": "Estimation strategy. Part 3. Return to scale (2)\nFrom the main equation,\n\\[\n\\begin{aligned}\n\\ln \\text{output}_{it} & = A_0 + \\beta_1 \\cdot \\ln \\text{land}_{it}\n+ \\beta_2 \\cdot \\ln \\text{labor}_{it} \\\\\n& + \\beta_3 \\cdot \\ln \\text{seed}_{it} + \\beta_4 \\cdot \\ln  \\text{urea}_{it} \\\\\n& + \\beta_5 \\cdot \\ln \\text{pesticide}_{it} \\\\\n& + \\color{Red}{\\alpha_i} + e_{it}\n\\end{aligned}\n\\]\n\nreturn to scale can be estimated as a sum of all coefficients:\n\n\\[\n\\text{Rreturn to scale} = \\beta_1 + \\beta_2 + \\beta_3 + \\beta_4 + \\beta_5\n\\]\n\nTo perform a hypothesis testing about the return to scale, we need to employ:\n\nHT about a linear combination of parameters, and\ndelta method for estimating standard errors."
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#estimation-strategy.-part-3.-return-to-scale-3",
    "href": "slides/w08b-panel-regression-production-function.html#estimation-strategy.-part-3.-return-to-scale-3",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Estimation strategy. Part 3. Return to scale (3)",
    "text": "Estimation strategy. Part 3. Return to scale (3)\nHT about a linear combination of parameters:\n\nH0 - \\(\\beta_1 + \\beta_2 + \\beta_3 + \\beta_4 + \\beta_5 = 1\\) (also can be \\(=0\\) or any number)\nH1 - \\(\\beta_1 + \\beta_2 + \\beta_3 + \\beta_4 + \\beta_5 \\ne 1\\) (also can be \\(\\ne0\\) or any number)\n\nWe compute standard errors using delta method (car::deltaMethod()).\nAnd perform HT using F statistics."
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#part-4.-return-to-scale-1",
    "href": "slides/w08b-panel-regression-production-function.html#part-4.-return-to-scale-1",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Part 4. Return to scale (1)",
    "text": "Part 4. Return to scale (1)\n\nrice_fe\n\n\nModel Formula: l_output ~ l_land + l_labor + l_seed + l_urea + l_pest_smart + \n    pest_revdum\n\nCoefficients:\n      l_land      l_labor       l_seed       l_urea l_pest_smart  pest_revdum \n     0.41984      0.27084      0.12381      0.16291      0.11029      0.63498 \n\n\nComputing sum of the coefficients and robust SE:\n\nlibrary(car)\ndeltaMethod(\n  rice_fe, \n  \"l_land + l_labor + l_seed + l_urea + l_pest_smart\", \n  vcov = vcovHC(rice_fe, method = \"arellano\", type = \"HC3\", cluster = \"group\")\n)\n\n                                                  Estimate       SE    2.5 %\nl_land + l_labor + l_seed + l_urea + l_pest_smart 1.087685 0.031039 1.026850\n                                                  97.5 %\nl_land + l_labor + l_seed + l_urea + l_pest_smart 1.1485"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#part-4.-return-to-scale-2",
    "href": "slides/w08b-panel-regression-production-function.html#part-4.-return-to-scale-2",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Part 4. Return to scale (2)",
    "text": "Part 4. Return to scale (2)\nHT about the sum of the coefficients:\n\n\\(H_0: \\text{return to scale} = 1\\)\n\\(H_0: \\text{return to scale} \\ne 1\\)\n\n\nlinearHypothesis(rice_fe, \n  \"l_land + l_labor + l_seed + l_urea + l_pest_smart = 1\", \n  vcov = vcovHC(rice_fe, method = \"arellano\", type = \"HC3\", cluster = \"group\"))\n\nLinear hypothesis test\n\nHypothesis:\nl_land  + l_labor  + l_seed  + l_urea  + l_pest_smart = 1\n\nModel 1: restricted model\nModel 2: l_output ~ l_land + l_labor + l_seed + l_urea + l_pest_smart + \n    pest_revdum\n\nNote: Coefficient covariance matrix supplied.\n\n  Res.Df Df  Chisq Pr(>Chisq)   \n1    850                        \n2    849  1 7.9806   0.004728 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#conclusions",
    "href": "slides/w08b-panel-regression-production-function.html#conclusions",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Conclusions",
    "text": "Conclusions\n\n\nRegression results\n\n\n\n\n \n  \n      \n    Pooled \n    FE (rev. dum.) \n    FE (lazy) \n  \n \n\n  \n    (Intercept) \n    4.3144*** (0.2691) \n     \n     \n  \n  \n    l_land \n    0.4348*** (0.0418) \n    0.4198*** (0.0479) \n    0.4210*** (0.0486) \n  \n  \n    l_labor \n    0.2535*** (0.0333) \n    0.2708*** (0.0347) \n    0.2639*** (0.0348) \n  \n  \n    l_seed \n    0.1484*** (0.0344) \n    0.1238*** (0.0370) \n    0.1344*** (0.0386) \n  \n  \n    l_urea \n    0.1715*** (0.0228) \n    0.1629*** (0.0262) \n    0.1741*** (0.0269) \n  \n  \n    l_pest_smart \n    0.0912*** (0.0172) \n    0.1103*** (0.0185) \n     \n  \n  \n    pest_revdum \n    0.5129*** (0.1141) \n    0.6350*** (0.1290) \n     \n  \n  \n    l_pest_lazy \n     \n     \n    0.0077*** (0.0020) \n  \n  \n    Num.Obs. \n    1026 \n    1026 \n    1026 \n  \n  \n    R2. adj \n    0.875 \n    0.676 \n    0.666 \n  \n  \n    F stat. \n    1 198.277 \n    386.282 \n    444.057 \n  \n  \n    p-value \n    <0.001 \n    <0.001 \n    <0.001 \n  \n  \n    df \n    1 019 \n    849 \n    850 \n  \n\n\n Note: All reported variables are transformed with a natural logarithm. Reverse dummy variable that compensates for the zero pesticides use is omitted. Robust standard errors clustered at the group level are reported in the brackets.\n\n\n\n\n\nReturn to scale\n\nlibrary(car)\ndeltaMethod(rice_fe, \n            \"l_land + l_labor + l_seed + l_urea + l_pest_smart\", \n            vcov = vcovHC(rice_fe, method = \"arellano\", type = \"HC3\", cluster = \"group\"))\n\n                                                  Estimate       SE    2.5 %\nl_land + l_labor + l_seed + l_urea + l_pest_smart 1.087685 0.031039 1.026850\n                                                  97.5 %\nl_land + l_labor + l_seed + l_urea + l_pest_smart 1.1485\n\n\n\nlinearHypothesis(rice_fe, \n                 \"l_land + l_labor + l_seed + l_urea + l_pest_smart = 1\", \n                 vcov = vcovHC(rice_fe, method = \"arellano\", type = \"HC3\", cluster = \"group\"))\n\nLinear hypothesis test\n\nHypothesis:\nl_land  + l_labor  + l_seed  + l_urea  + l_pest_smart = 1\n\nModel 1: restricted model\nModel 2: l_output ~ l_land + l_labor + l_seed + l_urea + l_pest_smart + \n    pest_revdum\n\nNote: Coefficient covariance matrix supplied.\n\n  Res.Df Df  Chisq Pr(>Chisq)   \n1    850                        \n2    849  1 7.9806   0.004728 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#individual-fixed-effects-1",
    "href": "slides/w08b-panel-regression-production-function.html#individual-fixed-effects-1",
    "title": "Panel Regression Analysis: Micro application",
    "section": "Individual fixed effects (1)",
    "text": "Individual fixed effects (1)\nFrom the regression equation below,\n\\[\n\\begin{aligned}\n\\ln \\text{output}_{it} & = A_0 + \\beta_1 \\cdot \\ln \\text{land}_{it}\n+ \\beta_2 \\cdot \\ln \\text{labor}_{it} \\\\\n& + \\beta_3 \\cdot \\ln \\text{seed}_{it} + \\beta_4 \\cdot \\ln  \\text{urea}_{it} \\\\\n& + \\beta_5 \\cdot \\ln \\text{pesticide}_{it} \\\\\n& + \\color{Red}{\\alpha_i} + e_{it}\n\\end{aligned}\n\\]\nwe know that \\(\\color{Red}{\\alpha_i}\\) are the individual fixed effects.\n\nR calculates them and we can explore them.\nIn fact, those individual fixed effects are simples possible measured of farms efficiency!"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#individual-fixed-effects-extraction-1",
    "href": "slides/w08b-panel-regression-production-function.html#individual-fixed-effects-extraction-1",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Individual fixed effects (extraction 1)",
    "text": "Individual fixed effects (extraction 1)\nIn the model:\n\nrice_fe\n\n\nModel Formula: l_output ~ l_land + l_labor + l_seed + l_urea + l_pest_smart + \n    pest_revdum\n\nCoefficients:\n      l_land      l_labor       l_seed       l_urea l_pest_smart  pest_revdum \n     0.41984      0.27084      0.12381      0.16291      0.11029      0.63498 \n\n\nIndividual fixed effects can be extracted as:\n\nfixef(rice_fe)\n\n101001 101017 101026 101035 101056 101057 101067 101068 101069 101073 101089 \n3.8470 4.1040 4.2375 4.2631 4.5649 4.2051 4.2591 4.3471 4.3598 4.0599 4.0611 \n101094 102111 102113 102119 102126 102157 102194 102220 201001 201002 201003 \n4.0503 4.1591 4.1073 4.2289 3.9991 4.2420 3.9798 4.2032 4.3270 4.3719 4.2906 \n201009 202039 202061 202066 203079 203080 204096 204114 204116 204124 205132 \n4.1036 4.2036 4.1584 4.1318 4.2522 4.0977 4.2539 3.9338 4.3095 4.0783 3.9765 \n205136 205151 205153 206158 206169 207209 208225 209232 209241 209250 301004 \n3.9870 4.2319 4.2504 4.4004 4.0218 4.2235 4.2522 4.1496 4.0292 4.0267 4.0658 \n301010 301023 301038 301055 301058 301067 301070 301075 301084 301105 301110 \n3.7676 4.1062 4.1388 4.1066 4.1385 4.1110 4.0471 4.3944 3.8461 3.9620 3.9876 \n302116 302120 302131 302134 302137 302142 302143 302144 302146 302147 302151 \n4.1645 3.8708 4.0910 4.2774 4.1428 4.0946 4.0225 4.1853 4.1072 3.8265 4.1892 \n302153 302161 302163 302169 302182 302189 302192 302194 302195 302197 302199 \n3.9304 4.1563 4.1978 4.2176 4.1114 4.2771 4.3379 4.1691 4.3303 4.2350 4.0927 \n302205 302207 302209 401002 401006 401032 401034 401036 401037 401041 401043 \n4.1584 3.9223 4.1140 4.2507 3.9059 4.2725 4.0764 3.9731 3.8727 4.0375 4.1468 \n401049 401058 401063 401069 401075 401077 401092 401095 401109 401122 401124 \n4.0360 3.9986 3.9069 4.0990 4.2085 4.0976 4.3029 4.2423 4.3479 4.2510 4.1138 \n401125 401138 402150 402155 402162 402167 402168 402169 402171 402176 402179 \n4.3466 4.1990 4.1397 4.1476 4.2274 4.0829 3.9949 3.9759 4.1879 4.0469 4.1212 \n402201 402203 402208 501001 501008 501020 501034 501041 501045 502057 502062 \n4.1575 4.1735 4.0134 3.8749 4.0124 4.3724 3.8748 4.7488 4.3660 4.0882 4.0824 \n502080 502081 502112 503135 503136 503143 504161 504162 504167 504168 504169 \n4.1904 4.1741 4.3069 4.1730 4.2219 4.1793 4.2881 4.2402 4.4824 4.4172 4.0891 \n504197 504201 504204 601005 601010 601016 602034 603043 603052 603053 603062 \n4.1560 4.1874 4.3086 4.0255 4.2169 3.9834 4.0436 4.0758 4.2705 3.8823 3.8317 \n603065 603067 603068 603070 604074 605108 605109 605116 606133 606145 606147 \n3.9770 3.8956 4.2175 4.2099 4.2280 4.1904 4.0838 4.4748 4.5949 4.4224 4.2922 \n606151 607164 607167 607168 607170 607188 607195 608205 608207 608215 609227 \n4.3270 4.3790 4.3251 4.3789 4.3182 4.4098 4.2933 4.1269 4.6940 4.7730 4.1891 \n609231 609234 609241 609242 609244 609245 \n4.1425 4.5465 4.2098 4.1420 4.1388 4.2396"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#individual-fixed-effects-extraction-3",
    "href": "slides/w08b-panel-regression-production-function.html#individual-fixed-effects-extraction-3",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Individual fixed effects (extraction 3)",
    "text": "Individual fixed effects (extraction 3)\nOr, we can extract individual fixed effects with effect-specific standard errors:\n\nfef_dta <- \n  fixef(rice_fe) %>% \n  summary() %>% \n  as.data.frame() %>% rownames_to_column(\"id\") %>% \n  as_tibble() %>% \n  mutate(id = as.double(id))\nfef_dta\n\n# A tibble: 171 × 5\n       id Estimate `Std. Error` `t-value` `Pr(>|t|)`\n    <dbl>    <dbl>        <dbl>     <dbl>      <dbl>\n 1 101001     3.85        0.317      12.2   1.96e-31\n 2 101017     4.10        0.282      14.6   5.20e-43\n 3 101026     4.24        0.285      14.9   1.09e-44\n 4 101035     4.26        0.279      15.3   1.11e-46\n 5 101056     4.56        0.285      16.0   9.02e-51\n 6 101057     4.21        0.292      14.4   3.46e-42\n 7 101067     4.26        0.296      14.4   2.89e-42\n 8 101068     4.35        0.283      15.4   3.87e-47\n 9 101069     4.36        0.279      15.6   1.62e-48\n10 101073     4.06        0.294      13.8   3.35e-39\n# ℹ 161 more rows"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#farm-size-and-efficiency",
    "href": "slides/w08b-panel-regression-production-function.html#farm-size-and-efficiency",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Farm size and efficiency",
    "text": "Farm size and efficiency\nLet us compute average farm size and plot fixed effects versus farm size:\n\nfarm_mean_size <- farm_dta_log %>% group_by(id) %>% summarise(mean_size = mean(land))\nglimpse(farm_mean_size)\n\nRows: 171\nColumns: 2\n$ id        <dbl> 101001, 101017, 101026, 101035, 101056, 101057, 101067, 1010…\n$ mean_size <dbl> 2.5240000, 0.7230000, 0.1558333, 0.3211667, 0.3360000, 0.189…\n\n\n\nplot_dta <- farm_mean_size %>% left_join(fef_dta)\nglimpse(plot_dta)\n\nRows: 171\nColumns: 6\n$ id           <dbl> 101001, 101017, 101026, 101035, 101056, 101057, 101067, 1…\n$ mean_size    <dbl> 2.5240000, 0.7230000, 0.1558333, 0.3211667, 0.3360000, 0.…\n$ Estimate     <dbl> 3.847036, 4.103960, 4.237548, 4.263063, 4.564856, 4.20514…\n$ `Std. Error` <dbl> 0.3166160, 0.2819603, 0.2847204, 0.2792276, 0.2846006, 0.…\n$ `t-value`    <dbl> 12.15048, 14.55510, 14.88319, 15.26734, 16.03951, 14.3927…\n$ `Pr(>|t|)`   <dbl> 1.958956e-31, 5.202280e-43, 1.090622e-44, 1.109427e-46, 9…\n\n\n\nplot_dta %>% ggplot() + aes(x = mean_size, y = Estimate) + \n  geom_point() + geom_smooth() + scale_x_log10() +\n  xlab(\"Farm-specific fixed effect\") + \n  ylab(\"Average farm size, ha\") + \n  annotation_logticks(side = \"b\")"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#farm-size-and-efficiency-output",
    "href": "slides/w08b-panel-regression-production-function.html#farm-size-and-efficiency-output",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Farm size and efficiency",
    "text": "Farm size and efficiency"
  },
  {
    "objectID": "slides/w08b-panel-regression-production-function.html#individual-fixed-effects",
    "href": "slides/w08b-panel-regression-production-function.html#individual-fixed-effects",
    "title": "Panel Regression Analysis: Micro application to production function",
    "section": "Individual fixed effects",
    "text": "Individual fixed effects\nFrom the regression equation below,\n\\[\n\\begin{aligned}\n\\ln \\text{output}_{it} & = A_0 + \\beta_1 \\cdot \\ln \\text{land}_{it}\n+ \\beta_2 \\cdot \\ln \\text{labor}_{it} \\\\\n& + \\beta_3 \\cdot \\ln \\text{seed}_{it} + \\beta_4 \\cdot \\ln  \\text{urea}_{it} \\\\\n& + \\beta_5 \\cdot \\ln \\text{pesticide}_{it} \\\\\n& + \\color{Red}{\\alpha_i} + e_{it}\n\\end{aligned}\n\\]\nwe know that \\(\\color{Red}{\\alpha_i}\\) are the individual fixed effects.\n\nR calculates them and we can explore them.\nIn fact, those individual fixed effects are the simplest possible measured of farms efficiency!"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#fixed-effect-panel-regression-model",
    "href": "slides/w08a-panel-regression.html#fixed-effect-panel-regression-model",
    "title": "Panel Regression Analysis",
    "section": "Fixed Effect Panel Regression Model",
    "text": "Fixed Effect Panel Regression Model\nIndividual Fixed effect model:\n\\[\ny_{it} =  \\beta_1 \\cdot x_{1it} + \\beta_2 \\cdot x_{1it}  + \\dots + \\beta_k  \\cdot x_{kit}  + \\color{Red}{ \\alpha_i } + \\epsilon_{it}\n\\]\n\n\\(\\color{Red}{ \\alpha_i }\\) are the individual-specific (\\(i\\)) fixed effect;\nusually without the intercept \\(\\beta_0\\);\n\n\nTwo-ways fixed effect model (individual + time effect):\n\\[\ny_{it} =  \\beta_1 \\cdot x_{1it} + \\beta_2 \\cdot x_{1it}  + \\dots + \\beta_k  \\cdot x_{kit}  \\\\ + \\color{Red}{ \\alpha_i } + \\color{Blue}{ \\eta_t } + \\epsilon_{it}\n\\]\nTime Fixed Effect model:\n\\[\ny_{it} =  \\beta_1 \\cdot x_{1it} + \\beta_2 \\cdot x_{1it}  + \\dots + \\beta_k  \\cdot x_{kit}  \\\\ + \\color{Blue}{ \\eta_t } + \\epsilon_{it}\n\\]"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#fixed-effect-model-within-transformation-step-1",
    "href": "slides/w08a-panel-regression.html#fixed-effect-model-within-transformation-step-1",
    "title": "Panel Regression Analysis",
    "section": "Fixed Effect Model: Within transformation (Step 1)",
    "text": "Fixed Effect Model: Within transformation (Step 1)\nWithin-transformation subtracts group means from each observation and estimates \\(\\beta\\) on transformed data using OLS.\n\\[\n\\begin{aligned}\ny_{it} - \\overline{y_{i}} &\n= \\beta_1 (x_{1it} - \\overline{x_{1i}})\n+ \\beta_2 (x_{2it} - \\overline{x_{2i}}) \\\\\n& + \\beta_3 (x_{3i} - \\overline{x_{3i}})\n+ \\color{Red}{\\alpha_{i}}\n+ \\epsilon_{it},\n\\end{aligned}\n\\]\n\n\\(\\overline{y_{i}}\\) and \\(\\overline{x_{ki}}\\) are group \\(i\\)-specific means computed as: \\(\\overline{x_{ki}} = \\frac{1}{N_i} \\sum_t x_{kit}\\), where \\(N_i\\) is the number of observations (time periods \\(t\\)) in the group \\(i\\).\n\n\n\\(\\ddot{y_i} = y_{it} - \\overline{y_{i}}\\), \\(\\ddot{x_i} = x_{it} - \\overline{x_{i}}\\) are de-meaned regressand and regressors.\n\nNote! \\(\\beta_3=0\\), because any time-invariant \\(x_{ki}\\) (\\(x_k\\) without \\(t\\) index) will become zero: \\({x}_{i} - \\overline{{x}_{i}} = 0\\).\n\nSuch \\(x\\) are: gender, race, individual characteristics …"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#fixed-effect-model-within-transformation-step-2",
    "href": "slides/w08a-panel-regression.html#fixed-effect-model-within-transformation-step-2",
    "title": "Panel Regression Analysis",
    "section": "Fixed Effect Model: Within transformation (Step 2)",
    "text": "Fixed Effect Model: Within transformation (Step 2)\nBased on the demeaned data without time-invariant effects, OLS method is used to estimate \\(\\hat \\beta\\) for all \\(k\\) variables:\n\\[\n\\begin{aligned}\n\\ddot{y_i} & = \\hat \\beta_1 \\ddot{x}_{1it} + \\hat \\beta_2 \\ddot{x}_{2it} + \\dots + \\hat \\beta_k \\ddot{x}_{kit} + \\epsilon_{it}\n\\end{aligned}\n\\]\n\nEstimated \\(\\hat \\beta\\) are identical to the one obtain using LSDV model!"
  },
  {
    "objectID": "slides/w08a-panel-regression.html#fixed-effect-model-within-transformation-step-3",
    "href": "slides/w08a-panel-regression.html#fixed-effect-model-within-transformation-step-3",
    "title": "Panel Regression Analysis",
    "section": "Fixed Effect Model: Within transformation (Step 3)",
    "text": "Fixed Effect Model: Within transformation (Step 3)\nIndividual Fixed Effects \\(\\alpha_i\\) are computed as:\n\\[\n\\alpha_i = \\overline y_i - (\\hat \\beta_1 \\overline{x}_{1i} + \\hat \\beta_2 \\overline{x}_{2i} + \\dots + \\hat \\beta_k \\overline{x}_{ki} )\n\\]\nIndividual fixed effects are identical to \\(\\delta_i\\) from LSDV model:\n\\[\n\\alpha_i = \\beta_0 + \\delta_i\n\\]\n\nIgnoring FE causes bias to the estimates."
  },
  {
    "objectID": "slides/w10-IV.html#return-to-schooling-and-the-selection-bias",
    "href": "slides/w10-IV.html#return-to-schooling-and-the-selection-bias",
    "title": "Instrumental Variable",
    "section": "Return to schooling and the Selection bias",
    "text": "Return to schooling and the Selection bias\n\nDoes more years of schooling cause higher wages?\nThink of an RCT experiment that could help to estimate true causal effect of schooling on income!\nWhat other methods can we use to compute the return to schooling?"
  },
  {
    "objectID": "slides/w10-IV.html#short-regression",
    "href": "slides/w10-IV.html#short-regression",
    "title": "Instrumental Variable",
    "section": "Short Regression",
    "text": "Short Regression\n\\[\nY_i = \\alpha^S + \\rho^S s_i + \\beta^S X_i + \\varepsilon_i^S\n\\qquad(1.1)\\]\n\nannual earning \\(Y_i\\)\nyears of education \\(s_i\\)\n\\(X_i\\) vector of other control variables, such as experience.\n\n\nIs the ceteris paribus fulfilled in Equation 1.1?\n\nIs control for experience and education sufficient?\nAt a given experience/education level, are more- and less-educated workers equally able and diligent? (see Angrist & Pischke, 2014, Ch. 6)"
  },
  {
    "objectID": "slides/w10-IV.html#long-regression",
    "href": "slides/w10-IV.html#long-regression",
    "title": "Instrumental Variable",
    "section": "Long Regression",
    "text": "Long Regression\n\\[\nY_i = \\alpha + \\rho s_i + \\beta X_i + \\gamma A^{'}_{i} + \\varepsilon_i\n\\qquad(1.2)\\]\n\nwhere \\(A^{'}_{i}\\) is the ability variable that we desire to have in order to ensure the unbiased estimates of \\(\\rho\\).\nOmitting \\(A^{'}_{i}\\) causes a selection bias or endogeneity:\n\n\\(\\rho^{S} = \\rho + \\underbrace{\\delta_{A^{'} s} \\times \\gamma}_{\\text{ability bias}}\\)"
  },
  {
    "objectID": "slides/w10-IV.html#definition",
    "href": "slides/w10-IV.html#definition",
    "title": "Instrumental Variable",
    "section": "Definition",
    "text": "Definition\n\nConsider following LONG and SHORT models:\n\n\n\\[\n\\begin{aligned}\nY_i &= \\alpha + \\rho s_i + \\beta X_i + \\gamma A^{'}_{i} + \\varepsilon_i , && \\text{long}\\\\\nY_i &= \\alpha^S + \\rho^S s_i + \\beta^S X_i + \\varepsilon_i^S , && \\text{short}\n\\end{aligned}\n\\]\n\n\\(s_i\\) is a causal variable of interest (education)\n\\(A^{'}_{i}\\) is the vector of control variables that we desire to have in order to ensure unbiased estimates of \\(\\rho\\);\nVariable \\(s_i\\) is endogenous if it correlates with the error terms \\(\\varepsilon^{S}_i\\) :\n\n\n\n\\[\nCov(s_i, \\varepsilon^{S}_i) \\neq 0\n\\]"
  },
  {
    "objectID": "slides/w10-IV.html#definition-of-the-endogeneity",
    "href": "slides/w10-IV.html#definition-of-the-endogeneity",
    "title": "Instrumental Variable",
    "section": "Definition of the endogeneity",
    "text": "Definition of the endogeneity\nVariable \\(s_i\\) is endogenous if it correlates with the error terms \\(\\varepsilon^{S}_i\\) :\n\\[\nCov(s_i, \\varepsilon^{S}_i) \\neq 0\n\\]\n\nIn practice, this means that:\n\nvariation in the independent variable \\(s_i\\) (education) are not “random” as compared to the variation in the dependent variable \\(Y_i\\), but\nan external process \\(U\\) affects variation in both \\(s_i\\) and \\(Y_i\\);\nthus, \\(s_i\\) is endogenous to \\(Y_i\\);\n\n\n\nIf variance of \\(s_i\\) is truly independent of \\(Y_i\\), \\(s_i\\) is exogenous."
  },
  {
    "objectID": "slides/w10-IV.html#causes-of-endogeneity",
    "href": "slides/w10-IV.html#causes-of-endogeneity",
    "title": "Instrumental Variable",
    "section": "Causes of endogeneity",
    "text": "Causes of endogeneity\n\nOmitted Variable Bias (familiar)\nMeasurement Error\nSimultaneity"
  },
  {
    "objectID": "slides/w10-IV.html#measurement-error",
    "href": "slides/w10-IV.html#measurement-error",
    "title": "Instrumental Variable",
    "section": "Measurement error",
    "text": "Measurement error"
  },
  {
    "objectID": "slides/w10-IV.html#simultaneity",
    "href": "slides/w10-IV.html#simultaneity",
    "title": "Instrumental Variable",
    "section": "Simultaneity",
    "text": "Simultaneity"
  },
  {
    "objectID": "slides/w10-IV.html#endogeneity-in-practice",
    "href": "slides/w10-IV.html#endogeneity-in-practice",
    "title": "Instrumental Variable",
    "section": "Endogeneity in practice:",
    "text": "Endogeneity in practice:\n\nvariation in the independent variable \\(s_i\\) (education) is not “random” as compared to the variation in the dependent variable \\(Y_i\\), but\nan external process \\(U\\) affects variation in both \\(s_i\\) and \\(Y_i\\);\nthus, \\(s_i\\) is endogenous to \\(Y_i\\);\n\n\nIf variance of \\(s_i\\) is truly independent of \\(Y_i\\), \\(s_i\\) is exogenous."
  },
  {
    "objectID": "slides/w10-IV.html#ovb",
    "href": "slides/w10-IV.html#ovb",
    "title": "Instrumental Variable",
    "section": "OVB",
    "text": "OVB"
  },
  {
    "objectID": "slides/w10-IV.html#measurement-error-1",
    "href": "slides/w10-IV.html#measurement-error-1",
    "title": "Instrumental Variable",
    "section": "Measurement error",
    "text": "Measurement error\n\nWe estimate a long model: \\(Y_i = \\alpha + \\beta s^*_i + e_i\\) ,\n\nbut \\(s^*_i\\) is unavailable, we only have \\(s_i = s^*_i + m_i\\) instead,\n\\(m_i\\) is a systematic measurement error,\n\\(E[m_i] =0\\) and \\(Cov(s^*_i, m_i) = Cov(e_i, m_i) = 0\\).\n\nDesired coefficient \\(\\beta = \\frac{Cov(Y_i, s_i)}{Var(s_i)}\\)\nBut with the erroneous data, we estimate biased coefficient \\(\\beta_b\\)\n\n\n\\[\n\\begin{aligned}\n\\beta_b & =  \n\\frac{Cov(Y_i, s_i)}{Var(s_i)}  =\n\\frac{Cov(a+\\beta s^*_i + e_i, s^*_i + m_i)}{Var(s_i)} \\\\  \n& =  \\frac{\\beta \\cdot Cov(s^*_i, s^*_i)}{Var(s_i)}  =  \\beta \\frac{Var(s^{*}_i)}{Var(s_i)}\n\\end{aligned}\n\\]\n\n(see Angrist & Pischke, 2014, Ch. 6)"
  },
  {
    "objectID": "slides/w10-IV.html#simultaneity-1",
    "href": "slides/w10-IV.html#simultaneity-1",
    "title": "Instrumental Variable",
    "section": "Simultaneity",
    "text": "Simultaneity\n\nSimultaneity occurs if at least two variables are jointly determined.\n\nA typical case is when observed outcomes are the result of separate behavioral mechanisms that are coordinated in an equilibrium.\n\nThe prototypical case is a system of demand and supply equations:\n\n\\(D(p)\\) = how high would demand be if the price was set to \\(p\\)?\n\\(S(p)\\) = how high would supply be if the price was set to \\(p\\)?\n\nNumber of police people and the crime rate.\n(see M. J. Wooldridge, 2020, Ch. 17) for more details on the problem and solutions."
  },
  {
    "objectID": "slides/w10-IV.html#instrumental-variable-is-another-variable-z_i-that-satisfies",
    "href": "slides/w10-IV.html#instrumental-variable-is-another-variable-z_i-that-satisfies",
    "title": "Instrumental Variable",
    "section": "Instrumental Variable is another variable \\(Z_i\\) that satisfies:",
    "text": "Instrumental Variable is another variable \\(Z_i\\) that satisfies:\n\nRelevance condition:\n\n\\(Z_i\\) has a causal effect on \\(s_i\\) (or strong association with (see Hernan2020?));\n\nExclusion restriction:\n\n\\(Z_i\\) does not affect \\(Y_i\\) directly, except through its potential effect on \\(s_i\\);\n\nIndependence assumption:\n\n\\(Z_i\\) is randomly assigned or “as good as randomly assigned”, the same as\n\\(Z_i\\) is unrelated to the omitted variables \\(A^{'}_i\\), same as\n\\(Z_i\\) and \\(Y_i\\) do not share any common causes\n\n\n\n\n\n\n\n\nImportant\n\n\n(see Angrist & Pischke, 2014 Ch. 3 and 6; Angrist & Pischke, 2009, Ch. 4.; Hernan2020?, Ch. 16; J. M. Wooldridge, 2010, Ch. 8; Söderbom, Teal, & Eberhardt, 2014, Ch. 11)"
  },
  {
    "objectID": "slides/w10-IV.html#instrumental-variable-1",
    "href": "slides/w10-IV.html#instrumental-variable-1",
    "title": "Instrumental Variable",
    "section": "Instrumental Variable (1)",
    "text": "Instrumental Variable (1)"
  },
  {
    "objectID": "slides/w10-IV.html#instrumental-variable-2",
    "href": "slides/w10-IV.html#instrumental-variable-2",
    "title": "Instrumental Variable",
    "section": "Instrumental Variable (2)",
    "text": "Instrumental Variable (2)"
  },
  {
    "objectID": "slides/w10-IV.html#iv-violation-of-the-relevance-condition",
    "href": "slides/w10-IV.html#iv-violation-of-the-relevance-condition",
    "title": "Instrumental Variable",
    "section": "IV: violation of the Relevance condition",
    "text": "IV: violation of the Relevance condition"
  },
  {
    "objectID": "slides/w10-IV.html#iv-violation-of-the-exclusion-restriction",
    "href": "slides/w10-IV.html#iv-violation-of-the-exclusion-restriction",
    "title": "Instrumental Variable",
    "section": "IV: violation of the Exclusion restriction",
    "text": "IV: violation of the Exclusion restriction"
  },
  {
    "objectID": "slides/w10-IV.html#iv-violation-of-the-independence-assumption",
    "href": "slides/w10-IV.html#iv-violation-of-the-independence-assumption",
    "title": "Instrumental Variable",
    "section": "IV: violation of the Independence assumption",
    "text": "IV: violation of the Independence assumption"
  },
  {
    "objectID": "slides/w10-IV.html#iv-regression-angorithm-using-2sls-1",
    "href": "slides/w10-IV.html#iv-regression-angorithm-using-2sls-1",
    "title": "Instrumental Variable",
    "section": "IV regression angorithm using 2SLS (1)",
    "text": "IV regression angorithm using 2SLS (1)\nStage 1: regress endogenous variable \\(s_i\\) on all \\(X_i\\) plus the instrument \\(Z_i\\)\n\\[\ns_i = \\pi_0 + \\pi_1 Z_i + \\rho X_i + \\nu_i\n\\]\n\nCompute fitted values form the stage 1: \\(\\hat s_i = \\pi_0 + \\pi_1 Z_i + \\rho X_i\\).\n\n\nSubstitute \\(s_i\\) with the \\(\\hat{s_i}\\) from the stage 1.\n\n\nStage 2: \\(Y_i = \\alpha^{IV} + \\rho^{IV} \\hat{s_i} + \\beta^{IV} X_i + \\varepsilon^{IV}_i\\)\n\n\nwhere\n\n\\(\\hat{s_i}\\) are the fitted values from the first stage\n\\(\\rho^{IV}\\) is the causal effect of interest from stage two that is asymptotically equal to \\(\\rho\\) , the true effect of interest (\\(\\rho^{IV} \\asymp \\rho\\))"
  },
  {
    "objectID": "slides/w10-IV.html#iv-intuition-using-2sls-2",
    "href": "slides/w10-IV.html#iv-intuition-using-2sls-2",
    "title": "Instrumental Variable",
    "section": "IV intuition using 2SLS (2)",
    "text": "IV intuition using 2SLS (2)"
  },
  {
    "objectID": "slides/w10-IV.html#consistency-and-unbiasedness",
    "href": "slides/w10-IV.html#consistency-and-unbiasedness",
    "title": "Instrumental Variable",
    "section": "Consistency and unbiasedness",
    "text": "Consistency and unbiasedness\n\nIV estimates are not unbiased, but they are consistent (Angrist2001?).\n\nUnbiasedness means the estimator has a sampling distribution centered on the parameter of interest in a sample of any size, while\nConsistency only means that the estimator converges to the population parameter as the sample size grows.\n\n\n\n\n\n\n\n\n\nNote\n\n\nResearchers that use IV should aspire to work with large samples.\n\n\n\n\nNo statistical tests is available for checking consistency"
  },
  {
    "objectID": "slides/w10-IV.html#bad-instruments-1",
    "href": "slides/w10-IV.html#bad-instruments-1",
    "title": "Instrumental Variable",
    "section": "Bad instruments (1)",
    "text": "Bad instruments (1)\n\n\\(Z_i\\) that does not satisfy any of the Relevance condition, Exclusion restriction and Independence assumption;"
  },
  {
    "objectID": "slides/w10-IV.html#bad-instruments-2",
    "href": "slides/w10-IV.html#bad-instruments-2",
    "title": "Instrumental Variable",
    "section": "Bad instruments (2)",
    "text": "Bad instruments (2)\n\n\\(Z_i\\) that correlate with omitted variable (OV) but do not cause changes in it or inflict simultaneity:\n\n\nThey result into much greater upwards shifting bias compare to the OLS;\nFor example the weather in Brazil and supply price and demand quantity of coffee:\n\nweather shifts the supply curve, it is random, thus it seems as a plausible instrument for price in the demand model\nthe weather in Brazil determines supply expectations on futures exchange, thus, it also shifts the demand for coffee before the supply price is affected;"
  },
  {
    "objectID": "slides/w10-IV.html#overidentification-1",
    "href": "slides/w10-IV.html#overidentification-1",
    "title": "Instrumental Variable",
    "section": "Overidentification (1)",
    "text": "Overidentification (1)\n\nnumber of instruments \\(G\\) exceeds the number of endogenous variables \\(K\\).\n\nwhen the IV is overidentified, estimates are biased;\nbias is proportional to \\(K - G\\);\nusing fewer instruments therefore reduces bias;\n\nIf you have few candidates for IV and one endogenous regressor:\n\nselect one IV for the first stage, and\nput the remaining instruments as controls into the second stage"
  },
  {
    "objectID": "slides/w10-IV.html#overidentification-2",
    "href": "slides/w10-IV.html#overidentification-2",
    "title": "Instrumental Variable",
    "section": "Overidentification (2)",
    "text": "Overidentification (2)\nSargan’s overidentification test:\n\n\\(H_0:Cov(Z^{'}_i,\\varepsilon^{IV}_i)=0\\) - the covariance between the instrument and the error term is zero\n\\(H_1:Cov(Z^{'}_i,\\varepsilon^{IV}_i)\\neq0\\)\nThus, by rejecting the \\(H_0\\), we conclude that at least one of the instruments is not valid."
  },
  {
    "objectID": "slides/w10-IV.html#wu-hausman-test-for-endogeneity",
    "href": "slides/w10-IV.html#wu-hausman-test-for-endogeneity",
    "title": "Instrumental Variable",
    "section": "Wu-Hausman test for endogeneity",
    "text": "Wu-Hausman test for endogeneity\nWu-Hausman test for endogeneity tests if the variable that we are worried about is indeed endogenous.\n\n\\(H_0:Cov(s_i,\\varepsilon_i)=0\\) - the covariance between potentially endogenous variable and the error term is zero\n\\(H_1:Cov(s_i,\\varepsilon_i) \\neq 0\\)\nThus, by rejecting the \\(H_0\\), we conclude that there is endogeneity and there might be a need for IV."
  },
  {
    "objectID": "slides/w10-IV.html#references",
    "href": "slides/w10-IV.html#references",
    "title": "Instrumental Variable",
    "section": "References",
    "text": "References\n\n\n\nMP223-Applied Econometrics (SoSe 2023). Author: Eduard Bukin (2023). | GitHub | StudIP | ILIAS.\n\n\n\nAngrist, J. D., & Pischke, J.-S. (2009). Mostly harmless econometrics. Princeton University Press. http://doi.org/10.1515/9781400829828\n\n\nAngrist, J. D., & Pischke, J.-S. (2014). Mastering’metrics: The path from cause to effect. Princeton University Press.\n\n\nSöderbom, M., Teal, F., & Eberhardt, M. (2014). Empirical development economics. ROUTLEDGE. Retrieved from https://www.ebook.de/de/product/21466458/mans_soederbom_francis_teal_markus_eberhardt_empirical_development_economics.html\n\n\nWooldridge, J. M. (2010). Econometric analysis of cross section and panel data. MIT press.\n\n\nWooldridge, M. J. (2020). Introductory econometrics: A modern approach. South-Western. Retrieved from https://www.cengage.uk/shop/isbn/9781337558860"
  },
  {
    "objectID": "slides/w10-IV.html#bad-instruments-3",
    "href": "slides/w10-IV.html#bad-instruments-3",
    "title": "Instrumental Variable",
    "section": "Bad instruments (3)",
    "text": "Bad instruments (3)\n\nWeak instrument \\(Z_i\\):\n\nWhen the instrument \\(Z_i\\) is only weakly correlates with endogenous regressor \\(s_i\\);\nFind a better one!"
  },
  {
    "objectID": "slides/w10-IV.html#weak-instrument-test",
    "href": "slides/w10-IV.html#weak-instrument-test",
    "title": "Instrumental Variable",
    "section": "Weak instrument test:",
    "text": "Weak instrument test:\n\nRun the first stage regression with and without the IV;\nCompare the F-statistics\n\nIf F-statistics with instrument is greater than that without by 5 of more,\nthis is a sign of a strong instrument (Staiger1997?);\n\nThis test does not ensure that our instruments are independent of omitted variable \\(A^{'}_i\\) or \\(Y_i\\);\n(Staiger1997?)"
  },
  {
    "objectID": "slides/w10-IV.html#research-question-and-the-problem",
    "href": "slides/w10-IV.html#research-question-and-the-problem",
    "title": "Instrumental Variable",
    "section": "Research question and the problem",
    "text": "Research question and the problem\n\nWhat are the fundamental causes of the large differences in income per capita across countries?\nwith better “institutions,” more secure property rights, and less distortionary policies,\n\ncountries invest more in physical and human capital, and\nuse these factors more efficiently to\nachieve a greater level of income.\n\nInstitutions are a likely cause of income growth."
  },
  {
    "objectID": "slides/w10-IV.html#endogeneity-problem",
    "href": "slides/w10-IV.html#endogeneity-problem",
    "title": "Instrumental Variable",
    "section": "Endogeneity problem",
    "text": "Endogeneity problem\nWhat could be the ideal experiment to find the effect of institutions on income?\n\nRich economies choose or can afford better institutions.\nEconomies that are different for a variety of reasons\n\nwill differ both in their institutions and in their income per capita.\n\nTo estimate the impact of institutions on income,\n\nwe need a source of exogenous variation in institutions."
  },
  {
    "objectID": "slides/w10-IV.html#identification-strategy",
    "href": "slides/w10-IV.html#identification-strategy",
    "title": "Instrumental Variable",
    "section": "Identification strategy",
    "text": "Identification strategy\nis the manner in which a researcher uses observational data (i.e., data not generated by a randomized trial) to approximate a real experiment (Angrist1991a?)\n\n\n\n\nCurrent performance is caused by:\nCurrent institutions, which are caused by\nEarly institutions, which are caused by\nSettlements types during colonization, which are caused by\nSettlers’ (potential) mortality or colonization risks."
  },
  {
    "objectID": "slides/w10-IV.html#empirical-model-ols-estimator",
    "href": "slides/w10-IV.html#empirical-model-ols-estimator",
    "title": "Instrumental Variable",
    "section": "Empirical model (OLS estimator)",
    "text": "Empirical model (OLS estimator)\n\\[\n\\begin{aligned}\n\\log (\\text{GDP per capita}_i) & = \\beta_0  \\\\\n& + \\beta_1 \\text{Proxy of the institutional quality} \\\\\n& + \\gamma \\text{Control variables} + \\epsilon_i\n\\end{aligned}\n\\]\n\n\\(i\\) is the country;\nDependent variable is the GDP per capita in 1995;\nAs the proxy of the institutional quality, authors used average protection against expropriation risk in 1985-1990 (index/country ranking);\nControls include latitude of the country and continent-specific dummy variables;"
  },
  {
    "objectID": "slides/w10-IV.html#the-data",
    "href": "slides/w10-IV.html#the-data",
    "title": "Instrumental Variable",
    "section": "The data",
    "text": "The data"
  },
  {
    "objectID": "slides/w10-IV.html#ols-estimation",
    "href": "slides/w10-IV.html#ols-estimation",
    "title": "Instrumental Variable",
    "section": "OLS estimation",
    "text": "OLS estimation"
  },
  {
    "objectID": "slides/w10-IV.html#empirical-model-iv-estimator",
    "href": "slides/w10-IV.html#empirical-model-iv-estimator",
    "title": "Instrumental Variable",
    "section": "Empirical model (IV estimator)",
    "text": "Empirical model (IV estimator)\nFirst stage:\n\n\\[\n\\begin{aligned}\n\\text{Proxy of the institutional quality}  & = \\beta_0  \\\\\n& + \\beta_1 \\log (\\text{European settlers mortality in the 16-18th centuries}) \\\\\n& + \\gamma \\text{Control variables} + e_i,\n\\end{aligned}\n\\]\n\nEuropean settlers mortality in the 16-18th centuries is the precise number of how many settlers died in the country that they tried to colonize.\n\n\n\nSecond stage:\n\n\n\\[\n\\begin{aligned}\n\\log (\\text{GDP per capita}_i) & = \\beta_0^{IV}  \\\\\n& + \\beta_1^{IV} \\widehat{ \\text{Proxy of the institutional quality}} \\\\\n& + \\gamma^{IV} \\text{Control variables} + \\epsilon_i^{IV},\n\\end{aligned}\n\\]\n\n\\(\\widehat{ \\text{Proxy of the institutional quality}}\\) are fitted values from the first stage."
  },
  {
    "objectID": "slides/w10-IV.html#iv-results",
    "href": "slides/w10-IV.html#iv-results",
    "title": "Instrumental Variable",
    "section": "IV results",
    "text": "IV results"
  },
  {
    "objectID": "slides/w10-IVa.html#short-regression",
    "href": "slides/w10-IVa.html#short-regression",
    "title": "Instrumental Variable",
    "section": "Short Regression",
    "text": "Short Regression\n\\[\nY_i = \\alpha^S + \\rho^S s_i + \\beta^S X_i + \\varepsilon_i^S\n\\qquad(1.1)\\]\n\nannual earning \\(Y_i\\)\nyears of education \\(s_i\\)\n\\(X_i\\) vector of other control variables, such as experience.\n\n\nIs the ceteris paribus fulfilled in Equation 1.1?\n\nIs control for experience and education sufficient?\nAt a given experience/education level, are more- and less-educated workers equally able and diligent? (see Joshua D. Angrist & Pischke, 2014, Ch. 6)"
  },
  {
    "objectID": "slides/w10-IVa.html#long-regression",
    "href": "slides/w10-IVa.html#long-regression",
    "title": "Instrumental Variable",
    "section": "Long Regression",
    "text": "Long Regression\n\\[\nY_i = \\alpha + \\rho s_i + \\beta X_i + \\gamma A^{'}_{i} + \\varepsilon_i\n\\qquad(1.2)\\]\n\nwhere \\(A^{'}_{i}\\) is the ability variable that we desire to have in order to ensure the unbiased estimates of \\(\\rho\\).\nOmitting \\(A^{'}_{i}\\) causes a selection bias or endogeneity:\n\n\\(\\rho^{S} = \\rho + \\underbrace{\\delta_{A^{'} s} \\times \\gamma}_{\\text{ability bias}}\\)"
  },
  {
    "objectID": "slides/w10-IVa.html#definition",
    "href": "slides/w10-IVa.html#definition",
    "title": "Instrumental Variable",
    "section": "Definition",
    "text": "Definition\n\nConsider following LONG and SHORT models:\n\n\n\\[\n\\begin{aligned}\nY_i &= \\alpha + \\rho s_i + \\beta X_i + \\gamma A^{'}_{i} + \\varepsilon_i , && \\text{long}\\\\\nY_i &= \\alpha^S + \\rho^S s_i + \\beta^S X_i + \\varepsilon_i^S , && \\text{short}\n\\end{aligned}\n\\]\n\n\\(s_i\\) is the causal variable of interest (education)\n\\(A^{'}_{i}\\) is the vector of control variables that we desire to have in order to ensure unbiased estimates of \\(\\rho\\);\nVariable \\(s_i\\) is endogenous if it correlates with the error terms \\(\\varepsilon^{S}_i\\) :\n\n\n\n\\[\nCov(s_i, \\varepsilon^{S}_i) \\neq 0\n\\]"
  },
  {
    "objectID": "slides/w10-IVa.html#endogeneity-in-practice",
    "href": "slides/w10-IVa.html#endogeneity-in-practice",
    "title": "Instrumental Variable",
    "section": "Endogeneity in practice:",
    "text": "Endogeneity in practice:\n\n\n\nvariation in the independent variable \\(s_i\\) (education) is not “random” as compared to the variation in the dependent variable \\(Y_i\\), but\nan external process \\(U\\) affects variation in both \\(s_i\\) and \\(Y_i\\);\nthus, \\(s_i\\) is endogenous to \\(Y_i\\);\n\n\n\n\n\n\nIf variance of \\(s_i\\) is truly independent of \\(Y_i\\), \\(s_i\\) is exogenous."
  },
  {
    "objectID": "slides/w10-IVa.html#causes-of-endogeneity",
    "href": "slides/w10-IVa.html#causes-of-endogeneity",
    "title": "Instrumental Variable",
    "section": "Causes of endogeneity",
    "text": "Causes of endogeneity\n\nOmitted Variable Bias (familiar)\nMeasurement Error\nSimultaneity"
  },
  {
    "objectID": "slides/w10-IVa.html#ovb",
    "href": "slides/w10-IVa.html#ovb",
    "title": "Instrumental Variable",
    "section": "OVB",
    "text": "OVB"
  },
  {
    "objectID": "slides/w10-IVa.html#measurement-error",
    "href": "slides/w10-IVa.html#measurement-error",
    "title": "Instrumental Variable",
    "section": "Measurement error",
    "text": "Measurement error"
  },
  {
    "objectID": "slides/w10-IVa.html#measurement-error-1",
    "href": "slides/w10-IVa.html#measurement-error-1",
    "title": "Instrumental Variable",
    "section": "Measurement error",
    "text": "Measurement error\n\nWe estimate a long model: \\(Y_i = \\alpha + \\beta s^*_i + e_i\\) ,\n\nbut \\(s^*_i\\) is unavailable, we only have \\(s_i = s^*_i + m_i\\) instead,\n\\(m_i\\) is a systematic measurement error,\n\\(E[m_i] =0\\) and \\(Cov(s^*_i, m_i) = Cov(e_i, m_i) = 0\\).\n\nDesired coefficient \\(\\beta = \\frac{Cov(Y_i, s_i)}{Var(s_i)}\\)\nBut with the erroneous data, we estimate biased coefficient \\(\\beta_b\\)\n\n\n\\[\n\\begin{aligned}\n\\beta_b & =  \n\\frac{Cov(Y_i, s_i)}{Var(s_i)}  =\n\\frac{Cov(a+\\beta s^*_i + e_i, s^*_i + m_i)}{Var(s_i)} \\\\  \n& =  \\frac{\\beta \\cdot Cov(s^*_i, s^*_i)}{Var(s_i)}  =  \\beta \\frac{Var(s^{*}_i)}{Var(s_i)}\n\\end{aligned}\n\\]\n\n(see Joshua D. Angrist & Pischke, 2014, Ch. 6)"
  },
  {
    "objectID": "slides/w10-IVa.html#simultaneity",
    "href": "slides/w10-IVa.html#simultaneity",
    "title": "Instrumental Variable",
    "section": "Simultaneity",
    "text": "Simultaneity"
  },
  {
    "objectID": "slides/w10-IVa.html#simultaneity-1",
    "href": "slides/w10-IVa.html#simultaneity-1",
    "title": "Instrumental Variable",
    "section": "Simultaneity",
    "text": "Simultaneity\n\nSimultaneity occurs if at least two variables are jointly determined.\n\nA typical case is when observed outcomes are the result of separate behavioral mechanisms that are coordinated in an equilibrium.\n\nThe prototypical case is a system of demand and supply equations:\n\n\\(D(p)\\) = how high would demand be if the price was set to \\(p\\)?\n\\(S(p)\\) = how high would supply be if the price was set to \\(p\\)?\n\nNumber of police people and the crime rate.\n(see M. J. Wooldridge, 2020, Ch. 17) for more details on the problem and solutions."
  },
  {
    "objectID": "slides/w10-IVa.html#instrumental-variable-is-another-variable-z_i-that-satisfies",
    "href": "slides/w10-IVa.html#instrumental-variable-is-another-variable-z_i-that-satisfies",
    "title": "Instrumental Variable",
    "section": "Instrumental Variable is another variable \\(Z_i\\) that satisfies:",
    "text": "Instrumental Variable is another variable \\(Z_i\\) that satisfies:\n\nRelevance condition:\n\n\\(Z_i\\) has a causal effect on \\(s_i\\) (or strong association with (see Hernan2020?));\n\nExclusion restriction:\n\n\\(Z_i\\) does not affect \\(Y_i\\) directly, except through its potential effect on \\(s_i\\);\n\nIndependence assumption:\n\n\\(Z_i\\) is randomly assigned or “as good as randomly assigned”, the same as\n\\(Z_i\\) is unrelated to the omitted variables \\(A^{'}_i\\), same as\n\\(Z_i\\) and \\(Y_i\\) do not share any common causes\n\n\n\n\n\n\n\n\nImportant\n\n\n(see Angrist & Pischke, 2014 Ch. 3 and 6; Angrist & Pischke, 2009, Ch. 4.; Hernan2020?, Ch. 16; J. M. Wooldridge, 2010, Ch. 8; Söderbom, Teal, & Eberhardt, 2014, Ch. 11)"
  },
  {
    "objectID": "slides/w10-IVa.html#instrumental-variable-1",
    "href": "slides/w10-IVa.html#instrumental-variable-1",
    "title": "Instrumental Variable",
    "section": "Instrumental Variable (1)",
    "text": "Instrumental Variable (1)"
  },
  {
    "objectID": "slides/w10-IVa.html#instrumental-variable-2",
    "href": "slides/w10-IVa.html#instrumental-variable-2",
    "title": "Instrumental Variable",
    "section": "Instrumental Variable (2)",
    "text": "Instrumental Variable (2)"
  },
  {
    "objectID": "slides/w10-IVa.html#iv-violation-of-the-relevance-condition",
    "href": "slides/w10-IVa.html#iv-violation-of-the-relevance-condition",
    "title": "Instrumental Variable",
    "section": "IV: violation of the Relevance condition",
    "text": "IV: violation of the Relevance condition"
  },
  {
    "objectID": "slides/w10-IVa.html#iv-violation-of-the-exclusion-restriction",
    "href": "slides/w10-IVa.html#iv-violation-of-the-exclusion-restriction",
    "title": "Instrumental Variable",
    "section": "IV: violation of the Exclusion restriction",
    "text": "IV: violation of the Exclusion restriction"
  },
  {
    "objectID": "slides/w10-IVa.html#iv-violation-of-the-independence-assumption",
    "href": "slides/w10-IVa.html#iv-violation-of-the-independence-assumption",
    "title": "Instrumental Variable",
    "section": "IV: violation of the Independence assumption",
    "text": "IV: violation of the Independence assumption"
  },
  {
    "objectID": "slides/w10-IVa.html#iv-regression-angorithm-using-2sls-1",
    "href": "slides/w10-IVa.html#iv-regression-angorithm-using-2sls-1",
    "title": "Instrumental Variable",
    "section": "IV regression angorithm using 2SLS (1)",
    "text": "IV regression angorithm using 2SLS (1)\nStage 1: regress endogenous variable \\(s_i\\) on all \\(X_i\\) plus the instrument \\(Z_i\\)\n\\[\ns_i = \\pi_0 + \\pi_1 Z_i + \\rho X_i + \\nu_i\n\\]\n\nCompute fitted values form the stage 1: \\(\\hat s_i = \\pi_0 + \\pi_1 Z_i + \\rho X_i\\).\n\n\nSubstitute \\(s_i\\) with the \\(\\hat{s_i}\\) from the stage 1.\n\n\nStage 2: \\(Y_i = \\alpha^{IV} + \\rho^{IV} \\hat{s_i} + \\beta^{IV} X_i + \\varepsilon^{IV}_i\\)\n\n\nwhere\n\n\\(\\hat{s_i}\\) are the fitted values from the first stage\n\\(\\rho^{IV}\\) is the causal effect of interest from stage two that is asymptotically equal to \\(\\rho\\) , the true effect of interest (\\(\\rho^{IV} \\asymp \\rho\\))"
  },
  {
    "objectID": "slides/w10-IVa.html#manual-iv-in-r",
    "href": "slides/w10-IVa.html#manual-iv-in-r",
    "title": "Instrumental Variable",
    "section": "Manual IV in R",
    "text": "Manual IV in R"
  },
  {
    "objectID": "slides/w10-IVa.html#consistency-and-unbiasedness",
    "href": "slides/w10-IVa.html#consistency-and-unbiasedness",
    "title": "Instrumental Variable",
    "section": "Consistency and unbiasedness",
    "text": "Consistency and unbiasedness\n\nIV estimates are not unbiased, but they are consistent (Joshua D. Angrist & Krueger, 2001).\n\nUnbiasedness means the estimator has a sampling distribution centered on the parameter of interest in a sample of any size, while\nConsistency only means that the estimator converges to the population parameter as the sample size grows.\n\n\n\n\n\n\n\n\n\nNote\n\n\nResearchers that use IV should aspire to work with large samples.\n\n\n\n\nNo statistical tests is available for checking consistency"
  },
  {
    "objectID": "slides/w10-IVa.html#bad-instruments-1",
    "href": "slides/w10-IVa.html#bad-instruments-1",
    "title": "Instrumental Variable",
    "section": "Bad instruments (1)",
    "text": "Bad instruments (1)\n\n\\(Z_i\\) that does not satisfy any of the Relevance condition, Exclusion restriction and Independence assumption;"
  },
  {
    "objectID": "slides/w10-IVa.html#bad-instruments-2",
    "href": "slides/w10-IVa.html#bad-instruments-2",
    "title": "Instrumental Variable",
    "section": "Bad instruments (2)",
    "text": "Bad instruments (2)\n\n\\(Z_i\\) that correlate with omitted variable (OV) but do not cause changes in it or inflict simultaneity:\n\n\nThey result into much greater upwards shifting bias compare to the OLS;\nFor example the weather in Brazil and supply price and demand quantity of coffee:\n\nweather shifts the supply curve, it is random, thus it seems as a plausible instrument for price in the demand model\nthe weather in Brazil determines supply expectations on futures exchange, thus, it also shifts the demand for coffee before the supply price is affected;"
  },
  {
    "objectID": "slides/w10-IVa.html#bad-instruments-3",
    "href": "slides/w10-IVa.html#bad-instruments-3",
    "title": "Instrumental Variable",
    "section": "Bad instruments (3)",
    "text": "Bad instruments (3)\n\nWeak instrument \\(Z_i\\):\n\nWhen the instrument \\(Z_i\\) is only weakly correlates with endogenous regressor \\(s_i\\);\nFind a better one!"
  },
  {
    "objectID": "slides/w10-IVa.html#weak-instrument-test",
    "href": "slides/w10-IVa.html#weak-instrument-test",
    "title": "Instrumental Variable",
    "section": "Weak instrument test:",
    "text": "Weak instrument test:\n\nRun the first stage regression with and without the IV;\nCompare the F-statistics\n\nIf F-statistics with instrument is greater than that without by 5 of more,\nthis is a sign of a strong instrument (Staiger & Stock, 1997);\n\nThis test does not ensure that our instruments are independent of omitted variable \\(A^{'}_i\\) or \\(Y_i\\);\nStaiger & Stock (1997)"
  },
  {
    "objectID": "slides/w10-IVa.html#overidentification-1",
    "href": "slides/w10-IVa.html#overidentification-1",
    "title": "Instrumental Variable",
    "section": "Overidentification (1)",
    "text": "Overidentification (1)\n\nnumber of instruments \\(G\\) exceeds the number of endogenous variables \\(K\\).\n\nwhen the IV is overidentified, estimates are biased;\nbias is proportional to \\(K - G\\);\nusing fewer instruments therefore reduces bias;\n\nIf you have few candidates for IV and one endogenous regressor:\n\nselect one IV for the first stage, and\nput the remaining instruments as controls into the second stage"
  },
  {
    "objectID": "slides/w10-IVa.html#overidentification-2",
    "href": "slides/w10-IVa.html#overidentification-2",
    "title": "Instrumental Variable",
    "section": "Overidentification (2)",
    "text": "Overidentification (2)\nSargan’s overidentification test:\n\n\\(H_0:Cov(Z^{'}_i,\\varepsilon^{IV}_i)=0\\) - the covariance between the instrument and the error term is zero\n\\(H_1:Cov(Z^{'}_i,\\varepsilon^{IV}_i)\\neq0\\)\nThus, by rejecting the \\(H_0\\), we conclude that at least one of the instruments is not valid."
  },
  {
    "objectID": "slides/w10-IVa.html#wu-hausman-test-for-endogeneity",
    "href": "slides/w10-IVa.html#wu-hausman-test-for-endogeneity",
    "title": "Instrumental Variable",
    "section": "Wu-Hausman test for endogeneity",
    "text": "Wu-Hausman test for endogeneity\nWu-Hausman test for endogeneity tests if the variable that we are worried about is indeed endogenous.\n\n\\(H_0:Cov(s_i,\\varepsilon_i)=0\\) - the covariance between potentially endogenous variable and the error term is zero\n\\(H_1:Cov(s_i,\\varepsilon_i) \\neq 0\\)\nThus, by rejecting the \\(H_0\\), we conclude that there is endogeneity and there might be a need for IV."
  },
  {
    "objectID": "slides/w10-IVa.html#research-question-and-the-problem",
    "href": "slides/w10-IVa.html#research-question-and-the-problem",
    "title": "Instrumental Variable",
    "section": "Research question and the problem",
    "text": "Research question and the problem\n\nWhat are the fundamental causes of the large differences in income per capita across countries?\nwith better “institutions,” more secure property rights, and less distortionary policies,\n\ncountries invest more in physical and human capital, and\nuse these factors more efficiently to\nachieve a greater level of income.\n\nInstitutions are a likely cause of income growth."
  },
  {
    "objectID": "slides/w10-IVa.html#endogeneity-problem",
    "href": "slides/w10-IVa.html#endogeneity-problem",
    "title": "Instrumental Variable",
    "section": "Endogeneity problem",
    "text": "Endogeneity problem\nWhat could be the ideal experiment to find the effect of institutions on income?\n\nRich economies choose or can afford better institutions.\nEconomies that are different for a variety of reasons\n\nwill differ both in their institutions and in their income per capita.\n\nTo estimate the impact of institutions on income,\n\nwe need a source of exogenous variation in institutions."
  },
  {
    "objectID": "slides/w10-IVa.html#identification-strategy",
    "href": "slides/w10-IVa.html#identification-strategy",
    "title": "Instrumental Variable",
    "section": "Identification strategy",
    "text": "Identification strategy\nis the manner in which a researcher uses observational data (i.e., data not generated by a randomized trial) to approximate a real experiment (Joshua D. Angrist & Krueger, 1991)\n\n\n\n\nCurrent performance is caused by:\nCurrent institutions, which are caused by\nEarly institutions, which are caused by\nSettlements types during colonization, which are caused by\nSettlers’ (potential) mortality or colonization risks."
  },
  {
    "objectID": "slides/w10-IVa.html#empirical-model-ols-estimator",
    "href": "slides/w10-IVa.html#empirical-model-ols-estimator",
    "title": "Instrumental Variable",
    "section": "Empirical model (OLS estimator)",
    "text": "Empirical model (OLS estimator)\n\\[\n\\begin{aligned}\n\\log (\\text{GDP per capita}_i) & = \\beta_0  \\\\\n& + \\beta_1 \\text{Proxy for institutions} \\\\\n& + \\gamma \\text{Control variables} + \\epsilon_i\n\\end{aligned}\n\\]\n\n\\(i\\) is the country;\nDependent variable is the GDP per capita in 1995;\nAs the proxy of the institutional quality, authors used average protection against expropriation risk in 1985-1990 (index/country ranking);\nControls include latitude of the country and continent-specific dummy variables;"
  },
  {
    "objectID": "slides/w10-IVa.html#the-data",
    "href": "slides/w10-IVa.html#the-data",
    "title": "Instrumental Variable",
    "section": "The data",
    "text": "The data"
  },
  {
    "objectID": "slides/w10-IVa.html#ols-estimation",
    "href": "slides/w10-IVa.html#ols-estimation",
    "title": "Instrumental Variable",
    "section": "OLS estimation",
    "text": "OLS estimation"
  },
  {
    "objectID": "slides/w10-IVa.html#empirical-model-iv-estimator",
    "href": "slides/w10-IVa.html#empirical-model-iv-estimator",
    "title": "Instrumental Variable",
    "section": "Empirical model (IV estimator)",
    "text": "Empirical model (IV estimator)\nFirst stage:\n\n\\[\n\\begin{aligned}\n\\text{Proxy for institutions}  & = \\beta_0  \\\\\n& + \\beta_1 \\log (\\text{Settlers mortality in 16-18th cent.}) \\\\\n& + \\gamma \\text{Control variables} + e_i,\n\\end{aligned}\n\\]\n\nEuropean settlers mortality in the 16-18th centuries is the precise number of how many settlers died in the country that they tried to colonize.\n\n\n\nSecond stage:\n\n\n\\[\n\\begin{aligned}\n\\log (\\text{GDP per capita}_i) & = \\beta_0^{IV}  \\\\\n& + \\beta_1^{IV} \\widehat{ \\text{Proxy for institutions}} \\\\\n& + \\gamma^{IV} \\text{Control variables} + \\epsilon_i^{IV},\n\\end{aligned}\n\\]\n\n\\(\\widehat{ \\{ \\text{Proxy for institutions} \\}}\\) are fitted values from the first stage."
  },
  {
    "objectID": "slides/w10-IVa.html#iv-results",
    "href": "slides/w10-IVa.html#iv-results",
    "title": "Instrumental Variable",
    "section": "IV results",
    "text": "IV results"
  },
  {
    "objectID": "slides/w10-IVa.html#references",
    "href": "slides/w10-IVa.html#references",
    "title": "Instrumental Variable",
    "section": "References",
    "text": "References\n\n\n\nMP223-Applied Econometrics (SoSe 2023). Author: Eduard Bukin (2023). | GitHub | StudIP | ILIAS.\n\n\n\nAcemoglu, D., Johnson, S., & Robinson, J. A. (2001). The colonial origins of comparative development: An empirical investigation. American Economic Review, 91(5), 1369–1401. http://doi.org/10.1257/aer.91.5.1369\n\n\nAngrist, Joshua D., & Krueger, A. B. (1991). Does compulsory school attendance affect schooling and earnings? The Quarterly Journal of Economics, 106(4), 979–1014. http://doi.org/10.2307/2937954\n\n\nAngrist, Joshua D., & Krueger, A. B. (2001). Instrumental variables and the search for identification: From supply and demand to natural experiments. Journal of Economic Perspectives, 15(4), 69–85. http://doi.org/10.1257/jep.15.4.69\n\n\nAngrist, Joshua D., & Pischke, J.-S. (2009). Mostly harmless econometrics. Princeton University Press. http://doi.org/10.1515/9781400829828\n\n\nAngrist, Joshua D., & Pischke, J.-S. (2014). Mastering’metrics: The path from cause to effect. Princeton University Press.\n\n\nBuckles, K. S., & Hungerman, D. M. (2013). Season of birth and later outcomes: Old questions, new answers. Review of Economics and Statistics, 95(3), 711–724. http://doi.org/10.1162/rest_a_00314\n\n\nCard, D. (1994). Earnings, schooling, and ability revisited. National Bureau of Economic Research.\n\n\nDuflo, E. (2001). Schooling and labor market consequences of school construction in indonesia: Evidence from an unusual policy experiment. American Economic Review, 91(4), 795–813. http://doi.org/10.1257/aer.91.4.795\n\n\nHernán, M. A., & Robins, J. M. (2020). Causal Inference: What If. Chapman & Hall/CRC.\n\n\nImbens, G. W. (2020). Potential outcome and directed acyclic graph approaches to causality: Relevance for empirical practice in economics. Journal of Economic Literature, 58(4), 1129–1179. http://doi.org/10.1257/jel.20191597\n\n\nSöderbom, M., Teal, F., & Eberhardt, M. (2014). Empirical development economics. ROUTLEDGE. Retrieved from https://www.ebook.de/de/product/21466458/mans_soederbom_francis_teal_markus_eberhardt_empirical_development_economics.html\n\n\nStaiger, D., & Stock, J. H. (1997). Instrumental variables regression with weak instruments. Econometrica, 65(3), 557. http://doi.org/10.2307/2171753\n\n\nWooldridge, J. M. (2010). Econometric analysis of cross section and panel data. MIT press.\n\n\nWooldridge, M. J. (2020). Introductory econometrics: A modern approach. South-Western. Retrieved from https://www.cengage.uk/shop/isbn/9781337558860"
  },
  {
    "objectID": "slides/w10-IVa.html#instrumental-variable",
    "href": "slides/w10-IVa.html#instrumental-variable",
    "title": "Instrumental Variable",
    "section": "Instrumental Variable",
    "text": "Instrumental Variable\nis another variable \\(Z_i\\) that affects only endogenous regressor \\(s_i\\) and satisfies:\n\n\n\n\n\n\n\n\nRelevance condition:\nExclusion restriction:\nIndependence assumption:\n\n\n\n\n(see Joshua D. Angrist & Pischke, 2014 Ch. 3 and 6; Joshua D. Angrist & Pischke, 2009, Ch. 4.; Hernán & Robins, 2020, Ch. 16; J. M. Wooldridge, 2010, Ch. 8; Söderbom, Teal, & Eberhardt, 2014, Ch. 11; Imbens, 2020)"
  },
  {
    "objectID": "slides/w10-IVa.html#relevance-condition",
    "href": "slides/w10-IVa.html#relevance-condition",
    "title": "Instrumental Variable",
    "section": "1. Relevance condition:",
    "text": "1. Relevance condition:\n\n\n\n\\(Z_i\\) has a causal effect on \\(s_i\\);\n\n\n\nViolation of the relevance condition:"
  },
  {
    "objectID": "slides/w10-IVa.html#exclusion-restriction",
    "href": "slides/w10-IVa.html#exclusion-restriction",
    "title": "Instrumental Variable",
    "section": "2. Exclusion restriction:",
    "text": "2. Exclusion restriction:\n\n\n\n\\(Z_i\\) does not affect \\(Y_i\\) directly, except through its potential effect on \\(s_i\\);\n\n\n\nViolation of the exclusion restriction:"
  },
  {
    "objectID": "slides/w10-IVa.html#independence-assumption",
    "href": "slides/w10-IVa.html#independence-assumption",
    "title": "Instrumental Variable",
    "section": "3. Independence assumption:",
    "text": "3. Independence assumption:\n\n\n\n\\(Z_i\\) is randomly assigned or “as good as randomly assigned”, the same as\n\\(Z_i\\) is unrelated to the omitted variables \\(A^{'}_i\\), same as\n\\(Z_i\\) and \\(Y_i\\) do not share any common causes\n\n\n\nViolation of the independence assumption:"
  },
  {
    "objectID": "slides/w10-IVa.html#wage-and-education-again-1",
    "href": "slides/w10-IVa.html#wage-and-education-again-1",
    "title": "Instrumental Variable",
    "section": "Wage and Education (again)",
    "text": "Wage and Education (again)\n\\[\nY_i = \\alpha^S + \\rho^S s_i + \\beta^S X_i + \\varepsilon_i^S\n\\]\n\nWe know that estimate of years of education \\(s_i\\) is biased because of the OVB (ability bias).\nThink of an RCT experiment that could help to estimate true causal effect of \\(s_i\\) on income!\nWhat instrument \\(Z_i\\) can we use for education?"
  },
  {
    "objectID": "slides/w10-IVa.html#fantastic-ivs-and-how-to-find-them",
    "href": "slides/w10-IVa.html#fantastic-ivs-and-how-to-find-them",
    "title": "Instrumental Variable",
    "section": "Fantastic IVs and how to find them…",
    "text": "Fantastic IVs and how to find them…\n\nUse theory!\n\nhuman capital theory suggests that people make schooling choices by comparing the costs and benefits of alternatives.\n\nThink and speculate:\n\nWhat is the ideal experiment that could capture the effect of schooling on education?\nWhat are the forces you’d like to manipulate and the factors you’d like to hold constant?\nWhat are the other processes that are independent of wage, but may affect schooling?\n\nAnalyze, what were/are the policies/environments that could mimic the experimental setting?\n\n\n\n\n\nReasoning on how researcher use theory and available observational data to approximate real experiment is called Identification strategy!"
  },
  {
    "objectID": "slides/w10-IVa.html#fantastic-ivs-for-education",
    "href": "slides/w10-IVa.html#fantastic-ivs-for-education",
    "title": "Instrumental Variable",
    "section": "Fantastic IVs for education",
    "text": "Fantastic IVs for education\n\nLoan policies or other subsidies that vary independently of ability or earnings potential\nRegion and time variation in school construction (Duflo, 2001)\nProximity to college(Card, 1994)\nQuarter of birth (Joshua D. Angrist & Krueger, 1991)\nParents education (Buckles & Hungerman, 2013)\nNumber of siblings"
  },
  {
    "objectID": "slides/w10-IVa.html#using-parents-education-as-the-iv-for-education",
    "href": "slides/w10-IVa.html#using-parents-education-as-the-iv-for-education",
    "title": "Instrumental Variable",
    "section": "Using parents education as the IV for education",
    "text": "Using parents education as the IV for education\n\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(modelsummary)\ndta <-\n  read_csv(\"education_parents.csv\") %>% \n  mutate(lwagehour = log(wage/hours)) %>% \n  mutate(parents_edu = feduc + meduc)\nglimpse(dta)\n\n\n\nRows: 722\nColumns: 19\n$ wage        <dbl> 769, 808, 825, 650, 562, 600, 1154, 1000, 930, 900, 1318, …\n$ hours       <dbl> 40, 50, 40, 40, 40, 40, 45, 40, 43, 45, 38, 40, 50, 45, 40…\n$ IQ          <dbl> 93, 119, 108, 96, 74, 91, 111, 95, 132, 125, 119, 118, 105…\n$ KWW         <dbl> 35, 41, 46, 32, 27, 24, 37, 44, 44, 40, 24, 47, 37, 39, 36…\n$ educ        <dbl> 12, 18, 14, 12, 11, 10, 15, 12, 18, 15, 16, 16, 10, 15, 11…\n$ exper       <dbl> 11, 11, 11, 13, 14, 13, 13, 16, 8, 4, 7, 9, 17, 6, 19, 10,…\n$ tenure      <dbl> 2, 16, 9, 7, 5, 0, 1, 16, 13, 3, 2, 9, 2, 9, 10, 4, 3, 8, …\n$ age         <dbl> 31, 37, 33, 32, 34, 30, 36, 36, 38, 30, 28, 34, 35, 36, 38…\n$ married     <dbl> 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ black       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ south       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ urban       <dbl> 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ sibs        <dbl> 1, 1, 1, 4, 10, 1, 2, 1, 1, 2, 3, 1, 1, 3, 2, 2, 5, 2, 0, …\n$ brthord     <dbl> 2, NA, 2, 3, 6, 2, 3, 1, 1, NA, 1, 1, 2, 3, 3, 1, 1, 1, 1,…\n$ meduc       <dbl> 8, 14, 14, 12, 6, 8, 14, 12, 13, 12, 10, 12, 6, 12, 10, 12…\n$ feduc       <dbl> 8, 14, 14, 12, 11, 8, 5, 11, 14, 12, 10, 12, 8, 10, 8, 12,…\n$ lwage       <dbl> 6.645091, 6.694562, 6.715384, 6.476973, 6.331502, 6.396930…\n$ lwagehour   <dbl> 2.956212, 2.782539, 3.026504, 2.788093, 2.642622, 2.708050…\n$ parents_edu <dbl> 16, 28, 28, 24, 17, 16, 19, 23, 27, 24, 20, 24, 14, 22, 18…"
  },
  {
    "objectID": "slides/w10-IVa.html#estimating-iv-manually",
    "href": "slides/w10-IVa.html#estimating-iv-manually",
    "title": "Instrumental Variable",
    "section": "Estimating IV manually",
    "text": "Estimating IV manually\n\n# No IV\nols <-  lm(log(wage) ~ educ + exper + I(exper^2), data = dta)\n\n# No IV but with controls for IQ\nols_iq <-  lm(log(wage) ~ educ + exper + I(exper^2) + IQ, data = dta)\n\n# First stage\nfirst_stage <- lm(educ ~ parents_edu + exper + I(exper^2), data = dta)\n\n# Fitted values of endogenous regressor\ndta_fitted <- dta %>% mutate(educ_fit = fitted(first_stage))\n\n# Second stage\nsecond_stage <- lm(log(wage) ~ educ_fit + exper + I(exper^2), data = dta_fitted)\n\n\n\n\n\n \n  \n      \n    OLS \n    OLS (with ability proxi) \n    1 stage (par. educ.) \n     2 stage (par. educ.) \n  \n \n\n  \n    Education \n    0.078*** (0.007) \n    0.058*** (0.008) \n     \n    0.146*** (0.019) \n  \n  \n    Parents educ. \n     \n     \n    0.148*** (0.013) \n     \n  \n  \n    Experience \n    0.009 (0.015) \n    0.010 (0.015) \n    −0.021 (0.072) \n    0.008 (0.016) \n  \n  \n    Experience sq. \n    0.001 (0.001) \n    0.001 (0.001) \n    −0.007* (0.003) \n    0.001+ (0.001) \n  \n  \n    Ability proxi \n     \n    0.006*** (0.001) \n     \n     \n  \n  \n    Num.Obs. \n    722 \n    722 \n    722 \n    722 \n  \n  \n    R2 \n    0.141 \n    0.169 \n    0.326 \n    0.076 \n  \n  \n    R2 Adj. \n    0.137 \n    0.164 \n    0.323 \n    0.072 \n  \n  \n    Log.Lik. \n    −341.745 \n    −329.737 \n    −1462.726 \n    −368.199 \n  \n  \n    F \n    39.259 \n    36.461 \n    115.813 \n    19.574"
  },
  {
    "objectID": "slides/w10-IVa.html#using-sibilins-number-as-the-iv-for-education",
    "href": "slides/w10-IVa.html#using-sibilins-number-as-the-iv-for-education",
    "title": "Instrumental Variable",
    "section": "Using sibilins number as the IV for education",
    "text": "Using sibilins number as the IV for education\n\nlibrary(ivreg)\niv_fit2 <- ivreg(\n  log(wage) ~ educ + exper + I(exper ^ 2) | sibs + exper + I(exper ^ 2) ,\n  data = dta \n)\n\n\n\n\n\n \n  \n      \n    OLS \n    First stage (parents education) \n    Second stage (parents education) \n    Second stage number of sibilins \n  \n \n\n  \n    Education, years \n    0.078*** (0.007) \n     \n    0.146*** (0.019) \n    0.128*** (0.033) \n  \n  \n    Parents education, years \n     \n    0.148*** (0.013) \n     \n     \n  \n  \n    Experience \n    0.009 (0.015) \n    −0.021 (0.072) \n    0.008 (0.016) \n    0.008 (0.016) \n  \n  \n    Experience square \n    0.001 (0.001) \n    −0.007* (0.003) \n    0.001+ (0.001) \n    0.001 (0.001) \n  \n  \n    Num.Obs. \n    722 \n    722 \n    722 \n    722 \n  \n  \n    R2 \n    0.141 \n    0.326 \n    0.076 \n    0.085 \n  \n  \n    R2 Adj. \n    0.137 \n    0.323 \n    0.072 \n    0.081 \n  \n  \n    Log.Lik. \n    −341.745 \n    −1462.726 \n    −368.199 \n     \n  \n  \n    F \n    39.259 \n    115.813 \n    19.574"
  },
  {
    "objectID": "slides/w10-IVa.html#iv-regression-algorithm-using-2sls-1",
    "href": "slides/w10-IVa.html#iv-regression-algorithm-using-2sls-1",
    "title": "Instrumental Variable",
    "section": "IV regression algorithm using 2SLS (1)",
    "text": "IV regression algorithm using 2SLS (1)\nStage 1: regress endogenous variable \\(s_i\\) on all \\(X_i\\) plus the instrument \\(Z_i\\)\n\\[\ns_i = \\pi_0 + \\pi_1 Z_i + \\rho X_i + \\nu_i\n\\]\n\nCompute fitted values form the stage 1: \\(\\hat s_i = \\pi_0 + \\pi_1 Z_i + \\rho X_i\\).\n\n\nSubstitute \\(s_i\\) with the \\(\\hat{s_i}\\) from the stage 1.\n\n\nStage 2: \\(Y_i = \\alpha^{IV} + \\rho^{IV} \\hat{s_i} + \\beta^{IV} X_i + \\varepsilon^{IV}_i\\)\n\n\nwhere\n\n\\(\\hat{s_i}\\) are the fitted values from the first stage\n\\(\\rho^{IV}\\) is the causal effect of interest from stage two that is asymptotically equal to \\(\\rho\\) , the true effect of interest (\\(\\rho^{IV} \\asymp \\rho\\))"
  },
  {
    "objectID": "slides/w10-IVa.html#using-siblings-number-as-the-iv-for-education",
    "href": "slides/w10-IVa.html#using-siblings-number-as-the-iv-for-education",
    "title": "Instrumental Variable",
    "section": "Using siblings number as the IV for education",
    "text": "Using siblings number as the IV for education\n\nlibrary(ivreg)\niv_fit2 <- ivreg(\n  log(wage) ~ educ + exper + I(exper ^ 2) | sibs + exper + I(exper ^ 2) ,\n  data = dta )\n\n\n\n\n\n \n  \n      \n    OLS \n    OLS (with ability proxi) \n    1 stage (par. educ.) \n     2 stage (par. educ.) \n     2 stage (siblings) \n  \n \n\n  \n    Education \n    0.078*** (0.007) \n    0.058*** (0.008) \n     \n    0.146*** (0.019) \n    0.128*** (0.033) \n  \n  \n    Parents educ. \n     \n     \n    0.148*** (0.013) \n     \n     \n  \n  \n    Experience \n    0.009 (0.015) \n    0.010 (0.015) \n    −0.021 (0.072) \n    0.008 (0.016) \n    0.008 (0.016) \n  \n  \n    Experience sq. \n    0.001 (0.001) \n    0.001 (0.001) \n    −0.007* (0.003) \n    0.001+ (0.001) \n    0.001 (0.001) \n  \n  \n    Ability proxi \n     \n    0.006*** (0.001) \n     \n     \n     \n  \n  \n    Num.Obs. \n    722 \n    722 \n    722 \n    722 \n    722 \n  \n  \n    R2 \n    0.141 \n    0.169 \n    0.326 \n    0.076 \n    0.085 \n  \n  \n    R2 Adj. \n    0.137 \n    0.164 \n    0.323 \n    0.072 \n    0.081 \n  \n  \n    Log.Lik. \n    −341.745 \n    −329.737 \n    −1462.726 \n    −368.199 \n     \n  \n  \n    F \n    39.259 \n    36.461 \n    115.813 \n    19.574"
  },
  {
    "objectID": "slides/w10b-IV-examples.html#research-faqs",
    "href": "slides/w10b-IV-examples.html#research-faqs",
    "title": "Instrumental Variable: Examples",
    "section": "Research FAQs:",
    "text": "Research FAQs:\nBefore running a regression, ask the following four questions (see J. D. Angrist & Pischke, 2009, Ch. 1)\n\nWhat is the causal relationship of interest?\nWhat is the experiment that could ideally be used to capture the causal effect of interest?\nWhat is your identification strategy?\nWhat is your mode of statistical inference?"
  },
  {
    "objectID": "slides/w10b-IV-examples.html#faq-1.-what-is-the-causal-relationship-of-interest",
    "href": "slides/w10b-IV-examples.html#faq-1.-what-is-the-causal-relationship-of-interest",
    "title": "Instrumental Variable: Examples",
    "section": "FAQ 1. What is the causal relationship of interest?",
    "text": "FAQ 1. What is the causal relationship of interest?"
  },
  {
    "objectID": "slides/w10b-IV-examples.html#faq-2.-what-is-the-experiment",
    "href": "slides/w10b-IV-examples.html#faq-2.-what-is-the-experiment",
    "title": "Instrumental Variable: Examples",
    "section": "FAQ 2. What is the experiment…?",
    "text": "FAQ 2. What is the experiment…?\n\nDescribe an ideal experiment.\nHighlight the forces you’d like to manipulate and the factors you’d like to hold constant.\nFUQs: fundamentally unidentified questions\n\nCausal effect of race or gender;\n\nHowever, we can experiment with how believes about a person’s gender of race affect decisions (Bertrand & Mullainathan, 2004).\n\nDo children that start school 1 year later learn more in the primary school?\n\nBecause older kinds are in general better learners there is not counter factual.\nHowever, it is possible to establish this school starting effect on adults (Black, Devereux, & Salvanes, 2008)."
  },
  {
    "objectID": "slides/w10b-IV-examples.html#faq-3.-what-is-your-identification-strategy",
    "href": "slides/w10b-IV-examples.html#faq-3.-what-is-your-identification-strategy",
    "title": "Instrumental Variable: Examples",
    "section": "FAQ 3. What is your identification strategy?",
    "text": "FAQ 3. What is your identification strategy?\n\n\n\n\n\n\n\nIdentification strategy\n\n\nis the manner in which a researcher uses observational data (i.e., data not generated by a randomized trial) to approximate a real experiment (J. D. Angrist & Krueger, 1991)\n\n\n\n\nUse theory!\nAnalyze, what were/are the policies/environments that could mimic the experimental setting?"
  },
  {
    "objectID": "slides/w10b-IV-examples.html#faq-4.-what-is-your-mode-of-statistical-inference",
    "href": "slides/w10b-IV-examples.html#faq-4.-what-is-your-mode-of-statistical-inference",
    "title": "Instrumental Variable: Examples",
    "section": "FAQ 4. What is your mode of statistical inference?",
    "text": "FAQ 4. What is your mode of statistical inference?\n\ndescribes the population to be studied,\nthe sample to be used,\nand the assumptions made when constructing standard errors.\nchoose appropriate statistical methods\napply them diligently."
  },
  {
    "objectID": "slides/w10b-IV-examples.html#example-1.-wage-education-and-random-nature-of-the-date-of-birth",
    "href": "slides/w10b-IV-examples.html#example-1.-wage-education-and-random-nature-of-the-date-of-birth",
    "title": "Instrumental Variable: Examples",
    "section": "Example 1. wage, education and random nature of the date of birth",
    "text": "Example 1. wage, education and random nature of the date of birth\nAngrist, J. D., & Krueger, A. B. (1991). Does Compulsory School Attendance Affect Schooling and Earnings? The Quarterly Journal of Economics, 106, 979–1014. https://doi.org/10.2307/2937954\nIdentification strategy:\n\nPolicy required students to enter school in the calendar year in which they turned six years old;\nChildren born in the fourth quarter enter school at age 5 and 3⁄4 , while those born in the first quarter enter school at age 6 3⁄4;\nCompulsory schooling laws require students to remain in school until their 16th birthdays;\n\n\nCombination of school start age policies and compulsory schooling laws creates a natural experiment in which children are compelled to attend school for different lengths of time depending on their birthdays."
  },
  {
    "objectID": "slides/w10b-IV-examples.html#average-schooling-by-quarter-of-birth",
    "href": "slides/w10b-IV-examples.html#average-schooling-by-quarter-of-birth",
    "title": "Instrumental Variable: Examples",
    "section": "Average schooling by quarter of birth",
    "text": "Average schooling by quarter of birth\n\n\nSource: (J. D. Angrist & Krueger, 1991)"
  },
  {
    "objectID": "slides/w10b-IV-examples.html#average-wage-by-quarter-of-birth",
    "href": "slides/w10b-IV-examples.html#average-wage-by-quarter-of-birth",
    "title": "Instrumental Variable: Examples",
    "section": "Average wage by quarter of birth",
    "text": "Average wage by quarter of birth\n\n\nSource: (J. D. Angrist & Krueger, 1991)"
  },
  {
    "objectID": "slides/w10b-IV-examples.html#fantastic-instrumental-variable",
    "href": "slides/w10b-IV-examples.html#fantastic-instrumental-variable",
    "title": "Instrumental Variable: Examples",
    "section": "Fantastic instrumental variable:",
    "text": "Fantastic instrumental variable:\n\nQuarter of birth;\nThe intuition is:\n\nOnly a small part of variance in education (the one linked to the quarter of birth) is used to identify the return to education.\nThis small part of variance occurs due to random natural experiment, thus the ceteris paribus holds here."
  },
  {
    "objectID": "slides/w10b-IV-examples.html#estimates",
    "href": "slides/w10b-IV-examples.html#estimates",
    "title": "Instrumental Variable: Examples",
    "section": "Estimates",
    "text": "Estimates"
  },
  {
    "objectID": "slides/w10b-IV-examples.html#conclusions",
    "href": "slides/w10b-IV-examples.html#conclusions",
    "title": "Instrumental Variable: Examples",
    "section": "Conclusions",
    "text": "Conclusions\n\nIV estimates are very close to the OLS;\nWhat does it mean?\n\nAbility bias was small in the OLS!"
  },
  {
    "objectID": "slides/w10b-IV-examples.html#research-question-and-the-problem",
    "href": "slides/w10b-IV-examples.html#research-question-and-the-problem",
    "title": "Instrumental Variable: Examples",
    "section": "Research question and the problem",
    "text": "Research question and the problem\n\nWhat is the effect of additional child on women labor market participation?\nConventional wisdom:\n\nMore children require more time therefore, women used to sacrifice own employment opportunities."
  },
  {
    "objectID": "slides/w10b-IV-examples.html#endogeneity-problem",
    "href": "slides/w10b-IV-examples.html#endogeneity-problem",
    "title": "Instrumental Variable: Examples",
    "section": "Endogeneity problem",
    "text": "Endogeneity problem\nWhat would the ideal experiment here?\n\nFamilies without children are inappropriate counter factual\nRich families can afford more children: inappropriate counter factual\nFamily usually plan for having an additional children\n\nthus, a families with 1 children are also inappropriate counter factual\n\nwe need a source of exogenous variation in children"
  },
  {
    "objectID": "slides/w10b-IV-examples.html#identification-strategy-1",
    "href": "slides/w10b-IV-examples.html#identification-strategy-1",
    "title": "Instrumental Variable: Examples",
    "section": "Identification strategy",
    "text": "Identification strategy\nPeople may plan for a second child, but they cannot plan for having twins!!!\n\nWe can use dummy variable for a twin birth as instrument for the number of children."
  },
  {
    "objectID": "slides/w10b-IV-examples.html#results",
    "href": "slides/w10b-IV-examples.html#results",
    "title": "Instrumental Variable: Examples",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "slides/w10a-IV.html#short-regression",
    "href": "slides/w10a-IV.html#short-regression",
    "title": "Instrumental Variable",
    "section": "Short Regression",
    "text": "Short Regression\n\\[\nY_i = \\alpha^S + \\rho^S s_i + \\beta^S X_i + \\varepsilon_i^S\n\\tag{1.1}\\]\n\nannual earning \\(Y_i\\)\nyears of education \\(s_i\\)\n\\(X_i\\) vector of other control variables, such as experience.\n\n\nIs the ceteris paribus fulfilled in Equation 1.1?\n\nIs control for experience and education sufficient?\nAt a given experience/education level, are more- and less-educated workers equally able and diligent? (see Joshua D. Angrist & Pischke, 2014, Ch. 6)"
  },
  {
    "objectID": "slides/w10a-IV.html#long-regression",
    "href": "slides/w10a-IV.html#long-regression",
    "title": "Instrumental Variable",
    "section": "Long Regression",
    "text": "Long Regression\n\\[\nY_i = \\alpha + \\rho s_i + \\beta X_i + \\gamma A^{'}_{i} + \\varepsilon_i\n\\tag{1.2}\\]\n\nwhere \\(A^{'}_{i}\\) is the ability variable that we desire to have in order to ensure the unbiased estimates of \\(\\rho\\).\nOmitting \\(A^{'}_{i}\\) causes a selection bias or endogeneity:\n\n\\(\\rho^{S} = \\rho + \\underbrace{\\delta_{A^{'} s} \\times \\gamma}_{\\text{ability bias}}\\)"
  },
  {
    "objectID": "slides/w10a-IV.html#definition",
    "href": "slides/w10a-IV.html#definition",
    "title": "Instrumental Variable",
    "section": "Definition",
    "text": "Definition\n\nConsider following LONG and SHORT models:\n\n\n\\[\n\\begin{aligned}\nY_i &= \\alpha + \\rho s_i + \\beta X_i + \\gamma A^{'}_{i} + \\varepsilon_i , && \\text{long}\\\\\nY_i &= \\alpha^S + \\rho^S s_i + \\beta^S X_i + \\varepsilon_i^S , && \\text{short}\n\\end{aligned}\n\\]\n\n\\(s_i\\) is the causal variable of interest (education)\n\\(A^{'}_{i}\\) is the vector of control variables that we desire to have in order to ensure unbiased estimates of \\(\\rho\\);\nVariable \\(s_i\\) is endogenous if it correlates with the error terms \\(\\varepsilon^{S}_i\\) :\n\n\n\n\\[\nCov(s_i, \\varepsilon^{S}_i) \\neq 0\n\\]"
  },
  {
    "objectID": "slides/w10a-IV.html#endogeneity-in-practice",
    "href": "slides/w10a-IV.html#endogeneity-in-practice",
    "title": "Instrumental Variable",
    "section": "Endogeneity in practice:",
    "text": "Endogeneity in practice:\n\n\n\nvariation in the independent variable \\(s_i\\) (education) is not “random” as compared to the variation in the dependent variable \\(Y_i\\), but\nan external process \\(U\\) affects variation in both \\(s_i\\) and \\(Y_i\\);\nthus, \\(s_i\\) is endogenous to \\(Y_i\\);\n\n\n\n\n\n\nIf variance of \\(s_i\\) is truly independent of \\(Y_i\\), \\(s_i\\) is exogenous."
  },
  {
    "objectID": "slides/w10a-IV.html#causes-of-endogeneity",
    "href": "slides/w10a-IV.html#causes-of-endogeneity",
    "title": "Instrumental Variable",
    "section": "Causes of endogeneity",
    "text": "Causes of endogeneity\n\nOmitted Variable Bias (familiar)\nMeasurement Error\nSimultaneity"
  },
  {
    "objectID": "slides/w10a-IV.html#ovb",
    "href": "slides/w10a-IV.html#ovb",
    "title": "Instrumental Variable",
    "section": "OVB",
    "text": "OVB"
  },
  {
    "objectID": "slides/w10a-IV.html#measurement-error",
    "href": "slides/w10a-IV.html#measurement-error",
    "title": "Instrumental Variable",
    "section": "Measurement error",
    "text": "Measurement error"
  },
  {
    "objectID": "slides/w10a-IV.html#measurement-error-1",
    "href": "slides/w10a-IV.html#measurement-error-1",
    "title": "Instrumental Variable",
    "section": "Measurement error",
    "text": "Measurement error\n\nWe estimate a long model: \\(Y_i = \\alpha + \\beta s^*_i + e_i\\) ,\n\nbut \\(s^*_i\\) is unavailable, we only have \\(s_i = s^*_i + m_i\\) instead,\n\\(m_i\\) is a systematic measurement error,\n\\(E[m_i] =0\\) and \\(Cov(s^*_i, m_i) = Cov(e_i, m_i) = 0\\).\n\nDesired coefficient \\(\\beta = \\frac{Cov(Y_i, s_i)}{Var(s_i)}\\)\nBut with the erroneous data, we estimate biased coefficient \\(\\beta_b\\)\n\n\n\\[\n\\begin{aligned}\n\\beta_b & =  \n\\frac{Cov(Y_i, s_i)}{Var(s_i)}  =\n\\frac{Cov(a+\\beta s^*_i + e_i, s^*_i + m_i)}{Var(s_i)} \\\\  \n& =  \\frac{\\beta \\cdot Cov(s^*_i, s^*_i)}{Var(s_i)}  =  \\beta \\frac{Var(s^{*}_i)}{Var(s_i)}\n\\end{aligned}\n\\]\n\n(see Joshua D. Angrist & Pischke, 2014, Ch. 6)"
  },
  {
    "objectID": "slides/w10a-IV.html#simultaneity",
    "href": "slides/w10a-IV.html#simultaneity",
    "title": "Instrumental Variable",
    "section": "Simultaneity",
    "text": "Simultaneity"
  },
  {
    "objectID": "slides/w10a-IV.html#simultaneity-1",
    "href": "slides/w10a-IV.html#simultaneity-1",
    "title": "Instrumental Variable",
    "section": "Simultaneity",
    "text": "Simultaneity\n\nSimultaneity occurs if at least two variables are jointly determined.\n\nA typical case is when observed outcomes are the result of separate behavioral mechanisms that are coordinated in an equilibrium.\n\nThe prototypical case is a system of demand and supply equations:\n\n\\(D(p)\\) = how high would demand be if the price was set to \\(p\\)?\n\\(S(p)\\) = how high would supply be if the price was set to \\(p\\)?\n\nNumber of police people and the crime rate.\n(see M. J. Wooldridge, 2020, Ch. 17) for more details on the problem and solutions."
  },
  {
    "objectID": "slides/w10a-IV.html#instrumental-variable",
    "href": "slides/w10a-IV.html#instrumental-variable",
    "title": "Instrumental Variable",
    "section": "Instrumental Variable",
    "text": "Instrumental Variable\nis another variable \\(Z_i\\) that affects only endogenous regressor \\(s_i\\) and satisfies:\n\n\n\n\n\n\n\n\nRelevance condition:\nExclusion restriction:\nIndependence assumption:\n\n\n\n\n(see Joshua D. Angrist & Pischke, 2014 Ch. 3 and 6; Joshua D. Angrist & Pischke, 2009, Ch. 4.; Hernán & Robins, 2020, Ch. 16; J. M. Wooldridge, 2010, Ch. 8; Söderbom, Teal, & Eberhardt, 2014, Ch. 11; Imbens, 2020)"
  },
  {
    "objectID": "slides/w10a-IV.html#relevance-condition",
    "href": "slides/w10a-IV.html#relevance-condition",
    "title": "Instrumental Variable",
    "section": "1. Relevance condition:",
    "text": "1. Relevance condition:\n\n\n\n\\(Z_i\\) has a causal effect on \\(s_i\\);\n\n\n\nViolation of the relevance condition:"
  },
  {
    "objectID": "slides/w10a-IV.html#exclusion-restriction",
    "href": "slides/w10a-IV.html#exclusion-restriction",
    "title": "Instrumental Variable",
    "section": "2. Exclusion restriction:",
    "text": "2. Exclusion restriction:\n\n\n\n\\(Z_i\\) does not affect \\(Y_i\\) directly, except through its potential effect on \\(s_i\\);\n\n\n\nViolation of the exclusion restriction:"
  },
  {
    "objectID": "slides/w10a-IV.html#independence-assumption",
    "href": "slides/w10a-IV.html#independence-assumption",
    "title": "Instrumental Variable",
    "section": "3. Independence assumption:",
    "text": "3. Independence assumption:\n\n\n\n\\(Z_i\\) is randomly assigned or “as good as randomly assigned”, the same as\n\\(Z_i\\) is unrelated to the omitted variables \\(A^{'}_i\\), same as\n\\(Z_i\\) and \\(Y_i\\) do not share any common causes\n\n\n\nViolation of the independence assumption:"
  },
  {
    "objectID": "slides/w10a-IV.html#iv-regression-algorithm-using-2sls-1",
    "href": "slides/w10a-IV.html#iv-regression-algorithm-using-2sls-1",
    "title": "Instrumental Variable",
    "section": "IV regression algorithm using 2SLS (1)",
    "text": "IV regression algorithm using 2SLS (1)\nStage 1: regress endogenous variable \\(s_i\\) on all \\(X_i\\) plus the instrument \\(Z_i\\)\n\\[\ns_i = \\pi_0 + \\pi_1 Z_i + \\rho X_i + \\nu_i\n\\]\n\nCompute fitted values form the stage 1: \\(\\hat s_i = \\pi_0 + \\pi_1 Z_i + \\rho X_i\\).\n\n\nSubstitute \\(s_i\\) with the \\(\\hat{s_i}\\) from the stage 1.\n\n\nStage 2: \\(Y_i = \\alpha^{IV} + \\rho^{IV} \\hat{s_i} + \\beta^{IV} X_i + \\varepsilon^{IV}_i\\)\n\n\nwhere\n\n\\(\\hat{s_i}\\) are the fitted values from the first stage\n\\(\\rho^{IV}\\) is the causal effect of interest from stage two that is asymptotically equal to \\(\\rho\\) , the true effect of interest (\\(\\rho^{IV} \\asymp \\rho\\))"
  },
  {
    "objectID": "slides/w10a-IV.html#wage-and-education-again-1",
    "href": "slides/w10a-IV.html#wage-and-education-again-1",
    "title": "Instrumental Variable",
    "section": "Wage and Education (again)",
    "text": "Wage and Education (again)\n\\[\nY_i = \\alpha^S + \\rho^S s_i + \\beta^S X_i + \\varepsilon_i^S\n\\]\n\nWe know that estimate of years of education \\(s_i\\) is biased because of the OVB (ability bias).\nThink of an RCT experiment that could help to estimate true causal effect of \\(s_i\\) on income!\nWhat instrument \\(Z_i\\) can we use for education?"
  },
  {
    "objectID": "slides/w10a-IV.html#fantastic-ivs-and-how-to-find-them",
    "href": "slides/w10a-IV.html#fantastic-ivs-and-how-to-find-them",
    "title": "Instrumental Variable",
    "section": "Fantastic IVs and how to find them…",
    "text": "Fantastic IVs and how to find them…\n\nUse theory!\n\nhuman capital theory suggests that people make schooling choices by comparing the costs and benefits of alternatives.\n\nThink and speculate:\n\nWhat is the ideal experiment that could capture the effect of schooling on education?\nWhat are the forces you’d like to manipulate and the factors you’d like to hold constant?\nWhat are the other processes that are independent of wage, but may affect schooling?\n\nAnalyze, what were/are the policies/environments that could mimic the experimental setting?\n\n\n\n\n\n\n\n\nReasoning on how researcher use theory and available observational data to approximate real experiment is called Identification strategy!"
  },
  {
    "objectID": "slides/w10a-IV.html#fantastic-ivs-for-education",
    "href": "slides/w10a-IV.html#fantastic-ivs-for-education",
    "title": "Instrumental Variable",
    "section": "Fantastic IVs for education",
    "text": "Fantastic IVs for education\n\nLoan policies or other subsidies that vary independently of ability or earnings potential\nRegion and time variation in school construction (Duflo, 2001)\nProximity to college(Card, 1994)\nQuarter of birth (Joshua D. Angrist & Krueger, 1991)\nParents education (Buckles & Hungerman, 2013)\nNumber of siblings"
  },
  {
    "objectID": "slides/w10a-IV.html#using-parents-education-as-the-iv-for-education",
    "href": "slides/w10a-IV.html#using-parents-education-as-the-iv-for-education",
    "title": "Instrumental Variable",
    "section": "Using parents education as the IV for education",
    "text": "Using parents education as the IV for education\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(modelsummary)\ndta <-\n  read_csv(\"education_parents.csv\") %>% \n  mutate(lwagehour = log(wage/hours)) %>% \n  mutate(parents_edu = feduc + meduc)\nglimpse(dta)\n\n\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(modelsummary)\ndta <-\n  read_csv(here::here(\"exercises\",\"ex10-IV\", \"education_parents.csv\")) %>% \n  mutate(lwagehour = log(wage/hours)) %>% \n  mutate(parents_edu = feduc + meduc) %>% \n  filter(!is.na(parents_edu))\n\n\nRows: 935 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (17): wage, hours, IQ, KWW, educ, exper, tenure, age, married, black, so...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow the code\nglimpse(dta)\n\n\nRows: 722\nColumns: 19\n$ wage        <dbl> 769, 808, 825, 650, 562, 600, 1154, 1000, 930, 900, 1318, …\n$ hours       <dbl> 40, 50, 40, 40, 40, 40, 45, 40, 43, 45, 38, 40, 50, 45, 40…\n$ IQ          <dbl> 93, 119, 108, 96, 74, 91, 111, 95, 132, 125, 119, 118, 105…\n$ KWW         <dbl> 35, 41, 46, 32, 27, 24, 37, 44, 44, 40, 24, 47, 37, 39, 36…\n$ educ        <dbl> 12, 18, 14, 12, 11, 10, 15, 12, 18, 15, 16, 16, 10, 15, 11…\n$ exper       <dbl> 11, 11, 11, 13, 14, 13, 13, 16, 8, 4, 7, 9, 17, 6, 19, 10,…\n$ tenure      <dbl> 2, 16, 9, 7, 5, 0, 1, 16, 13, 3, 2, 9, 2, 9, 10, 4, 3, 8, …\n$ age         <dbl> 31, 37, 33, 32, 34, 30, 36, 36, 38, 30, 28, 34, 35, 36, 38…\n$ married     <dbl> 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ black       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ south       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ urban       <dbl> 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ sibs        <dbl> 1, 1, 1, 4, 10, 1, 2, 1, 1, 2, 3, 1, 1, 3, 2, 2, 5, 2, 0, …\n$ brthord     <dbl> 2, NA, 2, 3, 6, 2, 3, 1, 1, NA, 1, 1, 2, 3, 3, 1, 1, 1, 1,…\n$ meduc       <dbl> 8, 14, 14, 12, 6, 8, 14, 12, 13, 12, 10, 12, 6, 12, 10, 12…\n$ feduc       <dbl> 8, 14, 14, 12, 11, 8, 5, 11, 14, 12, 10, 12, 8, 10, 8, 12,…\n$ lwage       <dbl> 6.645091, 6.694562, 6.715384, 6.476973, 6.331502, 6.396930…\n$ lwagehour   <dbl> 2.956212, 2.782539, 3.026504, 2.788093, 2.642622, 2.708050…\n$ parents_edu <dbl> 16, 28, 28, 24, 17, 16, 19, 23, 27, 24, 20, 24, 14, 22, 18…"
  },
  {
    "objectID": "slides/w10a-IV.html#estimating-iv-manually",
    "href": "slides/w10a-IV.html#estimating-iv-manually",
    "title": "Instrumental Variable",
    "section": "Estimating IV manually",
    "text": "Estimating IV manually\n\n\nShow the code\n# No IV\nols <-  lm(log(wage) ~ educ + exper + I(exper^2), data = dta)\n\n# No IV but with controls for IQ\nols_iq <-  lm(log(wage) ~ educ + exper + I(exper^2) + IQ, data = dta)\n\n# First stage\nfirst_stage <- lm(educ ~ parents_edu + exper + I(exper^2), data = dta)\n\n# Fitted values of endogenous regressor\ndta_fitted <- dta %>% mutate(educ_fit = fitted(first_stage))\n\n# Second stage\nsecond_stage <- lm(log(wage) ~ educ_fit + exper + I(exper^2), data = dta_fitted)\n\n\n\n\nShow the code\nmodelsummary(\n  list(`OLS` = ols, `OLS (with ability proxi)` = ols_iq,\n       `1 stage (par. educ.)` = first_stage, \n       `2 stage (par. educ.)` = second_stage),\n  fmt = \"%.3f\",\n  estimate = \"{estimate}{stars} ({std.error})\",\n  statistic = NULL,\n  coef_map = c(\n    \"educ\" = \"Education\", \n    \"educ_fit\" = \"Education\", \n    \"parents_edu\" = \"Parents educ.\",\n    \"exper\" = \"Experience\",  \n    \"I(exper^2)\" = \"Experience sq.\",  \n    \"IQ\" = \"Ability proxi\"\n  ),\n  gof_omit = \"AIC|BIC|RMSE\"\n)\n\n\n\n\n \n  \n      \n    OLS \n    OLS (with ability proxi) \n    1 stage (par. educ.) \n     2 stage (par. educ.) \n  \n \n\n  \n    Education \n    0.078*** (0.007) \n    0.058*** (0.008) \n     \n    0.146*** (0.019) \n  \n  \n    Parents educ. \n     \n     \n    0.148*** (0.013) \n     \n  \n  \n    Experience \n    0.009 (0.015) \n    0.010 (0.015) \n    −0.021 (0.072) \n    0.008 (0.016) \n  \n  \n    Experience sq. \n    0.001 (0.001) \n    0.001 (0.001) \n    −0.007* (0.003) \n    0.001+ (0.001) \n  \n  \n    Ability proxi \n     \n    0.006*** (0.001) \n     \n     \n  \n  \n    Num.Obs. \n    722 \n    722 \n    722 \n    722 \n  \n  \n    R2 \n    0.141 \n    0.169 \n    0.326 \n    0.076 \n  \n  \n    R2 Adj. \n    0.137 \n    0.164 \n    0.323 \n    0.072 \n  \n  \n    Log.Lik. \n    −341.745 \n    −329.737 \n    −1462.726 \n    −368.199 \n  \n  \n    F \n    39.259 \n    36.461 \n    115.813 \n    19.574"
  },
  {
    "objectID": "slides/w10a-IV.html#using-siblings-number-as-the-iv-for-education",
    "href": "slides/w10a-IV.html#using-siblings-number-as-the-iv-for-education",
    "title": "Instrumental Variable",
    "section": "Using siblings number as the IV for education",
    "text": "Using siblings number as the IV for education\n\n\nShow the code\nlibrary(ivreg)\niv_fit2 <- ivreg(\n  log(wage) ~ educ + exper + I(exper ^ 2) | sibs + exper + I(exper ^ 2) ,\n  data = dta )\n\n\n\n\nShow the code\nmodelsummary(\n  list(`OLS` = ols, `OLS (with ability proxi)` = ols_iq,\n       `1 stage (par. educ.)` = first_stage, \n       `2 stage (par. educ.)` = second_stage,\n       `2 stage (siblings)` = iv_fit2),\n  fmt = \"%.3f\",\n  estimate = \"{estimate}{stars} ({std.error})\",\n  statistic = NULL,\n  coef_map = c(\n    \"educ\" = \"Education\", \n    \"educ_fit\" = \"Education\", \n    \"parents_edu\" = \"Parents educ.\",\n    \"exper\" = \"Experience\",  \n    \"I(exper^2)\" = \"Experience sq.\",  \n    \"IQ\" = \"Ability proxi\"\n  ),\n  gof_omit = \"AIC|BIC|RMSE\"\n)\n\n\n\n\n \n  \n      \n    OLS \n    OLS (with ability proxi) \n    1 stage (par. educ.) \n     2 stage (par. educ.) \n     2 stage (siblings) \n  \n \n\n  \n    Education \n    0.078*** (0.007) \n    0.058*** (0.008) \n     \n    0.146*** (0.019) \n    0.128*** (0.033) \n  \n  \n    Parents educ. \n     \n     \n    0.148*** (0.013) \n     \n     \n  \n  \n    Experience \n    0.009 (0.015) \n    0.010 (0.015) \n    −0.021 (0.072) \n    0.008 (0.016) \n    0.008 (0.016) \n  \n  \n    Experience sq. \n    0.001 (0.001) \n    0.001 (0.001) \n    −0.007* (0.003) \n    0.001+ (0.001) \n    0.001 (0.001) \n  \n  \n    Ability proxi \n     \n    0.006*** (0.001) \n     \n     \n     \n  \n  \n    Num.Obs. \n    722 \n    722 \n    722 \n    722 \n    722 \n  \n  \n    R2 \n    0.141 \n    0.169 \n    0.326 \n    0.076 \n    0.085 \n  \n  \n    R2 Adj. \n    0.137 \n    0.164 \n    0.323 \n    0.072 \n    0.081 \n  \n  \n    Log.Lik. \n    −341.745 \n    −329.737 \n    −1462.726 \n    −368.199 \n     \n  \n  \n    F \n    39.259 \n    36.461 \n    115.813 \n    19.574"
  },
  {
    "objectID": "slides/w10a-IV.html#consistency-and-unbiasedness",
    "href": "slides/w10a-IV.html#consistency-and-unbiasedness",
    "title": "Instrumental Variable",
    "section": "Consistency and unbiasedness",
    "text": "Consistency and unbiasedness\n\nIV estimates are not unbiased, but they are consistent (Joshua D. Angrist & Krueger, 2001).\n\nUnbiasedness means the estimator has a sampling distribution centered on the parameter of interest in a sample of any size, while\nConsistency only means that the estimator converges to the population parameter as the sample size grows.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nResearchers that use IV should aspire to work with large samples.\n\n\n\nNo statistical tests is available for checking consistency"
  },
  {
    "objectID": "slides/w10a-IV.html#bad-instruments-1",
    "href": "slides/w10a-IV.html#bad-instruments-1",
    "title": "Instrumental Variable",
    "section": "Bad instruments (1)",
    "text": "Bad instruments (1)\n\n\\(Z_i\\) that does not satisfy any of the Relevance condition, Exclusion restriction and Independence assumption;"
  },
  {
    "objectID": "slides/w10a-IV.html#bad-instruments-2",
    "href": "slides/w10a-IV.html#bad-instruments-2",
    "title": "Instrumental Variable",
    "section": "Bad instruments (2)",
    "text": "Bad instruments (2)\n\n\\(Z_i\\) that correlate with omitted variable (OV) but do not cause changes in it or inflict simultaneity:\n\n\nThey result into much greater upwards shifting bias compare to the OLS;\nFor example the weather in Brazil and supply price and demand quantity of coffee:\n\nweather shifts the supply curve, it is random, thus it seems as a plausible instrument for price in the demand model\nthe weather in Brazil determines supply expectations on futures exchange, thus, it also shifts the demand for coffee before the supply price is affected;"
  },
  {
    "objectID": "slides/w10a-IV.html#bad-instruments-3",
    "href": "slides/w10a-IV.html#bad-instruments-3",
    "title": "Instrumental Variable",
    "section": "Bad instruments (3)",
    "text": "Bad instruments (3)\n\nWeak instrument \\(Z_i\\):\n\nWhen the instrument \\(Z_i\\) is only weakly correlates with endogenous regressor \\(s_i\\);\nFind a better one!"
  },
  {
    "objectID": "slides/w10a-IV.html#weak-instrument-test",
    "href": "slides/w10a-IV.html#weak-instrument-test",
    "title": "Instrumental Variable",
    "section": "Weak instrument test:",
    "text": "Weak instrument test:\n\nRun the first stage regression with and without the IV;\nCompare the F-statistics\n\nIf F-statistics with instrument is greater than that without by 5 of more,\nthis is a sign of a strong instrument (Staiger & Stock, 1997);\n\nThis test does not ensure that our instruments are independent of omitted variable \\(A^{'}_i\\) or \\(Y_i\\);\nStaiger & Stock (1997)"
  },
  {
    "objectID": "slides/w10a-IV.html#overidentification-1",
    "href": "slides/w10a-IV.html#overidentification-1",
    "title": "Instrumental Variable",
    "section": "Overidentification (1)",
    "text": "Overidentification (1)\n\nnumber of instruments \\(G\\) exceeds the number of endogenous variables \\(K\\).\n\nwhen the IV is overidentified, estimates are biased;\nbias is proportional to \\(K - G\\);\nusing fewer instruments therefore reduces bias;\n\nIf you have few candidates for IV and one endogenous regressor:\n\nselect one IV for the first stage, and\nput the remaining instruments as controls into the second stage"
  },
  {
    "objectID": "slides/w10a-IV.html#overidentification-2",
    "href": "slides/w10a-IV.html#overidentification-2",
    "title": "Instrumental Variable",
    "section": "Overidentification (2)",
    "text": "Overidentification (2)\nSargan’s overidentification test:\n\n\\(H_0:Cov(Z^{'}_i,\\varepsilon^{IV}_i)=0\\) - the covariance between the instrument and the error term is zero\n\\(H_1:Cov(Z^{'}_i,\\varepsilon^{IV}_i)\\neq0\\)\nThus, by rejecting the \\(H_0\\), we conclude that at least one of the instruments is not valid."
  },
  {
    "objectID": "slides/w10a-IV.html#wu-hausman-test-for-endogeneity",
    "href": "slides/w10a-IV.html#wu-hausman-test-for-endogeneity",
    "title": "Instrumental Variable",
    "section": "Wu-Hausman test for endogeneity",
    "text": "Wu-Hausman test for endogeneity\nWu-Hausman test for endogeneity tests if the variable that we are worried about is indeed endogenous.\n\n\\(H_0:Cov(s_i,\\varepsilon_i)=0\\) - the covariance between potentially endogenous variable and the error term is zero\n\\(H_1:Cov(s_i,\\varepsilon_i) \\neq 0\\)\nThus, by rejecting the \\(H_0\\), we conclude that there is endogeneity and there might be a need for IV."
  },
  {
    "objectID": "slides/w10a-IV.html#research-question-and-the-problem",
    "href": "slides/w10a-IV.html#research-question-and-the-problem",
    "title": "Instrumental Variable",
    "section": "Research question and the problem",
    "text": "Research question and the problem\n\nWhat are the fundamental causes of the large differences in income per capita across countries?\nwith better “institutions,” more secure property rights, and less distortionary policies,\n\ncountries invest more in physical and human capital, and\nuse these factors more efficiently to\nachieve a greater level of income.\n\nInstitutions are a likely cause of income growth."
  },
  {
    "objectID": "slides/w10a-IV.html#endogeneity-problem",
    "href": "slides/w10a-IV.html#endogeneity-problem",
    "title": "Instrumental Variable",
    "section": "Endogeneity problem",
    "text": "Endogeneity problem\nWhat could be the ideal experiment to find the effect of institutions on income?\n\nRich economies choose or can afford better institutions.\nEconomies that are different for a variety of reasons\n\nwill differ both in their institutions and in their income per capita.\n\nTo estimate the impact of institutions on income,\n\nwe need a source of exogenous variation in institutions."
  },
  {
    "objectID": "slides/w10a-IV.html#identification-strategy",
    "href": "slides/w10a-IV.html#identification-strategy",
    "title": "Instrumental Variable",
    "section": "Identification strategy",
    "text": "Identification strategy\nis the manner in which a researcher uses observational data (i.e., data not generated by a randomized trial) to approximate a real experiment (Joshua D. Angrist & Krueger, 1991)\n\n\n\n\nCurrent performance is caused by\nCurrent institutions, which are caused by\nEarly institutions, which are caused by\nSettlements types during colonization, which are caused by\nSettlers’ (potential) mortality or colonization risks."
  },
  {
    "objectID": "slides/w10a-IV.html#empirical-model-ols-estimator",
    "href": "slides/w10a-IV.html#empirical-model-ols-estimator",
    "title": "Instrumental Variable",
    "section": "Empirical model (OLS estimator)",
    "text": "Empirical model (OLS estimator)\n\\[\n\\begin{aligned}\n\\log (\\text{GDP per capita}_i) & = \\beta_0  \\\\\n& + \\beta_1 \\text{Proxy for institutions} \\\\\n& + \\gamma \\text{Control variables} + \\epsilon_i\n\\end{aligned}\n\\]\n\n\\(i\\) is the country;\nDependent variable is the GDP per capita in 1995;\nAs the proxy of the institutional quality, authors used average protection against expropriation risk in 1985-1990 (index/country ranking);\nControls include latitude of the country and continent-specific dummy variables;"
  },
  {
    "objectID": "slides/w10a-IV.html#ols-estimation",
    "href": "slides/w10a-IV.html#ols-estimation",
    "title": "Instrumental Variable",
    "section": "OLS estimation",
    "text": "OLS estimation"
  },
  {
    "objectID": "slides/w10a-IV.html#empirical-model-iv-estimator",
    "href": "slides/w10a-IV.html#empirical-model-iv-estimator",
    "title": "Instrumental Variable",
    "section": "Empirical model (IV estimator)",
    "text": "Empirical model (IV estimator)\nFirst stage:\n\n\\[\n\\begin{aligned}\n\\text{Proxy for institutions}  & = \\beta_0  \\\\\n& + \\beta_1 \\log (\\text{Settlers mortality in 16-18th cent.}) \\\\\n& + \\gamma \\text{Control variables} + e_i,\n\\end{aligned}\n\\]\n\nEuropean settlers mortality in the 16-18th centuries is the precise number of how many settlers died in the country that they tried to colonize.\n\n\n\nSecond stage:\n\n\n\\[\n\\begin{aligned}\n\\log (\\text{GDP per capita}_i) & = \\beta_0^{IV}  \\\\\n& + \\beta_1^{IV} \\widehat{ \\text{Proxy for institutions}} \\\\\n& + \\gamma^{IV} \\text{Control variables} + \\epsilon_i^{IV},\n\\end{aligned}\n\\]\n\n\\(\\widehat{ \\{ \\text{Proxy for institutions} \\}}\\) are fitted values from the first stage."
  },
  {
    "objectID": "slides/w10a-IV.html#iv-results",
    "href": "slides/w10a-IV.html#iv-results",
    "title": "Instrumental Variable",
    "section": "IV results",
    "text": "IV results"
  }
]