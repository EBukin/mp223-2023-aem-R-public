---
title: "Multiple Linear Regression: practical aspects"
editor: source
format:
  revealjs: 
    smaller: false
    scrollable: false
    code-overflow: wrap
    code-copy: true
    header-includes: |
      <link href="custom.css" rel="stylesheet">
bibliography: ../bib/references.bib
editor_options: 
  chunk_output_type: console
---

```{r echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(
  fig.align = "center",
  fig.width = 7,
  fig.asp = 0.618,
  fig.retina = 3,
  out.width = "100%"
)

library(tidyverse)
library(ggdag)      # For plotting DAGs
library(dagitty)    # For working with DAG logic
library(ggpubr)
library(wooldridge)
library(here)
theme_set(theme_bw())
```

# Relevance of a theory in econometric models

## The Basic Theory of Production [@Sadoulet1995, Chapter 3]

::: notes
Explain why production function should be zero. 
-   Intensive to use all variables factors to produce goods.
-   Leaving underutilized production factors makes production inefficient.
:::

**Farm's production function**

. . .

$$
h(q,x,z) = 0,
$$
. . .

where: 

-   $q$ - output quantities (agricultural produce)

-   $x$ - variable input quantities (fertilizers, labor, seed, water, ...)

-   $z$ - fixed factors (private: land, equipment; public: infrastructure, extension; exogenous features ...)

## Farm's optimization problem

::: notes
Ask what is the optimization problem. Ask how the profit is being defined on the farm.
:::

Prices of **inputs $w^{'}$** and **outputs $p^{'}$** affect farmers decisions of:

-   what and how much to **produce $q$**, and 

-   what and how much to use as the **input $x$**.

. . .

Farm's objective function is **to maximize own profit**:

. . .

$$
\max_{x,q} \;\;\; p^{'}q-w^{'}x, \\ \text{s.t.} \;\; h(q,x,z) = 0
$$

## Solution to the producer's optimization problem

$$
\max_{x,q} \;\;\; p^{'}q-w^{'}x, \\ \text{s.t.} \;\; h(q,x,z) = 0
$$

Is a set of **input demand** and **output supply** functions:

$$
\begin{cases}
x = x(p,w,z), & \text{factor demand function} \\
q = q(p,w,z), & \text{supply function}
\end{cases}
$$


## Farm's optimization problem is

::: columns
::: {.column width="50%"}
**Structural form**

$$
\max_{x,q} \;\;\; p^{'}q-w^{'}x, \\ \text{s.t.} \;\; h(q,x,z) = 0
$$

-   Models that we can simulate with linear programming, structural equation modelling

::: 
::: {.column width="50%"}

**Reduced forms**
$$
\begin{cases}
x = x(p,w,z), & \text{factor demand function} \\
q = q(p,w,z), & \text{supply function}
\end{cases}
$$

-   Models that we estimate with regressions
::: 
::: 

# The Production Function [@Sadoulet1995, Appendix A.4]

Mathematical function that transforms inputs quantities $x$ into output $q$.

## Taylor expansion [@Sadoulet1995, Appendix A.2]

Introduced by Brook Taylor in 1715 ...

. . .

Value of any function could be approximated at an arbitrary point $x_0$ with a Taylor expansion:

. . .

$$
f(x) \approx f(x_0) + f^{'}(x_0)(x - x_0) + \frac{f^{''}(x_0)}{2!}(x - x_0)^2 + \cdots .
$$

## Taylor expansion example: 

What is the value of $e^{0.4}$?

. . .

$$
e^x = \sum_{n=0}^{\infty} \frac{x^n}{n!} \approx 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \cdots
$$

. . .

$$
e^{0.4} \approx 1 + .4 + \frac{.4^2}{2!} + \frac{.4^3}{3!} + \frac{.4^4}{4!} + \frac{.4^5}{5!} 
$$

. . .

$$
\approx 1 + .4 + .16/2 + 0.064/6 + 0.0256/24 + 0.01024 / 120
$$

. . .

$$
\approx  1 + .4 + .08 + 0.0106(6) + 0.00106(6) + 0.000106(6) 
$$
. . .

$$
\approx  1.491819  
$$

. . .

$$
\text{exp(0.4)} = 1.491825
$$

## First order Taylor approximation

$q \approx f(x)$ at $x_0$.

$$
f(x) \approx  f(x_0) + f^{'}(x_0)(x-x_0) \\ \approx 
\underbrace{f(x_0) - f^{'}(x_0) x_0}_{\alpha} + \underbrace{f^{'}(x_0)}_{\beta}\, x
$$

. . .

Simple regression: $q \approx \alpha + \beta \, x$

. . .

$q \approx f(x_1, x_2)$ at $a, b$

$$
f(x_1, x_2) \approx
\underbrace{f(a,b) - f_{x_1}^{'}(a,b)a - f_{x_2}^{'}(a,b)b}_{\alpha} + \underbrace{f_{x_1}^{'}(a,b)}_{\beta_1}\, x_1 + \underbrace{f_{x_2}^{'}(a,b)}_{\beta_2}\, x_2 
$$

. . .

Multiple regression: $q \approx \alpha + \beta_1 \, x_1 + \beta_2 \, x_2$


## Second and greater order Taylor approximation

Results in polynomial functions with interaction terms:

$$
q \approx \alpha + \beta_1 \, x_1 + \beta_2 \, x_2 + \beta_3 \, x_1^2 + \beta_4 \, x_2^2 + \beta_5 x_1 x_2
$$

## Production functions

Consider 

$$q = f(x_i)$$

Production function of several inputs $x_i$, where 


$$x_i = e^{\ln x_i}$$

and $q$ is the quantity of goods produced.

. . .

Then, 

$$
\ln q = \ln f(e^{\ln x_i}) \; \; \; \; \text{or} \; \; \; \; \ln q = g(\ln x_i)
$$

. . .

Let us approximate $\ln q = g(\ln x_i)$ using Taylor expansion.

## Fist order Taylor approximation

. . .

$$
\ln q \approx \alpha + \sum_i \beta_i \ln x_i
$$

. . .

**This is a Cobb-Douglas production function**: $q \approx e^\alpha \prod_i x_i^{\beta_i}$


## Second order Taylor approximation

of $\ln q = g(\ln x_i)$ around an arbitrary point $x_{i0} \ne 0$

. . .

$$
\ln q \approx \alpha + \sum_i \beta_i  \ln x_i + \frac{1}{2}\sum_{ij} \gamma_{ij} \ln x_{i} \ln x_{j}
$$

. . .

**This is a trans-log production function**

# Example 1. production function

## Problem and research question

::: columns
::: {.column width="50%"}
::: incremental
-   Policy makers are considering to abolish the share-cropping land tenure system.

-   As an alternative either owner-cultivation with hired labor or pure rental relationship should be established. 

-   Main rationale is in the claim that sharecropping is an inefficient farm structure.
:::
:::
::: {.column width="50%"}
::: incremental
-   **Theoretical debate**

-   Contract theory [@Dasgupta1999] sees sharecropping as a form of the optimal risks and incentives sharing contract.

-   It is a **second best contract** in agriculture after pure rental of hired labor contract.

-   Sharecropping can be as efficient as rental/labor contracts, when interlinked labor, resources, credit and insurance markets fail.
:::
:::
:::

. . .

**Research question:** Is there any negative effect of sharecropping on farms productivity?

## How to answer this question? (Any ideas)

. . .

Data that we have covers a sample of the rice-cultivating farmers in India [@Feng2010]:

-   $\text{output}$ is gross output of rice in kg
-   $\text{land}$ the total area cultivated with rice, measured in hectares
-   $\text{labor}$ total labor inputs (excluding harvest labor) in hours
-   $\text{status}$ land tenure system on the farm: owner-operated, Share-cropping and mixed

. . .

Let us estimate a simple production function of a rice-producing farm using linear form and Cobb-Douglas function. 

. . .

$$\text{output} = \alpha + \beta_1 \text{status : share} + \beta_2 \text{status : mixed} + \beta_3 \text{land} + \beta_4 \text{labor} + e$$

. . .

$$\text{output} = \alpha + \beta_1 \text{status : share} + \beta_2 \text{status : mixed} + \beta_3 \ln \text{land} + \beta_4 \ln \text{labor} + e$$

## Data (1)

```{r echo=FALSE}
library(tidyverse)
library(GGally)
data("RiceFarms", package = "splm")
farm_dta <- RiceFarms %>% as_tibble() %>%
  select(output = goutput, land = size, labor = totlabor, status)

# fit1 <- lm(output ~ land + labor + status,  data =  farm_dta)
# fit1 %>% summary()
```

```{r echo=TRUE}
library(tidyverse)
library(GGally)
library(modelsummary)
glimpse(farm_dta)
```

## Exploratory staistics (1)

```{r echo=TRUE}
datasummary_skim(farm_dta, output = "markdown")
```

## Exploratory staistics (2)

```{r echo=TRUE}
datasummary_skim(farm_dta, output = "markdown", "categorical")
```

## Exploratory staistics (3)

```{r echo=TRUE}
datasummary( (land  + labor  + output) ~ status * (mean + sd), farm_dta, output = "markdown")
```

## Exploratory staistics (3)

```{r echo=TRUE}
ggpairs(farm_dta, aes(colour = status, alpha = 0.2))
```

## Regression and interpretation {.smaller}

```{r echo=TRUE, eval=FALSE}
#| output-location: column-fragment
fit1 <- lm(output ~ status + land + labor, 
           data = farm_dta)

fit2_cd <- lm(output ~ status + log(land) + log(labor), 
           data = farm_dta)

modelsummary(
  list(`Linear prod.fn. (level-level)` = fit1,
       `CD prod.fn. (log-log)` = fit2_cd),
  estimate = "{estimate} {stars} ({std.error})", 
  statistic = NULL
)
```

```{r echo=FALSE}
#| output-location: column-fragment
fit1 <- lm(output ~ status + land + labor, 
           data = farm_dta)

fit2_cd <- lm(output ~ status + log(land) + log(labor), 
           data = farm_dta)

modelsummary(
  list(`Linear prod.fn. (level-level)` = fit1,
       `CD prod.fn. (log-log)` = fit2_cd),
  estimate = "{estimate} {stars} </br> ({std.error})", 
  coef_map = c("(Intercept)" = "(Intercept)", 
               "statusshare" = "Sharecropping", 
               "statusmixed" = "Mixed", 
               "land" = "Land, ha", 
               "log(land)" = "Land, ha", 
               "labor" = "Labor, hours", 
               "log(labor)" = "Labor, hours"),
  statistic = NULL,
  gof_omit =  c("AIC|BIC|Log|RMS"),
  output  = "markdown"
)
```

## Conclusion {.smaller}


::: columns
::: {.column width="50%"}
```{r}
modelsummary(
  list(`Linear prod. fn. (level-level)` = fit1,
       `CD prod. fn. (log-log)` = fit2_cd),
  estimate = "{estimate} {stars} </br> ({std.error})", 
  coef_map = c("(Intercept)" = "(Intercept)", 
               "statusshare" = "Sharecropping", 
               "statusmixed" = "Mixed", 
               "land" = "Land, ha", 
               "log(land)" = "Land, ha", 
               "labor" = "Labor, hours", 
               "log(labor)" = "Labor, hours"),
  statistic = NULL,
  gof_omit =  c("AIC|BIC|Log|RMS"),
  output  = "markdown"
)
```


::: 
::: {.column width="50%"}
::: incremental
-   Interpret coefficients in both models.

-   What is the estimated effect of share-cropping?

-   Is this model shows causal effects of $x$ on $y$?

-   Is there a ceteris paribus?
:::
:::
:::


## Entertainment: Fitted values (1)

```{r}
#| fig-asp: 1.2
#| fig-width: 5.5
#| echo: true
#| output-location: column-fragment
farm_dta %>% 
  mutate(fitted = fitted(fit1)) %>% 
  ggplot() + 
  aes(x = land,
      y = output,
      colour = "Observed") +
  xlab("Land area, ha") + 
  ylab("Rice output, kg")
```

## Entertainment: Fitted values (1)

```{r}
#| fig-asp: 1.2
#| fig-width: 5.5
#| echo: true
#| output-location: column
farm_dta %>% 
  mutate(fitted = fitted(fit1)) %>% 
  ggplot() + 
  aes(x = land,
      y = output,
      colour = "Observed") +
  xlab("Land area, ha") + 
  ylab("Rice output, kg") +
  geom_point(alpha = 0.3) + 
  theme(legend.position = c(0.1, 0.9)) + 
  scale_color_manual(
    values = c('#BF382A', '#0C4B8E'))
```

## Entertainment: Fitted values (1)

```{r}
#| fig-asp: 1.2
#| fig-width: 5.5
#| echo: true
#| output-location: column
farm_dta %>%
  mutate(fitted = fitted(fit1)) %>%
  ggplot() +
  aes(x = land,
      y = output,
      colour = "Observed") +
  xlab("Land area, ha") + 
  ylab("Rice output, kg") +
  geom_point(alpha = 0.3) +
  theme(legend.position = c(0.1, 0.9)) + 
  scale_color_manual(
    values = c('#BF382A', '#0C4B8E')) +
  geom_point(
    aes(y = fitted, colour = "Gitted"),
    alpha = 0.3)
```

## Entertainment: Fitted values (1)

```{r}
#| fig-asp: 1.2
#| fig-width: 5.5
#| echo: true
#| output-location: column
farm_dta %>%
  mutate(fitted = fitted(fit1)) %>%
  ggplot() +
  aes(x = land,
      y = output,
      colour = "Observed") +
  xlab("Land area, ha") + 
  ylab("Rice output, kg") +
  geom_point(alpha = 0.3) +
  theme(legend.position = c(0.1, 0.9)) + 
  scale_color_manual(
    values = c('#BF382A', '#0C4B8E')) +
  geom_point(
    aes(y = fitted, colour = "Gitted"),
    alpha = 0.3) + 
  scale_x_log10() +
  scale_y_log10()
```

## Entertainment: Actual values 3D

```{r}
library(plotly)
set.seed(1)
ply_dta <- farm_dta %>% mutate(fitted = fitted(fit1)) %>% 
  filter(output < 3000) %>% sample_n(100)
plot_ly(
  data = ply_dta, 
  x = ~land, 
  y = ~labor, 
  z = ~output , 
  color = ~status, 
  colors = c('#BF382A', '#0C4B8E', '#2abf38')) %>%
  add_markers(opacity = 0.5) %>%
  layout(
    autosize = F, 
    width = 800, 
    height = 750,
    scene = list(
    xaxis = list(title = 'x1: Land, ha'),
    yaxis = list(title = 'x2: Labor, hours'),
    zaxis = list(title = 'y: Output, kg')
  ))
```

## Entertainment: Actual + Fitted values 3D

```{r}
plot_ly(
  data = ply_dta,
  x = ~ land,
  y = ~ labor,
  z = ~ output ,
  color = "Observed"
) %>%
  add_markers(#
    opacity = 0.5,
    marker = list(
      color = '#0C4B8E',
      size = 5,
      opacity = 0.5
    )) %>%
  add_markers(
    z = ~ fitted ,
    color = "Fitted",
    marker = list(
      color = '#BF382A',
      size = 5,
      opacity = 0.5
    )
  ) %>%
  layout(
    autosize = F, 
    width = 800, 
    height = 750,
    scene = list(
    xaxis = list(title = 'x1: Land, ha'),
    yaxis = list(title = 'x2: Labor, hours'),
    zaxis = list(title = 'y: Output, kg')
  ))
```

## Entertainment: Fitted values 3D v.2

```{r}
x_values <- ply_dta$land %>% round(3)
y_values <- ply_dta$labor %>% round(3)
z_values <- ply_dta$output %>% round(3)
x_grid <- seq(from = min(x_values), to = max(x_values), length = 10)
y_grid <- seq(from = min(y_values), to = max(y_values), length = 10)
z_grid <-
  tibble(
    land =  seq(from = min(x_values), to = max(x_values), length = 10),
    labor = seq(from = min(y_values), to = max(y_values), length = 10)
  ) %>% 
  expand(land, labor) %>% 
  mutate(status = unique(ply_dta$status[[1]])) %>% 
  mutate(plane = predict(fit1, newdata = .)) %>% 
  pull(plane) %>% 
  matrix(nrow = length(x_grid)) %>%
  t()


plot_ly() %>%
  # 3D scatterplot:
  add_markers(
    x = x_values,
    y = y_values,
    z = z_values,
    marker = list(size = 5)
  ) %>%
  # Regression plane:
  add_surface(
    x = x_grid,
    y = y_grid,
    z = z_grid,
    opacity = 0.5
  ) %>%
  # Axes labels and title:
  layout(
    autosize = F, 
    width = 800, 
    height = 750,
    scene = list(
      xaxis = list(title = 'x1: Land, ha'),
      yaxis = list(title = 'x2: Labor, hours'),
      zaxis = list(title = 'y: Output, kg')
    )
  )
```

# Assumptions

::: incremental

1.  Linearity - **validated using data**

2.  Random sampling - "accepted based on faith in data collectors"

3.  No (Perfect) Collinearity - **validated using data**

4.  No Endogeneity - OVB

5.  Homoscedasticity (No heteroscedasticity) - **validated using data**

6.  Normality - **validated using data**

:::

# Linearity

**Means:** in $y = \alpha + \beta x$; $y$ is a linear function of $x$.

. . .

**If violated:** $\alpha$ and $\beta$ are biased and inefficient.

. . .

**Detected:** with scatter plots: $y$ against $x$, observed/residuals against fitted).

. . .

**Resolved:** with a linear transformation.


## Linearity detection: scatter plots 

::: columns
::: {.column width="70%"}
```{r}
#| echo: true
#| output-location: fragment
#| fig-asp: 0.8
farm_dta %>% select(-status) %>% ggpairs()
```
:::

::: {.column width="30%"}
::: incremental
-   Note the magnitude of $x$ and $y$

-   From min to max $y$ increases about 5000 times.

-   If a variable increases more than 10 times between min and max, $log$ transformation may be needed.
:::
:::
:::

## Linearity detection: fitted values vs residuals 

```{r eval=FALSE}
#| echo: true
library(performance)
check_model(fit1, check = "linearity", panel = FALSE)
```

```{r}
#| echo: false
#| output-location: fragment
library(performance)
plot(check_model(fit1, check = "ncv", panel = FALSE))$NCV + 
  labs(caption = str_replace(as.character(fit1$call)[[2]], "~", "="))
```

## Linearity detection: fitted values vs residuals (v2)

```{r}
#| echo: true
#| output-location: fragment
#| fig-asp: 0.6
#| fig-width: 8
plot(fit1, which = 1)
```


## Linearity Conclusion

Model: 

```{r}
fit1$call
```

. . .

**Is the linearity assumption fulfilled?**

. . .

**What could be done to improve the model?**

. . .

Linear transformation with log: partial (level-log or log-level) or full (log-log).

. . .

$$\ln (\text{output}) = \alpha + \beta_1 \text{status : share} + \beta_2 \text{status : mixed} \\ + \beta_3 \ln (\text{land}) + \beta_4 \ln (\text{labor}) + e$$

## Linear transformation of the model

```{r echo=TRUE, eval = FALSE}
fit1 <- lm(output ~ status + land + labor, 
           data = farm_dta)

fit2_cd <- lm(output ~ status + log(land) + log(labor), 
           data = farm_dta)

modelsummary(
  list(`Linear prod.fn. (level-level)` = fit1,
       `CD prod.fn. (log-log)` = fit2_cd),
  estimate = "{estimate} {stars} ({std.error})", 
  statistic = NULL
)
```

## Linear transformation of the model

```{r echo=FALSE, eval = TRUE}
fit2_cd <- lm(log(output) ~ status + log(land) + log(labor), data = farm_dta)
modelsummary(
  list(`Linear prod.fn. (level-level)` = fit1, `CD prod.fn. (log-log)` = fit2_cd),
  estimate = "{estimate} {stars} ({std.error})", 
  coef_map = c("(Intercept)" = "(Intercept)", 
               "statusshare" = "Sharecropping", 
               "statusmixed" = "Mixed", 
               "land" = "Land, ha", 
               "log(land)" = "Land, ha", 
               "labor" = "Labor, hours", 
               "log(labor)" = "Labor, hours"),
  statistic = NULL,
  gof_omit =  c("AIC|BIC|Log|RMS"),
  output  = "markdown"
)
```

## Change in the lineariy assumption

::: columns
::: {.column width="50%"}

**Without log-log**

```{r}
#| echo: false
#| fig-asp: 1.1
plot(check_model(fit1, check = "linearity", panel = FALSE))$NCV + 
  labs(caption = str_replace(as.character(fit1$call)[[2]], "~", "="))
```

:::
::: {.column width="50%"}

**With log-log**

```{r}
#| echo: false
#| fig-asp: 1.1
plot(check_model(fit2_cd, check = "linearity", panel = FALSE))$NCV + 
  labs(caption = str_replace(as.character(fit2_cd$call)[[2]], "~", "="))
```

:::
:::

## Change in Fitted values 3D v.2

```{r}
ply_dta2 <- farm_dta %>% mutate(fitted = (fitted(fit2_cd)))# %>% sample_n(500)
x_values <- ply_dta2$land %>% round(3)
y_values <- ply_dta2$labor %>% round(3)
z_values <- ply_dta2$output %>% round(3)

x_grid <- seq(from = min(x_values), to = max(x_values), length = 50)
y_grid <- seq(from = min(y_values), to = max(y_values), length = 50)
z_grid <-
  tibble(
    land =  seq(from = min(x_values), to = max(x_values), length = 50),
    labor = seq(from = min(y_values), to = max(y_values), length = 50)
  ) %>% 
  expand(land, labor) %>% 
  mutate(status = unique(ply_dta2$status[[1]])) %>% 
  mutate(plane = exp(predict(fit2_cd, newdata = .))) %>% 
  pull(plane) %>% 
  matrix(nrow = length(x_grid)) %>%
  t()

library(plotly)
plot_ly() %>%
  # 3D scatterplot:
  add_markers(
    x = (x_values),
    y = (y_values),
    z = (z_values),
    marker = list(size = 5)
  ) %>%
  # Regression plane:
  add_surface(
    x = (x_grid),
    y = (y_grid),
    z = (z_grid),
    opacity = 0.5
  ) %>%
  # Axes labels and title:
  layout(
    autosize = F, 
    width = 800, 
    height = 750,
    scene = list(
      xaxis = list(title = 'x1: Land, ha'),
      yaxis = list(title = 'x2: Labor, hours'),
      zaxis = list(title = 'y: Output, kg')
    )
  )
```

## Partial transformation (1) {.smaller}

```{r}
#| echo: true
fit3_level_log <- lm(output ~ status + log(land) + log(labor), data = farm_dta)
```

```{r}
modelsummary(
  list(`Linear prod.fn. (level-level)` = fit1, 
       `CD prod.fn. (log-log)` = fit2_cd,
       `Level-log prod.fn. (level-log)` = fit3_level_log),
  estimate = "{estimate} {stars} ({std.error})", 
  coef_map = c("(Intercept)" = "(Intercept)", 
               "statusshare" = "Sharecropping", 
               "statusmixed" = "Mixed", 
               "land" = "Land, ha", 
               "log(land)" = "Land, ha", 
               "labor" = "Labor, hours", 
               "log(labor)" = "Labor, hours"),
  statistic = NULL,
  gof_omit =  c("AIC|BIC|Log|RMS"),
  output  = "markdown"
)
```


## Partial transformation (2)

```{r}
#| eval: false
#| echo: true
check_model(fit3_level_log, check = "linearity", panel = FALSE)
```

```{r}
#| echo: false
#| output-location: fragment
plot(check_model(fit3_level_log, check = "ncv", panel = FALSE))$NCV + 
  labs(caption = str_replace(as.character(fit1$call)[[2]], "~", "="))
```

# No (perfect) collinearity (multicollinearity)

Two sub-problems:

1.  Perfect collinearity: when present regression will not be estimated.

2.  Near-perfect collinearity: regression works, but inference is misleading.


# No Perfect Collinearity

::: incremental
**Problem:**

-   none of the regressors can be written as an **exact linear combinations** of some other regressors in the model.
-   For example:

    -   in $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3$ ,
    -   where $x_3 = x_2 + x_1$,
    -   all $x_i$ are collinear.
:::

. . .

**Detection:** Through a regression equation

. . .

**Solution:** model re-specification.


## Perfect Collinearity Example 1

$$
\begin{aligned}
\hat{wage} & = \alpha + \hat\beta_1 education + \hat\beta_2 experience \\ 
& + \hat\beta_3 male  + \hat\beta_4 female
\end{aligned}
$$

::: incremental
-   $male  + female = 1$

-   Is there a multicollinearity problem here?
:::

## Perfect Collinearity Example 2

$$
\begin{aligned}
\hat{output} & = \alpha + \hat\beta_1 land + \hat\beta_2 seeds + \hat\beta_3 fertilizers \\ &  + \hat\beta_4 others + \hat\beta_5 total
\end{aligned}
$$

::: incremental
-   where $total = seeds + fertilizers + others$

-   Is there a multicollinearity problem here?

-   Coefficient $\hat\beta_5$ is aliased and wont be estimated
::: 

## Perfect Collinearity Example 3

Same model but in log:

$$
\begin{aligned}
\ln (\hat{output})&  = \hat\beta_1 \ln (land) + \hat\beta_2 \ln (seeds) + \hat\beta_3 \ln (fertilizers) \\ & + \hat\beta_4 \ln (others) + \hat\beta_5 \ln (total) 
\end{aligned}
$$

where $total = seeds + fertilizers + others$


::: incremental
-   Is there a multicollinearity problem here?

-   Think!

-   $log(a) + log(b) = log(a * b)$

-   ::: callout-caution
    No, there is no collinearity because $\ln (seeds) + \ln (fertilizers) + \ln (others) \ne \ln (total)$. 
    This is not a linear combination!
    $\ln (seeds \cdot fertilizers \cdot others) \ne \ln (total)$
    :::
    
:::

## Perfect Collinearity Example 4 {.smaller}

Using the production function estimated above, let us convert tenure type into a set of dummy variables:

```{r}
#| echo: true
farm_dta_coll <- 
  farm_dta %>% 
  mutate(
    status_owner = ifelse(status == "owner", 1, 0),
    status_share = as.integer(status == "share"),
    status_mixed = as.integer(status == "mixed")
    )
glimpse(farm_dta_coll)
```

## Perfect Collinearity Example 4

```{r}
fit1_coll_1 <- 
  lm(output ~ status_owner + status_share + status_mixed + land + labor, 
     data = farm_dta_coll)
fit1_coll_1

fit1_coll_2 <- 
  lm(output ~ status_mixed + status_owner + status_share  + land + labor, 
     data = farm_dta_coll)
fit1_coll_2
```

## Perfect Collinearity Example 4 {.smaller}

```{r echo=FALSE}
modelsummary(
  list(`Linear prod.fn. (level-level)` = fit1,
       `Linear prod.fn. with dummies 1` = fit1_coll_1,
       `Linear prod.fn. with dummies 2` = fit1_coll_2),
  estimate = "{estimate} {stars} ({std.error})", 
  statistic = NULL,
  gof_omit =  c("AIC|BIC|Log|RMS"),
  output  = "markdown"
)
```

# Multicollinearity: Near-perfect collinearity

## Problem

::: incremental
-   When two variables $x_1$ and $x_2$ are highly correlated.

-   And both ($x_1$ and $x_2$) affect $y$ significantly.

-   $x_1$ and $x_2$ are collinear

-   Estimated Standard Errors are inflated (larger then they could be).

-   Inference about $x_1$ and $x_2$ (only) is misleading.
:::

## Detection

::: incremental
1.  Variance Inflation Factor
    
    -   Not a statistical test, only an informatice number.
    -   This shows by how much, the SE of the collinear variables are larger than what they could have been if there was no collinearity.

2.  Step-wise regression approach:

    -   Include collinear variables one by one and together, and
    -   observe how the $R^2$, and var's significances changes
:::

## Solutions

::: incremental
1.  Find different independent variables.

2.  Drop one if it is redundant and repeating.

3.  Keep both and ignore if $x_1$ and $x_2$ are just control variables.

4.  Keep is removing may cause the OVB.
:::

## Multicollinearity Example 1. Production Function

```{r}
library(performance)
check_collinearity(fit2_cd)
```

. . .

If VIF > 10, we may suspect multicollinearity.

. . .

Removing land or labor will cause the OVB (omitted variable bias), we must keep both variables. 

## Multicollinearity Example 2 {.smaller}

$$
\begin{aligned}
\hat{output} = \hat\beta_1 land + \hat\beta_2 seeds + \hat\beta_3 fertilizers + \hat\beta_4 others
\end{aligned}
$$

::: incremental
-   where $seeds$ and $fertilizers$ highly correlate between each other (r=0.9),
-   VIF of $seeds$ and $fertilizers$ is > 12.2
-   our key-interest variable is $land$.
-   As $fertilizers$ is a control variable and we may have OVB if we remove it, 
-   If we really want to reduce VIF...:

    -   Dis-aggregate fertilizers into mineral and organic, for example.
    -   Aggregate fertilizers and seeds
    
::: 

## Multicollinearity Example 3 {.smaller}

Same model but with a quadratic term:

$$\hat{output} = \hat\beta_1 land + \hat\beta_2 land^2 + \hat\beta_3 seeds + \hat\beta_4 fertilizers + \hat\beta_5 others$$

::: incremental
-   VIF of $land$ and $land^2$ is > 25
-   Is there a multicollinearity problem here?
-   Think!
-   Not really
-   $land^2$ is not a linear combination of $land$ ;
-   Linear combination is when $land + land$ not when $land \times land$ ;
:::

## Multicollinearity Example 4 {.smaller}

Step-wise approach to regression analysis:

```{r}
precip_dta <- 
    alr4::MinnWater %>%
    as_tibble() %>% 
    mutate(`log(muniPop)` = log(muniPop),
           `log(muniUse)` = log(muniUse)) %>% 
    select(year, muniPrecip, `log(muniPop)`, `log(muniUse)`) 

fit3.1 <- lm(`log(muniUse)` ~ muniPrecip , precip_dta)
fit3.2 <- lm(`log(muniUse)` ~ muniPrecip + year, precip_dta)
fit3.3 <- lm(`log(muniUse)` ~ muniPrecip + `log(muniPop)`, precip_dta)
fit3.4 <- lm(`log(muniUse)` ~ muniPrecip + year  + `log(muniPop)`, precip_dta)

my_summary <- function(x) {
  x <- set_names(x, str_c("Model ", seq_along(x)))
  all_eq <- x %>% imap_chr(. , ~ {str_c(.y, ": ", as.character(.x$call)[[2]])}) %>% 
    str_c(collapse = "</br>")
  
  modelsummary(
    x,
    estimate = "{estimate}{stars} ({std.error})",
    statistic = NULL,
    output = "markdown",
    gof_omit =  c("AIC|BIC|Log|F|RMS"),
    notes = all_eq
  )
}

my_summary(list(fit3.1, fit3.2, fit3.3, fit3.4))
```


# Homoscedasticity (No heteroscedasticity)

$$
Var(u|x_{i1}, x_{i2}, \cdots , x_{ik}) = \sigma^2
$$



```{r}
#| echo: false
#| fig-asp: 0.4
knitr::include_graphics("img/het-01.png")
```

## Problem



## Ne2 slide


```{r echo=FALSE}
library(parameters)
library(performance)
library(modelbased)

model <- lm(Sepal.Length ~ Petal.Length * Species, data = iris)

plot(estimate_relation(model))
slopes <- estimate_slopes(model, trend = "Petal.Length", at = "Species")

plot(slopes)



fit_tl <- 
  lm(log(output) ~ status * log(land) * log(labor), 
    data = farm_dta
  )




library(modelbased)
# model <- lm(mpg ~ hp * wt, data = mtcars)
slopes <- estimate_slopes(fit_tl, trend = "land", at = "status")
plot(slopes)
```







```{r eval=FALSE}
library(tidyverse)
library(GGally)
data("RiceFarms", package = "splm")
farm_dta <- RiceFarms %>% as_tibble() %>%
  select(output = goutput, land = size, labor = totlabor, status)
ggpairs(farm_dta, aes(colour = status, alpha = 0.2))
fit1 <- lm(output ~ land + labor + status,  data =  farm_dta)
fit1 %>% summary()
```



```{r eval=FALSE}


fit_cd <- 
  lm(
    output ~ land + labor, 
    data = farm_dta
  )

plot(fit_cd, which = 1)

# Elasticity of scale ### ### ### ### ### ###

library(car)
matchCoefs(fit_cd, "log")
car::linearHypothesis(
  fit_cd, 
  hypothesis.matrix = "log(land) + log(labor) + log(urea) = 1" 
)
car::deltaMethod(
  fit_cd, 
  "b2 + b3 + b4 - 1",
  parameterNames= paste("b", 1:4, sep="")
)

library(marginaleffects)
hypotheses(fit_cd)
hypotheses(fit_cd,  hypothesis = "b2 + b3 + b4 - 1 = 0")
hypotheses(fit_cd,  hypothesis = "b2 + b3 + b4 - 1 = 0", vcov = "HC3")


fit_tl <- 
  lm(
    log(output) ~ log(land) + log(labor) + log(urea) + 
      log(land):log(labor) + log(land):log(urea) + log(labor):log(urea), 
    data = farm_dta
  )


# Marginal Effects at Y
# hypotheses(fit_tl)
# hypotheses(fit_tl,  hypothesis = "b2 + b5 + b6 = 0")
slopes(fit_tl, slope = "dydx", variables = "land", by = c("labor", "urea"))

plot_predictions(fit_tl, condition = list("land", "labor" = "quartile")) + 
  scale_x_continuous(trans = "log")
plot_predictions(fit_tl, condition = list("land", "urea" = "quartile"), transform  = exp, vcov = "HC3")

avg_slopes(fit_tl, slope = "eyex")
slopes(fit_tl, slope = "dydx", by = "land")
hypotheses(fit_tl,  hypothesis = "b2 + b3 + b4 - 1 = 0")



matchCoefs(fit_cd, "log")

car::linearHypothesis(
  fit_cd, 
  hypothesis.matrix = "log(land) + log(labor) + log(urea) = 1" 
)

car::deltaMethod(
  fit_cd, 
  "b2 + b3 + b4 - 1",
  parameterNames= paste("b", 1:4, sep="")
)


library(marginaleffects)
hypotheses(fit_cd)
hypotheses(fit_cd,  hypothesis = "b2 + b3 + b4 - 1 = 0")
hypotheses(fit_cd,  hypothesis = "b2 + b3 + b4 - 1 = 0", vcov = "HC3")


fit_plmtl <- 
  plm(
    log(output) ~ log(land) + log(labor) + log(urea) + 
      log(land):log(labor) + log(land):log(urea) + log(labor):log(urea) , 
    data = farm_dta, model = "within", effect = "individual", index = c("id", "time")
  )

library(modelsummary)
library(marginaleffects)

modelsummary(
  list(CD = fit_cd,
       TL = fit_tl,
       TL_fe = fit_plmtl),
  estimate = "{estimate} {stars} ({std.error})",
  statistic = NULL#,
  # output  = "markdown"
)

library(car)
matchCoefs(fit_cd, "log")

car::linearHypothesis(
  fit_cd, 
  hypothesis.matrix = "log(land) + log(labor) + log(urea) = 1" 
)

car::deltaMethod(
  fit_cd, 
  "b1 + b2 + b3 - 1",
  parameterNames= paste("b", 0:3, sep="")
)

library(marginaleffects)
hypotheses(fit_cd,  hypothesis = "b1 + b2 + b3 - 1 = ")


library("margins")
mod1 <- lm(mpg ~ log(cyl) * hp + wt, data = mtcars)
(marg1 <- margins(mod1))
summary(marg1)

avg_slopes(mod1,  slope = "eyex")
avg_slopes(mod1,  slope = "dydx")

car::deltaMethod(
  mod1, "b1 + b3 * 15" ,
  parameterNames= paste("b", 0:3, sep="")
)




matchCoefs(fit_tl, "log")

car::deltaMethod(
  fit_tl, "b1 + b4 + b5" ,
  parameterNames= paste("b", 0:6, sep="")
)

car::deltaMethod(
  fit_tl, "b2 + b4 + b6" ,
  parameterNames= paste("b", 0:6, sep="")
)

car::deltaMethod(
  fit_tl, "b3 + b5 + b6" ,
  parameterNames= paste("b", 0:6, sep="")
)


avg_slopes(fit_cd,  slope = "eyex")
avg_slopes(fit_cd,  slope = "dydx")


hypotheses(mfx,  hypothesis = "b2 + b3 + b4 + b5 + b6 +  b7 = 1")


hypotheses(fit_tl, hypothesis = "b2 + b3 + b4 + b5 + b6 +  b7 = 1")


library(ggeffects)
ggemmeans(fit_tl, terms = c("land", "labor")) %>% plot()

avg_slopes(fit_tl, by = TRUE)

slopes(fit_tl, newdata = datagrid())
predictions(
    fit_cd,
    # newdata = datagrid(wt = 2:3),
    hypothesis = "b1 + b2 + b3 = 1")

slopes(fit_tl, newdata = datagrid(land = c(1, 10, 100)))


avg_slopes(fit_tl,
           slope = "eyex",
           variables = "labor",
           by = "land")

```


# Takeawyas

1.  Assumptions and how to validated them.

2.  Statistical plots, measures (VIF) and tests and howto interprete them.

# References

## References
